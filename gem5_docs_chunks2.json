[
    {
        "text": "gem5 Bootcamp 2024\nAs of gem5 v24.0, the most comprehensive, up to date guide for learning how to use gem5 is the\nmaterial from the\nsummer 2024 gem5 bootcamp\n.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation",
            "page_title": "gem5 Documentation",
            "parent_section": "gem5 Bootcamp 2024",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Learning gem5\nNotice: Many parts of Learning gem5 are outdated. Some sections of Learning gem5 have been updated for gem5 v24.1 based on content from the 2024 gem5 bootcamp, but others have not. Proceed with caution!\nLearning gem5\ngives a prose-heavy introduction to using gem5 for computer architecture research written by Jason Lowe-Power.\nThis is a great resource for junior researchers who plan on using gem5 heavily for a research project.\nIt covers details of how gem5 works starting with\nhow to create configuration scripts\n.\nIt then goes on to describe\nhow to modify and extend\ngem5 for your research including\ncreating\nSimObjects\n,\nusing gem5\u2019s event-driven simulation infrastructure\n, and\nadding memory system objects\n.\nIn\nLearning gem5 Part 3\nthe\nRuby cache coherence model\nis discussed in detail including a full implementation of an MSI cache coherence protocol.\nMore Learning gem5 parts are coming soon including:\nCPU models and ISAs\nDebugging gem5\nYour idea here!\nNote: this has been migrated from learning.gem5.org and there are minor problems due to this migration (e.g., missing links, bad formatting).\nPlease contact Jason (jason@lowepower.com) or create a PR if you find any errors!",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation",
            "page_title": "gem5 Documentation",
            "parent_section": "Learning gem5",
            "section_heading": "Overview"
        }
    },
    {
        "text": "gem5 101\ngem5 101\nis a set of assignments mostly from Wisconsin\u2019s graduate computer architecture classes (CS 752, CS 757, and CS 758) which will help you learn to use gem5 for research.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation",
            "page_title": "gem5 Documentation",
            "parent_section": "gem5 101",
            "section_heading": "Overview"
        }
    },
    {
        "text": "gem5 API documentation\nYou can find the doxygen-based documentation here:\nhttp://doxygen.gem5.org/release/current/index.html",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation",
            "page_title": "gem5 Documentation",
            "parent_section": "gem5 API documentation",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Other general gem5 documentation\nSee the navigation on the left side of the page!",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation",
            "page_title": "gem5 Documentation",
            "parent_section": "Other general gem5 documentation",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Developing your own gem5 standard library components\n\nThe above diagram shows the basic design of the gem5 library components.\nThere are four important abstract classes:\nAbstractBoard\n,\nAbstractProcessor\n,\nAbstractMemorySystem\n, and\nAbstractCacheHierarchy\n.\nEvery gem5 component inherits from one of these to be a gem5 component usable in a design.\nThe\nAbstractBoard\nmust be constructed by specifying an\nAbstractProcessor\n,\nAbstractMemorySystem\n, and an\nAbstractCacheHierarchy\n.\nWith this design any board may use any combination of components which inherit from\nAbstractProcessor\n,\nAbstractMemorySystem\n, and\nAbstractCacheHierarchy\n.\nFor example, using the image as a guide, we can add a\nSimpleProcessor\n,\nSingleChannelDDR3_1600\nand a\nPrivateL1PrivateL2CacheHierarchy\nto an\nX86Board\n.\nIf we desire, we can swap out the\nPrivateL1PrivateL2CacheHierarchy\nfor another class which inherits from\nAbstractCacheHierarchy\n.\nIn this tutorial we will imagine a user wishes to create a new cache hierarchy.\nAs you can see from the diagram, there are two subclasses which inherit from\nAbstractCacheHierarchy\n:\nAbstractRubyCacheHierarchy\nand\nAbstractClassicCacheHierarchy\n.\nWhile you\ncan\ninherit directly from\nAbstractCacheHierarchy\n, we recommend inheriting from the subclasses (depending on whether you wish to develop a ruby or classic cache hierarchy setup).\nWe will inherit from the\nAbstractClassicCacheHierarchy\nclass to create a classic cache setup.\nTo begin, we should create a new Python class which inherits from the\nAbstractClassicCacheHierarchy\n.\nIn this example we will call this\nUniqueCacheHierarchy\n, contained within a file\nunique_cache_hierarchy.py\n:\nfrom\nm5.objects\nimport\n(\nPort\n,\n)\nfrom\ngem5.components.boards.abstract_board\nimport\nAbstractBoard\nfrom\ngem5.components.cachehierarchies.classic.abstract_classic_cache_hierarchy\nimport\n(\nAbstractClassicCacheHierarchy\n,\n)\nclass\nUniqueCacheHierarchy\n(\nAbstractClassicCacheHierarchy\n):\ndef\n__init__\n()\n->\nNone\n:\nAbstractClassicCacheHierarchy\n.\n__init__\n(\nself\n=\nself\n)\ndef\nget_mem_side_port\n(\nself\n)\n->\nPort\n:\npass\ndef\nget_cpu_side_port\n(\nself\n)\n->\nPort\n:\npass\ndef\nincorporate_cache\n(\nself\n,\nboard\n:\nAbstractBoard\n)\n->\nNone\n:\npass\nAs with every abstract base class, there are virtual functions which must be implemented.\nOnce implemented the\nUniqueCacheHierarchy\ncan be used in simulations.\nThe\nget_mem_side_port\nand\nget_cpu_side_port\nare declared in the\nAbstractClassicCacheHierarchy\n, while\nincorporate_cache\nis declared in the\nAbstractCacheHierarchy\nThe\nget_mem_side_port\nand\nget_cpu_side_port\nfunctions return a\nPort\neach.\nAs their name suggests, these are ports used by the board to access the cache hierarchy from the memory side and the cpu side.\nThese must be specified for all classic cache hierarchy setups.\nThe\nincorporate_cache\nfunction is the function which is called to incorporate the cache into the board.\nThe contents of this function will vary between cache hierarchy setups but will typically inspect the board it is connected to, and use the board\u2019s API to connect the cache hierarchy.\nIn this example we assume the user is looking to implement a private L1 cache hierarchy, consisting of a data cache and instruction cache for each CPU core.\nThis has actually already been implemented in the gem5 stdlib as the\nPrivateL1CacheHierarchy\n, but for this example we shall duplicate the effort.\nFirst we start by implementing the\nget_mem_side_port\nand\nget_cpu_side_port\nfunctions:\nfrom\nm5.objects\nimport\n(\nBadAddr\n,\nPort\n,\nSystemXBar\n,\n)\nfrom\ngem5.components.boards.abstract_board\nimport\nAbstractBoard\nfrom\ngem5.components.cachehierarchies.classic.abstract_classic_cache_hierarchy\nimport\n(\nAbstractClassicCacheHierarchy\n,\n)\nclass\nUniqueCacheHierarchy\n(\nAbstractClassicCacheHierarchy\n):\ndef\n__init__\n(\nself\n)\n->\nNone\n:\nAbstractClassicCacheHierarchy\n.\n__init__\n(\nself\n=\nself\n)\nself\n.\nmembus\n=\nSystemXBar\n(\nwidth\n=\n64\n)\nself\n.\nmembus\n.\nbadaddr_responder\n=\nBadAddr\n()\nself\n.\nmembus\n.\ndefault\n=\nself\n.\nmembus\n.\nbadaddr_responder\n.\npio\ndef\nget_mem_side_port\n(\nself\n)\n->\nPort\n:\nreturn\nself\n.\nmembus\n.\nmem_side_ports\ndef\nget_cpu_side_port\n(\nself\n)\n->\nPort\n:\nreturn\nself\n.\nmembus\n.\ncpu_side_ports\ndef\nincorporate_cache\n(\nself\n,\nboard\n:\nAbstractBoard\n)\n->\nNone\n:\npass\nHere we have used a simple memory bus.\nNext, we implement the\nincorporate_cache\nfunction:\nfrom\nm5.objects\nimport\n(\nBadAddr\n,\nCache\n,\nPort\n,\nSystemXBar\n,\n)\nfrom\ngem5.components.boards.abstract_board\nimport\nAbstractBoard\nfrom\ngem5.components.cachehierarchies.classic.abstract_classic_cache_hierarchy\nimport\n(\nAbstractClassicCacheHierarchy\n,\n)\nfrom\ngem5.components.cachehierarchies.classic.caches.l1dcache\nimport\nL1DCache\nfrom\ngem5.components.cachehierarchies.classic.caches.l1icache\nimport\nL1ICache\nfrom\ngem5.components.cachehierarchies.classic.caches.mmu_cache\nimport\nMMUCache\nclass\nUniqueCacheHierarchy\n(\nAbstractClassicCacheHierarchy\n):\ndef\n__init__\n(\nself\n)\n->\nNone\n:\nAbstractClassicCacheHierarchy\n.\n__init__\n(\nself\n=\nself\n)\nself\n.\nmembus\n=\nSystemXBar\n(\nwidth\n=\n64\n)\nself\n.\nmembus\n.\nbadaddr_responder\n=\nBadAddr\n()\nself\n.\nmembus\n.\ndefault\n=\nself\n.\nmembus\n.\nbadaddr_responder\n.\npio\ndef\nget_mem_side_port\n(\nself\n)\n->\nPort\n:\nreturn\nself\n.\nmembus\n.\nmem_side_ports\ndef\nget_cpu_side_port\n(\nself\n)\n->\nPort\n:\nreturn\nself\n.\nmembus\n.\ncpu_side_ports\ndef\nincorporate_cache\n(\nself\n,\nboard\n:\nAbstractBoard\n)\n->\nNone\n:\n# Set up the system port for functional access from the simulator.\nboard\n.\nconnect_system_port\n(\nself\n.\nmembus\n.\ncpu_side_ports\n)\nfor\ncntr\nin\nboard\n.\nget_memory\n().\nget_memory_controllers\n():\ncntr\n.\nport\n=\nself\n.\nmembus\n.\nmem_side_ports\nself\n.\nl1icaches\n=\n[\nL1ICache\n(\nsize\n=\n\"32KiB\"\n)\nfor\ni\nin\nrange\n(\nboard\n.\nget_processor\n().\nget_num_cores\n())\n]\nself\n.\nl1dcaches\n=\n[\nL1DCache\n(\nsize\n=\n\"32KiB\"\n)\nfor\ni\nin\nrange\n(\nboard\n.\nget_processor\n().\nget_num_cores\n())\n]\n# ITLB Page walk caches\nself\n.\niptw_caches\n=\n[\nMMUCache\n(\nsize\n=\n\"8KiB\"\n)\nfor\n_\nin\nrange\n(\nboard\n.\nget_processor\n().\nget_num_cores\n())\n]\n# DTLB Page walk caches\nself\n.\ndptw_caches\n=\n[\nMMUCache\n(\nsize\n=\n\"8KiB\"\n)\nfor\n_\nin\nrange\n(\nboard\n.\nget_processor\n().\nget_num_cores\n())\n]\nif\nboard\n.\nhas_coherent_io\n():\nself\n.\n_setup_io_cache\n(\nboard\n)\nfor\ni\n,\ncpu\nin\nenumerate\n(\nboard\n.\nget_processor\n().\nget_cores\n()):\ncpu\n.\nconnect_icache\n(\nself\n.\nl1icaches\n[\ni\n].\ncpu_side\n)\ncpu\n.\nconnect_dcache\n(\nself\n.\nl1dcaches\n[\ni\n].\ncpu_side\n)\nself\n.\nl1icaches\n[\ni\n].\nmem_side\n=\nself\n.\nmembus\n.\ncpu_side_ports\nself\n.\nl1dcaches\n[\ni\n].\nmem_side\n=\nself\n.\nmembus\n.\ncpu_side_ports\nself\n.\niptw_caches\n[\ni\n].\nmem_side\n=\nself\n.\nmembus\n.\ncpu_side_ports\nself\n.\ndptw_caches\n[\ni\n].\nmem_side\n=\nself\n.\nmembus\n.\ncpu_side_ports\ncpu\n.\nconnect_walker_ports\n(\nself\n.\niptw_caches\n[\ni\n].\ncpu_side\n,\nself\n.\ndptw_caches\n[\ni\n].\ncpu_side\n)\nint_req_port\n=\nself\n.\nmembus\n.\nmem_side_ports\nint_resp_port\n=\nself\n.\nmembus\n.\ncpu_side_ports\ncpu\n.\nconnect_interrupt\n(\nint_req_port\n,\nint_resp_port\n)\ndef\n_setup_io_cache\n(\nself\n,\nboard\n:\nAbstractBoard\n)\n->\nNone\n:\n\"\"\"Create a cache for coherent I/O connections\"\"\"\nself\n.\niocache\n=\nCache\n(\nassoc\n=\n8\n,\ntag_latency\n=\n50\n,\ndata_latency\n=\n50\n,\nresponse_latency\n=\n50\n,\nmshrs\n=\n20\n,\nsize\n=\n\"1kB\"\n,\ntgts_per_mshr\n=\n12\n,\naddr_ranges\n=\nboard\n.\nmem_ranges\n,\n)\nself\n.\niocache\n.\nmem_side\n=\nself\n.\nmembus\n.\ncpu_side_ports\nself\n.\niocache\n.\ncpu_side\n=\nboard\n.\nget_mem_side_coherent_io_port\n()\nThis completes the code we\u2019d need to create our own cache hierarchy.\nTo use this code, a user can import it as they would any other Python module.\nAs long as this code is in gem5\u2019s python search path, you can import it.\nYou can also add\nimport sys; sys.path.append(<path to new component>)\nat the beginning of your gem5 runscript to add the path of this new component to the python search path.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/gem5-stdlib/develop-own-components-tutorial",
            "page_title": "No Title Found",
            "parent_section": "Developing your own gem5 standard library components",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Section: Contributing your component to the gem5 stdlib\n\nSub-section: Compiling your component into the gem5 standard library\n\nThe gem5 standard library code resides in\nsrc/python/gem5\n.\nThe basic directory structure is as follows:\ngem5/\n    components/                 # All the components to build the system to simulate.\n        boards/                 # The boards, typically broken down by ISA target.\n            experimental/       # Experimental boards.\n        cachehierarchies/       # The Cache Hierarchy components.\n            chi/                # CHI protocol cache hierarchies.\n            classic/            # Classic cache hierarchies.\n            ruby/               # Ruby cache hierarchies.\n        memory/                 # Memory systems.\n        processors/             # Processors.\n    prebuilt/                   # Prebuilt systems, ready to use.\n        demo/                   # Prebuilt System for demonstrations. (not be representative of real-world targets).\n    resources/                  # Utilities used for referencing and obtaining gem5-resources.\n    simulate/                   # A package for the automated running of gem5 simulations.\n    utils/                      # General utilities.\nWe recommend putting the\nunique_cache_hierarchy.py\nin\nsrc/python/gem5/components/cachehierarchies/classic/\n.\nFrom then you need to add the following line to\nsrc/python/SConscript\n:\nPySource('gem5.components.cachehierarchies.classic',\n    'gem5/components/cachehierarchies/classic/unique_cache_hierarchy.py')\nThen, when you recompile the gem5 binary, the\nUniqueCacheHierarchy\nclass will be included.\nTo use it in your own scripts you need only include it:\nfrom\ngem5.components.cachehierarchies.classic.unique_cache_hierarchy\nimport\nUniqueCacheHierarchy\n...\ncache_hierarchy\n=\nUniqueCacheHierarchy\n()\n...",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/gem5-stdlib/develop-own-components-tutorial",
            "page_title": "No Title Found",
            "parent_section": "Contributing your component to the gem5 stdlib",
            "section_heading": "Compiling your component into the gem5 standard library"
        }
    },
    {
        "text": "Section: Contributing your component to the gem5 stdlib\n\nSub-section: gem5 Code contribution and review\n\nIf you believe your addition to the gem5 stdlib would be beneficial to the gem5 community, you may submit it as a patch.\nPlease follow our\nContributing Guidelines\nif you have not contributed to gem5 before or need a reminder on our procedures.\nIn addition to our normal contribution guidelines, we strongly advise you do the following to your stdlib contribution:\nAdd Documentation\n: Classes and methods should be documented using\nreStructured text\n.\nPlease look over other source code in the stdlib to see how this is typically done.\nUse Python Typing\n: Utilize the\nPython typing module\nto specify parameter and method return types.\nUse relative imports\n: Within the gem5 stdlib, relative imports should be used to reference other modules/package in the stdlib (i.e., that contained in\nsrc/python/gem5\n).\nFormat using black\n: Please format your Python code with\nPython black\n, with 79 max line widths:\nblack --line-length=79 <file/directory>\n.\nNote\n: Python black does not always enforce line lengths.\nFor example, it will not reduce string lengths.\nYou may have to manually reduce the length of some lines.\nCode will be reviewed via\nGitHub\nlike all other contributions.\nWe would, however, emphasize that we will not accept patches to the library for simply being functional and tested;\nwe require some persuasion that the contribution improves the library and benefits the community.\nFor example, niche components may not be incorporated if they are seen to be low utility while increasing the library\u2019s maintenance overhead.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/gem5-stdlib/develop-own-components-tutorial",
            "page_title": "No Title Found",
            "parent_section": "Contributing your component to the gem5 stdlib",
            "section_heading": "gem5 Code contribution and review"
        }
    },
    {
        "text": "How to Create Your Own Board Using the gem5 Standard Library\nIn this tutorial we will cover how to create a custom board using the gem5 Standard Library.\nThis tutorial is based on the process used to make the\nRiscvMatched\n, a RISC-V prebuilt board that inherits from\nMinorCPU\n. This board can be found at\nsrc/python/gem5/prebuilt/riscvmatched\n.\nThis tutorial will create a single-channeled DDR4 memory of size 2 GiB, a core using the MinorCPU and the RISC-V ISA though the same process can be used for another type or size of memory, ISA and core.\nLikewise, this tutorial will utilize the UniqueCacheHierarchy made in the\nDeveloping Your Own Components Tutorial\n, though anyother cache hierarchy may be used.\nFirst, we start by importing the components and stdlib features we require.\nfrom\ntyping\nimport\nList\nfrom\nm5.objects\nimport\n(\nAddrRange\n,\nBaseCPU\n,\nBaseMMU\n,\nIOXBar\n,\nPort\n,\nProcess\n,\n)\nfrom\nm5.objects.RiscvCPU\nimport\nRiscvMinorCPU\nfrom\ngem5.components.boards.abstract_system_board\nimport\nAbstractSystemBoard\nfrom\ngem5.components.boards.se_binary_workload\nimport\nSEBinaryWorkload\nfrom\ngem5.components.cachehierarchies.classic.unique_cache_hierarchy\nimport\n(\nUniqueCacheHierarchy\n,\n)\nfrom\ngem5.components.memory\nimport\nSingleChannelDDR4_2400\nfrom\ngem5.components.processors.base_cpu_core\nimport\nBaseCPUCore\nfrom\ngem5.components.processors.base_cpu_processor\nimport\nBaseCPUProcessor\nfrom\ngem5.isas\nimport\nISA\nfrom\ngem5.utils.override\nimport\noverrides\nWe will begin development by creating a specialized CPU core for our board which inherits from an ISA-specific version of the chosen CPU.\nSince our ISA is RISC-V and the CPU type we desire is a MinorCPU, we will inherit from\nRiscvMinorCPU\n.\nThis is done so that we can set our own parameters to tailor the CPU it to our requirements.\nIn our example will override a single parameter:\ndecodeToExecuteForwardDelay\n(the default is 1).\nWe have called this new CPU core type\nUniqueCPU\n.\nclass\nUniqueCPU\n(\nRiscvMinorCPU\n):\ndecodeToExecuteForwardDelay\n=\n2\nAs\nRiscvMinorCPU\ninherits from\nBaseCPU\n, we can incorporate this into the standard library using\nBaseCPUCore\n, a Standard Library wrapper for\nBaseCPU\nobjects (source code for this can be found at\nsrc/python/gem5/components/processors/base_cpu_core.py\n).\nThe\nBaseCPUCore\ntakes the\nBaseCPU\nas an argument during construction.\nErgo, we can do the following:\ncore\n=\nBaseCPUCore\n(\ncore\n=\nUniqueCPU\n(),\nisa\n=\nISA\n.\nRISCV\n)\nNext we must define our processor.\nIn the gem5 Standard Library a processor is a collection of cores.\nIn cases, such as ours, we can utilize the library\u2019s\nBaseCPUProcessor\n, a processor which contains\nBaseCPUCore\nobjects (source code can be found in\nsrc/python/gem5/components/processors/base_cpu_processor.py\n).\nThe\nBaseCPUProcessor\nrequires a list of\nBaseCPUCore\ns.\nTherefore:\nprocessor\n=\nBaseCPUProcessor\n(\ncores\n=\n[\ncore\n])\nNext we focus on the construction of the board to host our components.\nAll boards must inherit from\nAbstractBoard\nand in most cases, gem5\u2019s\nSystem\nsimobject.\nTherefore, our board will inherit from\nAbstractSystemBoard\nin this case; an abstract class that inherits from both.\nIn order to run simulations with SE mode, we must also inherit from\nSEBinaryWorkload\n.\nAll\nAbstractBoard\ns must specify\nclk_freq\n(the clock frequency), the\nprocessor\n,\nmemory\n, and the\ncache_hierarchy\n.\nWe already have our processor, and will use the\nUniqueCacheHierarchy\nfor the\ncache_hierarchy\nand a\nSingleChannelDDR4_2400\n, with a size of 2GiB for the memory.\nWe will call this the\nUniqueBoard\nand it should look like the following:\nclass\nUniqueBoard\n(\nAbstractSystemBoard\n,\nSEBinaryWorkload\n):\ndef\n__init__\n(\nself\n,\nclk_freq\n:\nstr\n,\n)\n->\nNone\n:\ncore\n=\nBaseCPUCore\n(\ncore\n=\nUniqueCPU\n(),\nisa\n=\nISA\n.\nRISCV\n)\nprocessor\n=\nBaseCPUProcessor\n(\ncores\n=\n[\ncore\n])\nmemory\n=\nSingleChannelDDR4_2400\n(\n\"2GiB\"\n)\ncache_hierarchy\n=\nUniqueCacheHierarchy\n()\nsuper\n().\n__init__\n(\nclk_freq\n=\nclk_freq\n,\nprocessor\n=\nprocessor\n,\nmemory\n=\nmemory\n,\ncache_hierarchy\n=\ncache_hierarchy\n,\n)\nWith the contructor complete, we must implement the abstract methods in\nAbstractSystemBoard\n.\nIt is useful here to look at the source for\nAbstractBoard\nin\n/src/python/gem5/components/boards/abstract_system_board.py\n.\nThe abstract methods you choose to implement or not will depend on what type of system you are creating.\nIn our example functions such as\n_setup_board\n, are unneeded so we will implement them with\npass\n.\nIn other instances we will use\nNotImplementedError\nfor cases where a particular component/feature is not available on this board and an error should be returned if trying to access it.\nFor example, our board will have no IO bus.\nWe will therefore implement\nhas_io_bus\nto return\nFalse\nand have\nget_io_bus\nraise a\nNotImplementedError\nif called.\nWith the exception of\n_setup_memory_ranges\n, we do not implement many of the features the\nAbstractSystemBoard\nrequires. The board should look like this:\nclass\nUniqueBoard\n(\nAbstractSystemBoard\n,\nSEBinaryWorkload\n):\ndef\n__init__\n(\nself\n,\nclk_freq\n:\nstr\n,\n)\n->\nNone\n:\ncore\n=\nBaseCPUCore\n(\ncore\n=\nUniqueCPU\n(),\nisa\n=\nISA\n.\nRISCV\n)\nprocessor\n=\nBaseCPUProcessor\n(\ncores\n=\n[\ncore\n])\nmemory\n=\nSingleChannelDDR4_2400\n(\n\"2GiB\"\n)\ncache_hierarchy\n=\nUniqueCacheHierarchy\n()\nsuper\n().\n__init__\n(\nclk_freq\n=\nclk_freq\n,\nprocessor\n=\nprocessor\n,\nmemory\n=\nmemory\n,\ncache_hierarchy\n=\ncache_hierarchy\n,\n)\n@\noverrides\n(\nAbstractSystemBoard\n)\ndef\n_setup_board\n(\nself\n)\n->\nNone\n:\npass\n@\noverrides\n(\nAbstractSystemBoard\n)\ndef\nhas_io_bus\n(\nself\n)\n->\nbool\n:\nreturn\nFalse\n@\noverrides\n(\nAbstractSystemBoard\n)\ndef\nget_io_bus\n(\nself\n)\n->\nIOXBar\n:\nraise\nNotImplementedError\n(\n\"UniqueBoard does not have an IO Bus. \"\n\"Use `has_io_bus()` to check this.\"\n)\n@\noverrides\n(\nAbstractSystemBoard\n)\ndef\nhas_dma_ports\n(\nself\n)\n->\nbool\n:\nreturn\nFalse\n@\noverrides\n(\nAbstractSystemBoard\n)\ndef\nget_dma_ports\n(\nself\n)\n->\nList\n[\nPort\n]:\nraise\nNotImplementedError\n(\n\"UniqueBoard does not have DMA Ports. \"\n\"Use `has_dma_ports()` to check this.\"\n)\n@\noverrides\n(\nAbstractSystemBoard\n)\ndef\nhas_coherent_io\n(\nself\n)\n->\nbool\n:\nreturn\nFalse\n@\noverrides\n(\nAbstractSystemBoard\n)\ndef\nget_mem_side_coherent_io_port\n(\nself\n)\n->\nPort\n:\nraise\nNotImplementedError\n(\n\"UniqueBoard does not have any I/O ports. Use has_coherent_io to \"\n\"check this.\"\n)\n@\noverrides\n(\nAbstractSystemBoard\n)\ndef\n_setup_memory_ranges\n(\nself\n)\n->\nNone\n:\nmemory\n=\nself\n.\nget_memory\n()\nself\n.\nmem_ranges\n=\n[\nAddrRange\n(\nmemory\n.\nget_size\n())]\nmemory\n.\nset_memory_range\n(\nself\n.\nmem_ranges\n)\nThis concludes the creation of your custom board for the gem5 standard library.\nThe completed board is as follows:\nfrom\ntyping\nimport\nList\nfrom\nm5.objects\nimport\n(\nAddrRange\n,\nBaseCPU\n,\nBaseMMU\n,\nIOXBar\n,\nPort\n,\nProcess\n,\n)\nfrom\nm5.objects.RiscvCPU\nimport\nRiscvMinorCPU\nfrom\ngem5.components.boards.abstract_system_board\nimport\nAbstractSystemBoard\nfrom\ngem5.components.boards.se_binary_workload\nimport\nSEBinaryWorkload\nfrom\ngem5.components.cachehierarchies.classic.unique_cache_hierarchy\nimport\n(\nUniqueCacheHierarchy\n,\n)\nfrom\ngem5.components.memory\nimport\nSingleChannelDDR4_2400\nfrom\ngem5.components.processors.base_cpu_core\nimport\nBaseCPUCore\nfrom\ngem5.components.processors.base_cpu_processor\nimport\nBaseCPUProcessor\nfrom\ngem5.isas\nimport\nISA\nfrom\ngem5.utils.override\nimport\noverrides\nclass\nUniqueCPU\n(\nRiscvMinorCPU\n):\ndecodeToExecuteForwardDelay\n=\n2\nclass\nUniqueBoard\n(\nAbstractSystemBoard\n,\nSEBinaryWorkload\n):\ndef\n__init__\n(\nself\n,\nclk_freq\n:\nstr\n,\n)\n->\nNone\n:\ncore\n=\nBaseCPUCore\n(\ncore\n=\nUniqueCPU\n(),\nisa\n=\nISA\n.\nRISCV\n)\nprocessor\n=\nBaseCPUProcessor\n(\ncores\n=\n[\ncore\n])\nmemory\n=\nSingleChannelDDR4_2400\n(\n\"2GiB\"\n)\ncache_hierarchy\n=\nUniqueCacheHierarchy\n()\nsuper\n().\n__init__\n(\nclk_freq\n=\nclk_freq\n,\nprocessor\n=\nprocessor\n,\nmemory\n=\nmemory\n,\ncache_hierarchy\n=\ncache_hierarchy\n,\n)\n@\noverrides\n(\nAbstractSystemBoard\n)\ndef\n_setup_board\n(\nself\n)\n->\nNone\n:\npass\n@\noverrides\n(\nAbstractSystemBoard\n)\ndef\nhas_io_bus\n(\nself\n)\n->\nbool\n:\nreturn\nFalse\n@\noverrides\n(\nAbstractSystemBoard\n)\ndef\nget_io_bus\n(\nself\n)\n->\nIOXBar\n:\nraise\nNotImplementedError\n(\n\"UniqueBoard does not have an IO Bus. \"\n\"Use `has_io_bus()` to check this.\"\n)\n@\noverrides\n(\nAbstractSystemBoard\n)\ndef\nhas_dma_ports\n(\nself\n)\n->\nbool\n:\nreturn\nFalse\n@\noverrides\n(\nAbstractSystemBoard\n)\ndef\nget_dma_ports\n(\nself\n)\n->\nList\n[\nPort\n]:\nraise\nNotImplementedError\n(\n\"UniqueBoard does not have DMA Ports. \"\n\"Use `has_dma_ports()` to check this.\"\n)\n@\noverrides\n(\nAbstractSystemBoard\n)\ndef\nhas_coherent_io\n(\nself\n)\n->\nbool\n:\nreturn\nFalse\n@\noverrides\n(\nAbstractSystemBoard\n)\ndef\nget_mem_side_coherent_io_port\n(\nself\n)\n->\nPort\n:\nraise\nNotImplementedError\n(\n\"UniqueBoard does not have any I/O ports. Use has_coherent_io to \"\n\"check this.\"\n)\n@\noverrides\n(\nAbstractSystemBoard\n)\ndef\n_setup_memory_ranges\n(\nself\n)\n->\nNone\n:\nmemory\n=\nself\n.\nget_memory\n()\nself\n.\nmem_ranges\n=\n[\nAddrRange\n(\nmemory\n.\nget_size\n())]\nmemory\n.\nset_memory_range\n(\nself\n.\nmem_ranges\n)\nFrom this you can create a runscript and test your board:\nfrom\nunique_board\nimport\nUniqueBoard\nfrom\ngem5.resources.resource\nimport\nobtain_resource\nfrom\ngem5.simulate.simulator\nimport\nSimulator\nboard\n=\nUniqueBoard\n(\nclk_freq\n=\n\"1.2GHz\"\n)\n# As we are using the RISCV ISA, \"riscv-hello\" should work.\nboard\n.\nset_se_binary_workload\n(\nobtain_resource\n(\n\"riscv-hello\"\n))\nsimulator\n=\nSimulator\n(\nboard\n=\nboard\n)\nsimulator\n.\nrun\n()",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/gem5-stdlib/develop-stdlib-board",
            "page_title": "No Title Found",
            "parent_section": "How to Create Your Own Board Using the gem5 Standard Library",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Building a \u201cHello World\u201d example with the gem5 standard library\nIn this tutorial we will cover how to create a very basic simulation using gem5 components.\nThis simulation will setup a system consisting of a single-core processor, running in Atomic mode, connected directly to main memory with no caches, I/O, or other components.\nThe system will run an X86 binary in syscall emulation (SE) mode.\nThe binary will be obtained from gem5-resources and which will print a \u201cHello World!\u201d string to stdout upon execution.\nTo start we must compile the ALL build for gem5:\n# In the root of the gem5 directory\nscons build/ALL/gem5.opt\n-j\n<number of threads>\nAs of gem5 v24.1, the ALL build includes all Ruby protocols and all ISAs. If you are using a prebuilt gem5 binary, this step is not necessary.\nThen a new Python file should be created (we will refer to this as\nhello-world.py\ngoing forward).\nThe first lines in this file should be the needed imports:\nfrom\ngem5.components.boards.simple_board\nimport\nSimpleBoard\nfrom\ngem5.components.cachehierarchies.classic.no_cache\nimport\nNoCache\nfrom\ngem5.components.memory.single_channel\nimport\nSingleChannelDDR3_1600\nfrom\ngem5.components.processors.cpu_types\nimport\nCPUTypes\nfrom\ngem5.components.processors.simple_processor\nimport\nSimpleProcessor\nfrom\ngem5.isas\nimport\nISA\nfrom\ngem5.resources.resource\nimport\nobtain_resource\nfrom\ngem5.simulate.simulator\nimport\nSimulator\nAll these libraries are included inside the compiled gem5 binary.\nTherefore, you will not need to obtain them from elsewhere.\nfrom gem5.\nindicates we are importing from the\ngem5\nstandard library, and the lines starting with\nfrom gem5.components\nare importing components from the gem5 components package.\nThe\nfrom gem5.resources\nline means we are importing from the resources package, and\nfrom gem5.simulate\n, the simulate package.\nAll these packages,\ncomponents\n,\nresources\n, and\nsimulate\nare part of the gem5 standard library.\nNext we begin specifying the system.\nThe gem5 library requires the user to specify four main components: the\nboard\n, the\ncache hierarchy\n, the\nmemory system\n, and the\nprocessor\n.\nLet\u2019s start with the\ncache hierarchy\n:\ncache_hierarchy\n=\nNoCache\n()\nHere we are using\nNoCache()\n.\nThis means, for our system, we are stating there is no cache hierarchy (i.e., no caches).\nIn the gem5 library the cache hierarchy is a broad term for anything that exists between the processor cores and main memory.\nHere we are stating the processor is connected directly to main memory.\nNext we declare the\nmemory system\n:\nmemory\n=\nSingleChannelDDR3_1600\n(\n\"1GiB\"\n)\nThere exists many memory components to choose from within\ngem5.components.memory\n.\nHere we are using a single-channel DDR3 1600, and setting its size to 1 GiB.\nIt should be noted that setting the size here is technically optional.\nIf not set, the\nSingleChannelDDR3_1600\nwill default to 8 GiB.\nThen we consider the\nprocessor\n:\nprocessor\n=\nSimpleProcessor\n(\ncpu_type\n=\nCPUTypes\n.\nATOMIC\n,\nnum_cores\n=\n1\n,\nisa\n=\nISA\n.\nX86\n)\nA processor in\ngem5.components\nis an object which contains a number of gem5 CPU cores, of a particular or varying type (\nATOMIC\n,\nTIMING\n,\nKVM\n,\nO3\n, etc.).\nThe\nSimpleProcessor\nused in this example is a processor where all the CPU Cores are of an identical type.\nIt requires two arguments: the\ncpu_type\n, which we set to\nATOMIC\n, and\nnum_cores\n, the number of cores, which we set to one.\nFinally we specify which\nboard\nwe are using:\nboard\n=\nSimpleBoard\n(\nclk_freq\n=\n\"3GHz\"\n,\nprocessor\n=\nprocessor\n,\nmemory\n=\nmemory\n,\ncache_hierarchy\n=\ncache_hierarchy\n,\n)\nWhile the constructor of each board may vary, they will typically require the user to specify the\nprocessor\n,\nmemory system\n, and\ncache hierarchy\n, as well as the clock frequency to use.\nIn this example we use the\nSimpleBoard\n.\nThe\nSimpleBoard\nis a very basic system with no I/O which only supports SE-mode and can only work with \u201cclassic\u201d cache hierarchies.\nAt this point in the script we have specified everything we require to simulate our system.\nOf course, in order to run a meaningful simulation, we must specify a workload for this system to run.\nTo do so we add the following lines:\nbinary\n=\nobtain_resource\n(\n\"x86-hello64-static\"\n)\nboard\n.\nset_se_binary_workload\n(\nbinary\n)\nThe\nobtain_resource\nfunction takes a string which specifies which resource, from\ngem5-resources\n, is to be obtained for the simulation.\nAll the gem5 resources can be found on the\ngem5 Resources website\n.\nIf the resource is not present on the host system it\u2019ll be automatically downloaded.\nIn this example we are going to use the\nx86-hello-64-static\nresource;\nan x86, 64-bit, statically compiled binary which will print \u201cHello World!\u201d to stdout.\nAfter specifying the resource we set the workload via the board\u2019s\nset_se_binary_workload\nfunction.\nAs the name suggests\nset_se_binary_workload\nis a function used to set a binary to be executed in Syscall Execution mode.\nYou can see and search for available resources on the\ngem5 resources website\n.\nThis is all that is required to setup your simulation.\nFrom this you simply need to construct and run the\nSimulator\n:\nsimulator\n=\nSimulator\n(\nboard\n=\nboard\n)\nsimulator\n.\nrun\n()\nAs a recap, your script should look like the following:\nfrom\ngem5.components.boards.simple_board\nimport\nSimpleBoard\nfrom\ngem5.components.cachehierarchies.classic.no_cache\nimport\nNoCache\nfrom\ngem5.components.memory.single_channel\nimport\nSingleChannelDDR3_1600\nfrom\ngem5.components.processors.cpu_types\nimport\nCPUTypes\nfrom\ngem5.components.processors.simple_processor\nimport\nSimpleProcessor\nfrom\ngem5.isas\nimport\nISA\nfrom\ngem5.resources.resource\nimport\nobtain_resource\nfrom\ngem5.simulate.simulator\nimport\nSimulator\n# Obtain the components.\ncache_hierarchy\n=\nNoCache\n()\nmemory\n=\nSingleChannelDDR3_1600\n(\n\"1GiB\"\n)\nprocessor\n=\nSimpleProcessor\n(\ncpu_type\n=\nCPUTypes\n.\nATOMIC\n,\nnum_cores\n=\n1\n,\nisa\n=\nISA\n.\nX86\n)\n# Add them to the board.\nboard\n=\nSimpleBoard\n(\nclk_freq\n=\n\"3GHz\"\n,\nprocessor\n=\nprocessor\n,\nmemory\n=\nmemory\n,\ncache_hierarchy\n=\ncache_hierarchy\n,\n)\n# Set the workload.\nbinary\n=\nobtain_resource\n(\n\"x86-hello64-static\"\n)\nboard\n.\nset_se_binary_workload\n(\nbinary\n)\n# Setup the Simulator and run the simulation.\nsimulator\n=\nSimulator\n(\nboard\n=\nboard\n)\nsimulator\n.\nrun\n()\nIt can then be executed with:\n./build/ALL/gem5.opt hello-world.py\nIf you are using a pre-built binary, you can execute the simulation with:\ngem5 hello-world.py\nIf setup correctly, the output will look something like:\ninfo: Using default config\nGlobal frequency set at 1000000000000 ticks per second\nsrc/mem/dram_interface.cc:690: warn: DRAM device capacity (8192 Mbytes) does not match the address range assigned (1024 Mbytes)\nsrc/base/statistics.hh:279: warn: One of the stats is a legacy stat. Legacy stat is a stat that does not belong to any statistics::Group. Legacy stat is deprecated.\nboard.remote_gdb: Listening for connections on port 7005\nsrc/sim/simulate.cc:199: info: Entering event queue @ 0.  Starting simulation...\nsrc/sim/syscall_emul.hh:1117: warn: readlink() called on '/proc/self/exe' may yield unexpected results in various settings.\nsrc/sim/mem_state.cc:448: info: Increasing stack size by one page.\nHello world!\nIt should be obvious from this point that a\nboard\u2019s\nparameters may be altered to test other designs.\nFor example, if we want to test a\nTIMING\nCPU setup we\u2019d change our\nprocessor\nto:\nprocessor\n=\nSimpleProcessor\n(\ncpu_type\n=\nCPUTypes\n.\nTIMING\n,\nnum_cores\n=\n1\n,\nisa\n=\nISA\n.\nX86\n)\nThis is all that is required.\nThe gem5 standard library will reconfigure the design as is necessary.\nAs another example, consider swapping out a component for another.\nIn this design we decided on\nNoCache\nbut we could use another classic cache hierarchy, such as\nPrivateL1CacheHierarchy\n.\nTo do so we\u2019d change our\ncache_hierarchy\nparameter:\n# We import the cache hierarchy we want.\nfrom\ngem5.components.cachehierarchies.classic.private_l1_cache_hierarchy\nimport\nPrivateL1CacheHierarchy\n...\n# Then set it.\ncache_hierarchy\n=\nPrivateL1CacheHierarchy\n(\nl1d_size\n=\n\"32KiB\"\n,\nl1i_size\n=\n\"32KiB\"\n)\nNote here that\nPrivateL1CacheHierarchy\nrequires the user to specify the L1 data and instruction cache sizes to be constructed.\nNo other part of the design need change.\nThe gem5 standard library will incorporate the cache hierarchy as required.\nTo recap on what was learned in this tutorial:\nA system can be built with the gem5 components package using\nprocessor\n,\ncache hierarchy\n,\nmemory system\n, and\nboard\ncomponents.\nGenerally speaking, components of the same type are interchangeable as much as is possible. E.g., different\ncache hierarchy\ncomponents may be swapped in and out of a design without reconfiguration needed in other components.\nboards\ncontain functions to set workloads.\nThe resources package may be used to obtain prebuilt resources from gem5-resources.\nThese are typically workloads that may be run via set workload functions.\nThe simulate package can be used to run a board within a gem5 simulation.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/gem5-stdlib/hello-world-tutorial",
            "page_title": "No Title Found",
            "parent_section": "Building a \u201cHello World\u201d example with the gem5 standard library",
            "section_heading": "Overview"
        }
    },
    {
        "text": "An overview of the gem5 standard library\nSimilar to standard libraries in programming languages, the gem5 standard library is designed to provide users of gem5 with commonly used components, features, and functionality with the goal of improving their productivity.\nThe gem5 stdlib was introduced in\nv21.1\nin an alpha-release state (then referred to as \u201cgem5 components\u201d), and has been fully released as of\nv21.2\n.\nFor users new to the gem5 standard library, the following tutorials may be of help in understanding how the gem5 stdlib may be used to improve the creation of gem5 simulations.\nThey include a tutorial on building syscall emulation and full-system simulations, as well as a guide on how to extend the library and contribute.\nThe\nconfigs/examples/gem5_library\ndirectory in the gem5 repository also contains example scripts which use the library.\nThe following subsections give a broad overview of the gem5 stdlib packages and what there intended purposes are.\nNote: The documentation/tutorials/etc. related to the standard library have been updated for the v24.1 release.\nPlease ensure you have the correct version of gem5 before proceeding.\nAs part of\ngem5\u2019s 2022 Bootcamp\n, the stdlib was taught as a tutorial.\nSlides for this tutorial can be found\nhere\n.\nA video recording of this tutorial can be found\nhere\n.\nThe stdlib was also covered during the\n2024 gem5 Bootcamp\n.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/gem5-stdlib/overview",
            "page_title": "No Title Found",
            "parent_section": "An overview of the gem5 standard library",
            "section_heading": "Overview"
        }
    },
    {
        "text": "The gem5 stdlib components package and its design philosophy\nThe gem5 stdlib components package is the central part of the gem5 stdlib.\nWith it users can built complex systems from simple components which connect together using standardized APIs.\nThe metaphor that guided the components package development was that of building a computer using off-the-shelf components.\nWhen building a computer, someone may select components, plug them into a board, and assume the interface between the board and the component have been designed in a way in which they will \u201cjust work.\u201d\nFor example, someone can remove a processor from a board and add a different one, compatible with the same socket, without needing to change everything else in their setup.\nWhile there are always limitations to this design philosophy, the components package has a highly modular and extensible design with components of the same type being interchangeable with one another as much as is possible.\nAt the core of the components package is the idea of a\nboard\n.\nThis plays a similar role to the motherboard in a real-world system.\nWhile it may contain embedded caches, controllers, and other complex components, its main purpose is to expose standardized interfaces for other hardware to be added and handle communication between them.\nFor example, a memory device and a processor may be added to a board with the board responsible for communication without the designer of the memory or the processor having to consider this assuming they conform to known APIs.\nTypically, a gem5 components package\nboard\nrequires declaration of these three components:\nThe\nprocessor\n: The system processor. A processor component contains at least one\ncore\nwhich may be Atomic, O3, Timing, or KVM.\nThe\nmemory\nsystem: The memory system, for example, a DDR3_1600.\nThe\ncache hierarchies\n: This component defines any and all components between the processor and main memory, most notably the cache setup. In the simplest of setups this will connect memory directly to the processor.\nThe other devices required for full-system simulation, which rarely change between simulations, are handled by the board.\nA typical usage of the components may therefore look like:\ncache_hierarchy\n=\nMESITwoLevelCacheHierarchy\n(\nl1d_size\n=\n\"16kB\"\n,\nl1d_assoc\n=\n8\n,\nl1i_size\n=\n\"16kB\"\n,\nl1i_assoc\n=\n8\n,\nl2_size\n=\n\"256kB\"\n,\nl2_assoc\n=\n16\n,\nnum_l2_banks\n=\n1\n,\n)\nmemory\n=\nSingleChannelDDR3_1600\n(\nsize\n=\n\"3GB\"\n)\nprocessor\n=\nSimpleProcessor\n(\ncpu_type\n=\nCPUTypes\n.\nTIMING\n,\nnum_cores\n=\n1\n)\nboard\n=\nX86Board\n(\nclk_freq\n=\n\"3GHz\"\n,\nprocessor\n=\nprocessor\n,\nmemory\n=\nmemory\n,\ncache_hierarchy\n=\ncache_hierarchy\n,\n)\nThe following tutorials go into greater detail on how to use the components package to create gem5 simulations.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/gem5-stdlib/overview",
            "page_title": "No Title Found",
            "parent_section": "The gem5 stdlib components package and its design philosophy",
            "section_heading": "Overview"
        }
    },
    {
        "text": "The gem5 resources package\nThe gem5 stdlib\u2019s resource package is used to obtain and incorporate resources.\nA resource, in the context of gem5, is something used in a simulation, or by a simulation, but not directly used to construct a system to be simulated.\nTypically these are applications, kernels, disk images, benchmarks, or tests.\nAs these resources can be hard to find, or hard to create, we provide pre-built resources as part of\ngem5-resources\n.\nFor example, via gem5-resources, a user may download an Ubuntu 18.04 disk image with known compatibility with gem5.\nThey need not setup this themselves.\nA core feature of the gem5 stdlib resource package is that it allows users to\nautomatically obtain\nprebuilt gem5 resources for their simulation.\nA user may specify in their Python config file that a specific gem5 resource is required and, when run, the package will check if there is a local copy on the host system, and if not, download it.\nThe tutorials will demonstrate how to use the resource package in greater detail, but for now, a typical pattern is as follows:\nfrom\ngem5.resources.resource\nimport\nResource\nresource\n=\nResource\n(\n\"riscv-disk-img\"\n)\nprint\n(\nf\n\"The resources is available at\n{\nresource\n.\nget_local_path\n()\n}\n\"\n)\nThis will obtain the\nriscv-disk-img\nresource and store it locally for use in a gem5 simulation.\nThe resources package references the resources that are available to view at the\ngem5 Resources website\nand the\ngem5 Resources repository\n. The website is strongly recommended to get info on what resources are available and where they may be downloaded from.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/gem5-stdlib/overview",
            "page_title": "No Title Found",
            "parent_section": "The gem5 resources package",
            "section_heading": "Overview"
        }
    },
    {
        "text": "The Simulate package\nThe simulate package is used to run gem5 simulations.\nWhile there is some boilerplate code this module handles on the users behalf, its primary purpose is to provde default behavior and APIs for what we refer to as\nExit Events\n.\nExit events are when a simulation exits for a particular reason.\nA typical example of an exit event would be a\nWorkbegin\nexit event.\nThis is used to specify that a Region-of-Interest (ROI) has been reached.\nUsually this exit would be used to allow a user to begin logging statistics or to switch to a more detailed CPU model.\nPrior to the stdlib, the user would need to specify precisely what the expected behavior was at exit events such as this.\nThe simulation would exit and the configuration script would contain Python code specifying what to do next.\nNow, with the simulate package, there is a default behavior for this kind of event (the stats are reset), and an easy interface to override this behavior with something the user requires.\nMore information about exit events can be found in the\nM5ops documentation\n.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/gem5-stdlib/overview",
            "page_title": "No Title Found",
            "parent_section": "The Simulate package",
            "section_heading": "Overview"
        }
    },
    {
        "text": "authors:\nHarshil Patel\nlast edited:\n2025-08-21 21:40:17 +0000\ngem5 supports using local data sources in the form of a MongoDB Atlas and JSON datasource. gem5 has a default resources config in\nsrc/python/gem5_default_config.py\n. This resources config points to the MongoDB Atlas collection of gem5 resources. To utilize data sources other than the main gem5 resources database, you will need to override the gem5-resources-config.\nThere are several ways to update the gem5 resources configuration:\nSetting GEM5_CONFIG environment variable:\nYou can set the GEM5_CONFIG environment variable to specify a new configuration file. Doing this will replace the default resources configuration with the one you\u2019ve specified.\nUsing gem5-config.json:\nIf a file named gem5-config.json exists in the current working directory, it will take precedence over the default resources configuration.\nFallback to default resources config:\nIf neither of the above methods is used, the system will resort to using the default resources configuration.\nAdditionally, if you wish to utilize or add a local resource JSON file to the currently selected config (as mentioned in the above methods), you have two additional methods available:\nGEM5_RESOURCE_JSON environment variable:\nThis variable can be employed to override the current resources configuration and make use of a specified JSON file.\nGEM5_RESOURCE_JSON_APPEND environment variable:\nUse this variable to add a JSON file to the existing resources configuration without replacing it.\nIt\u2019s essential to note that overriding or appending doesn\u2019t modify the actual configuration files themselves. These methods allow you to temporarily specify or add resource configurations during runtime without altering the original configuration files.\nMongoDB Atlas Config Format:\n{\n\"sources\"\n:{\n\"example-atlas-config\"\n:\n{\n\"dataSource\"\n:\n\"datasource name\"\n,\n\"database\"\n:\n\"database name\"\n,\n\"collection\"\n:\n\"collection name\"\n,\n\"url\"\n:\n\"Atlas data API URL\"\n,\n\"authUrl\"\n:\n\"Atlas authentication URL\"\n,\n\"apiKey\"\n:\n\"API key for data API for MongoDB Atlas\"\n,\n\"isMongo\"\n:\ntrue\n}\n}\n}\nJSON Config Format:\n{\n\"sources\"\n:{\n\"example-json-config\"\n:\n{\n\"url\"\n:\n\"local path to JSON file or URL to a JSON file\"\n,\n\"isMongo\"\n:\nfalse\n}\n}\n}\nSetting up a MongoDB Atlas Database\nYou would need to set up an Atlas cluster, steps on setting up an Atlas cluster can be found here:\nhttps://www.mongodb.com/basics/mongodb-atlas-tutorial\nYou would also need to enable Atlas dataAPI, steps on enabling dataAPI can be found here:\nhttps://www.mongodb.com/docs/atlas/app-services/data-api/generated-endpoints/\nUsing Multiple Data Sources\ngem5 supports the use of more than one data source. The structure of the resource configuration is as follows:\n{\n\"sources\"\n:\n{\n\"gem5-resources\"\n:\n{\n\"dataSource\"\n:\n\"gem5-vision\"\n,\n\"database\"\n:\n\"gem5-vision\"\n,\n\"collection\"\n:\n\"resources\"\n,\n\"url\"\n:\n\"https://data.mongodb-api.com/app/data-ejhjf/endpoint/data/v1\"\n,\n\"authUrl\"\n:\n\"https://realm.mongodb.com/api/client/v2.0/app/data-ejhjf/auth/providers/api-key/login\"\n,\n\"apiKey\"\n:\n\"OIi5bAP7xxIGK782t8ZoiD2BkBGEzMdX3upChf9zdCxHSnMoiTnjI22Yw5kOSgy9\"\n,\n\"isMongo\"\n:\ntrue\n,\n},\n\"data-source-json-1\"\n:\n{\n\"url\"\n:\n\"path/to/json\"\n,\n\"isMongo\"\n:\nfalse\n,\n},\n\"data-source-json-2\"\n:\n{\n\"url\"\n:\n\"path/to/another/json\"\n,\n\"isMongo\"\n:\nfalse\n,\n},\n//\nAdd\nmore\ndata\nsources\nas\nneeded\n}\n}\nThe above example shows a gem5 resources config with a MongoDB Atlas data source and 2 JSON data sources. By default gem5 will create a union of all the resources present in all the specified data sources. If you ask to obtain a resource where multiple data sources have the same\nid\nand\nresource_version\nof the resource then an error will be thrown. You can also specify a subset of data sources to obtain resources from:\nresource\n=\nobtain_resource\n(\n\"id\"\n,\nclients\n=\n[\n\"data-source-json-1\"\n])\nUnderstanding Local Resources\nLocal resources, in the context of gem5, pertain to resources that users possess and wish to integrate into gem5 but aren\u2019t pre-existing in the gem5 resources database.\nFor users, This offers the flexibility to employ their own resources seamlessly within gem5, bypassing the need to create dedicated resource objects using\nBinaryResource(local_path=/path/to/binary)\n. Instead, they can directly utilize these local resources through\nobtain_resource()\n, streamlining the integration process.\nUsing Custom Resource Configuration and Local Resources\nIn this example, we will walk through how to set up your custom configuration and utilize your own local resources. For this illustration, we\u2019ll employ a JSON file as our resource data source.\nCreating a Custom Resource Data Source\nLet\u2019s begin by creating a local resource. This is a bare bones resource that will serve as an example. To use local resources with\nobtain_resource()\n, our bare bones resource need to have a binary file. Here we use an empty binary called\nfake-binary\n.\nNote\n: Make sure that Gem5 binary and\nfake-binary\nhave same ISA target (RISCV here).\nNext, let\u2019s create the JSON data source. I\u2019ll name the file\nmy-resources.json\n. The contents should look like this:\n[\n{\n\"category\"\n:\n\"binary\"\n,\n\"id\"\n:\n\"test-binary\"\n,\n\"description\"\n:\n\"A test binary\"\n,\n\"architecture\"\n:\n\"RISCV\"\n,\n\"size\"\n:\n1\n,\n\"tags\"\n:\n[\n\"test\"\n],\n\"is_zipped\"\n:\nfalse\n,\n\"md5sum\"\n:\n\"6d9494d22b90d817e826b0d762fda973\"\n,\n\"source\"\n:\n\"src/simple\"\n,\n\"url\"\n:\n\"file:// path to fake_binary\"\n,\n\"license\"\n:\n\"\"\n,\n\"author\"\n:\n[],\n\"source_url\"\n:\n\"https://github.com/gem5/gem5-resources/tree/develop/src/simple\"\n,\n\"resource_version\"\n:\n\"1.0.0\"\n,\n\"gem5_versions\"\n:\n[\n\"23.0\"\n],\n\"example_usage\"\n:\n\"obtain_resource(resource_id=\n\\\"\ntest-binary\n\\\"\n)\"\n}\n]\nThe JSON file of a resource should adhere to the\ngem5 resources schema\n.\nNote\n: While the\nurl\nfield can be a link, in this case, I\u2019m using a local file.\nCreating Your Custom Resource Configuration\nCreate a file named\ngem5-config.json\nwith the following content:\n{\n\"sources\"\n:\n{\n\"my-json-data-source\"\n:\n{\n\"url\"\n:\n\"path/to/my-resources.json\"\n,\n\"isMongo\"\n:\nfalse\n}\n}\n}\nNote\n: It is implied that isMongo = false means that the data source is a JSON data source as gem5 currently only supports 2 types of data sources.\nRunning gem5 with a Local Data Source\nFirst, build gem5 with the ALL build, which contains RISCV:\nscons build/ALL/gem5.opt\n-j\n`\nnproc\n`\nNext, run the\nlocal-resource-example.py\nfile using our local\ntest-binary\nresource:\nUsing environment variable\nGEM5_RESOURCE_JSON_APPEND\n=\npath/to/my-resources.json ./build/ALL/gem5.opt configs/example/gem5_library/local-resource-example.py\n--resource\ntest-binary\nor you can overwrite the\ngem5_default_config\nwith our own custom config:\nGEM5_CONFIG\n=\npath/to/gem5-config.json ./build/ALL/gem5.opt configs/example/gem5_library/local-resource-example.py\n--resource\ntest-binary\nThis command will execute the\nlocal-resource-example.py\nscript using our locally downloaded resource. This script just calls the obtain_resource function and prints the local path of the resource. This script indicates that local resources function similarly as resources on the gem5 resources database.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/gem5-stdlib/using-local-resources",
            "page_title": "No Title Found",
            "parent_section": "Overview",
            "section_heading": "using-local-resources"
        }
    },
    {
        "text": "Building an x86 full-system simulation with the gem5 standard library\nOne of the key ideas behind the gem5 standard library is to allow users to simulate, big, complex systems, with minimal effort.\nThis is done by making sensible assumptions about the nature of the system to simulate and connecting components in a manner which \u201cmakes sense.\u201d\nWhile this takes away some flexibility, it massively simplifies simulating typical hardware setups in gem5.\nThe overarching philosophy is to make the\ncommon case\nsimple.\nIn this tutorial we will build an X86 simulation, capable of running a full-system simulation, booting an Ubuntu operating system, and running a benchmark.\nThis system will utilize gem5\u2019s ability to switch cores, allowing booting of the operating system in KVM fast-forward mode and switching to a detailed CPU model to run the benchmark, and use a MESI Two Level Ruby cache hierarchy in a dual-core setup.\nWithout using the gem5 library this would take several hundred lines of Python, forcing the user to specify details such as every IO component and exactly how the cache hierarchy is setup.\nHere, we will demonstrate how simple this task can be with using the gem5 standard library.\nFirst, we build the ALL binary. This will allow us to run simulations for any ISA, including X86:\nscons build/ALL/gem5.opt\n-j\n<number of threads>\nIf you are using a prebuilt gem5 binary, this step is not necessary.\nTo start, create a new Python file.\nWe will refer to this as\nx86-ubuntu-run.py\n.\nTo begin we add our import statements:\nfrom\ngem5.coherence_protocol\nimport\nCoherenceProtocol\nfrom\ngem5.components.boards.x86_board\nimport\nX86Board\nfrom\ngem5.components.cachehierarchies.ruby.mesi_two_level_cache_hierarchy\nimport\n(\nMESITwoLevelCacheHierarchy\n,\n)\nfrom\ngem5.components.memory.single_channel\nimport\nSingleChannelDDR3_1600\nfrom\ngem5.components.processors.cpu_types\nimport\nCPUTypes\nfrom\ngem5.components.processors.simple_switchable_processor\nimport\n(\nSimpleSwitchableProcessor\n,\n)\nfrom\ngem5.isas\nimport\nISA\nfrom\ngem5.resources.resource\nimport\nobtain_resource\nfrom\ngem5.simulate.exit_event\nimport\nExitEvent\nfrom\ngem5.simulate.simulator\nimport\nSimulator\nfrom\ngem5.utils.requires\nimport\nrequires\nAs in other Python scripts, these are simply classes/functions needed in our script.\nThey are all included as part of the gem5 binary and therefore do not need to obtained elsewhere.\nA good start is to use the\nrequires\nfunction to specify what kind of gem5 binary/setup is required to run the script:\nrequires\n(\nisa_required\n=\nISA\n.\nX86\n,\ncoherence_protocol_required\n=\nCoherenceProtocol\n.\nMESI_TWO_LEVEL\n,\nkvm_required\n=\nTrue\n,\n)\nHere we state that we need gem5 compiled to run the X86 ISA and support the MESI Two Level protocol.\nWe also require the host system to have KVM.\nNOTE: Please ensure your host system supports KVM. If your system does not please remove the\nkvm_required\ncheck here\n.\nKVM will only work if the host platform and the simulated ISA are the same (e.g., X86 host and X86 simulation). You can learn more about using KVM with gem5\nhere\n.\nThis\nrequires\ncall is not required but provides a good safety net to those running the script.\nErrors that occur due to incompatible gem5 binaries may not make much sense otherwise.\nNext we start specifying the components in our system.\nWe start with the\ncache hierarchy\n:\ncache_hierarchy\n=\nMESITwoLevelCacheHierarchy\n(\nl1d_size\n=\n\"32KiB\"\n,\nl1d_assoc\n=\n8\n,\nl1i_size\n=\n\"32KiB\"\n,\nl1i_assoc\n=\n8\n,\nl2_size\n=\n\"256KiB\"\n,\nl2_assoc\n=\n16\n,\nnum_l2_banks\n=\n1\n,\n)\nHere we setup a MESI Two Level (ruby) cache hierarchy.\nVia the constructor we set the L1 data cache and L1 instruction cache to 32 KiB, and the L2 cache to 256 KiB.\nNext we setup the\nmemory system\n:\nmemory\n=\nSingleChannelDDR3_1600\n(\nsize\n=\n\"2GiB\"\n)\nThis is quite simple and should be intuitive: A single channel DDR3 1600 setup of size 2GiB.\nNote:\nby default the\nSingleChannelDDR3_1600\ncomponent has a size of 8GiB.\nHowever, due to\na known limitation with the X86Board\n, we cannot use a memory system greater than 3GiB.\nWe therefore must set the size.\nNext we setup the\nprocessor\n:\nprocessor\n=\nSimpleSwitchableProcessor\n(\nstarting_core_type\n=\nCPUTypes\n.\nKVM\n,\nswitch_core_type\n=\nCPUTypes\n.\nTIMING\n,\nisa\n=\nISA\n.\nX86\n,\nnum_cores\n=\n2\n,\n)\nHere we are utilizing the gem5 standard library\u2019s special\nSimpleSwitchableProcessor\n.\nThis processor can be used for simulations in which a user wants to switch out one type of core for another during a simulation.\nThe\nstarting_core_type\nparameter specifies which CPU type to start a simulation with.\nIn this case a KVM core.\n(Note: If your host system does not support KVM, this simulation will not run. You must change this to another CPU type, such as\nCPUTypes.ATOMIC\n)\nThe\nswitch_core_type\nparameter specifies which CPU type to switch to in a simulation.\nIn this case we\u2019ll be switching from KVM cores to TIMING cores.\nThe final parameter,\nnum_cores\n, specifies the number of cores within the processor.\nWith this processor a user can call\nprocessor.switch()\nto switch to and from the starting cores and the switch cores, which we will demonstrate later on in this tutorial.\nNext we add these components to the\nboard\n:\nboard\n=\nX86Board\n(\nclk_freq\n=\n\"3GHz\"\n,\nprocessor\n=\nprocessor\n,\nmemory\n=\nmemory\n,\ncache_hierarchy\n=\ncache_hierarchy\n,\n)\nHere we use the\nX86Board\n.\nThis is a board used to simulate a typical X86 system in full-system mode.\nAs a minimum, the board needs the\nclk_freq\n,\nprocessor\n,\nmemory\n, and\ncache_hierarchy\nparameters specified.\nThis finalizes our system design.\nNow we set the workload to run on the system:\nworkload\n=\nobtain_resource\n(\n\"x86-ubuntu-24.04-boot-with-systemd\"\n)\nboard\n.\nset_workload\n(\nworkload\n)\nThe\nobtain_resource\nfunction acquires the X86 Ubuntu 24.04 boot workload.\nThis workload contains a kernel resource, parameters to the kernel, a disk image resource, and a string indicating the underlying function that gem5 uses when\nboard.set_workload()\nis called.\nYou can see these details under the\nRaw\ntab of of the gem5 Resources website page for this workload.\nYou can also use\nset_kernel_disk_workload()\ninstead of\nset_workload()\nand set the disk image and kernel resources separately.\nThis can be used when you want to use your own resources, or a combination of resources that is not provided as a workload on\nthe gem5 resources website\n.\nNote: If a user wishes to use their own resource (that is, a resource not prebuilt as part of gem5-resources), they may follow the tutorial\nhere\n. A tutorial is also available at the\n2024 gem5 bootcamp website\nWhen using the\nset_kernel_disk_workload()\nfunction, you can also pass an optional\nreadfile_contents\nargument.\nThis will be run as a bash script after the system boots up, and can be used to launch a benchmark after the system boots if the disk image has benchmarks installed.\nAn example can be found\nhere\nFinally, we specify how the simulation is to be run with the following:\ndef\nexit_event_handler\n():\nprint\n(\n\"First exit: kernel booted\"\n)\nyield\nFalse\n# gem5 is now executing systemd startup\nprint\n(\n\"Second exit: Started `after_boot.sh` script\"\n)\n# The after_boot.sh script is executed after the kernel and systemd have\n# booted.\n# Here we switch the CPU type to Timing.\nprint\n(\n\"Switching to Timing CPU\"\n)\nprocessor\n.\nswitch\n()\nyield\nFalse\n# gem5 is now executing the `after_boot.sh` script\nprint\n(\n\"Third exit: Finished `after_boot.sh` script\"\n)\n# The after_boot.sh script will run a script if it is passed via\n# readfile_contents. This is the last exit event before the simulation exits.\nyield\nTrue\nsimulator\n=\nSimulator\n(\nboard\n=\nboard\n,\non_exit_event\n=\n{\nExitEvent\n.\nEXIT\n:\nexit_event_handler\n(),\n},\n)\nsimulator\n.\nrun\n()\nThe important thing to note here is the\non_exit_event\nargument.\nHere we can override default behavior.\nThe\non_exit_event\nparameter is a Python dictionary of exit events and\nPython generators\n.\nIn this tutorial we use the\nexit_event_handler\ngenerator to handle exit events of the type\nExitEvent.EXIT\n.\nThere are three\nEXIT\nexit events in the Ubuntu 24.04 disk image resource used by the workload.\nIf an exit event handler is not defined, the simulation will end after the first exit event, which takes place after the kernel finishes booting.\nYielding\nFalse\nallows the simulation to continue, while yielding\nTrue\nends the simulation.\nAfter the second exit event, we switch the cores from KVM to TIMING, then yield\nFalse\nto continue the simulation.\nAfter the third exit event, we yield\nTrue\n, ending the simulation.\nThis completes the setup of our script. To execute the script we run:\n./build/ALL/gem5.opt x86-ubuntu-run.py\nIf you are using a pre-built binary, you can execute the simulation with:\ngem5 hello-world.py\nYou can see the output of the simulator in\nm5out/system.pc.com_1.device\n.\nBelow is the configuration script in full.\nIt mirrors closely the example script at\nconfigs/example/gem5_library/x86-ubuntu-run-with-kvm.py\nin the gem5 repository.\nfrom\ngem5.coherence_protocol\nimport\nCoherenceProtocol\nfrom\ngem5.components.boards.x86_board\nimport\nX86Board\nfrom\ngem5.components.cachehierarchies.ruby.mesi_two_level_cache_hierarchy\nimport\n(\nMESITwoLevelCacheHierarchy\n,\n)\nfrom\ngem5.components.memory.single_channel\nimport\nSingleChannelDDR3_1600\nfrom\ngem5.components.processors.cpu_types\nimport\nCPUTypes\nfrom\ngem5.components.processors.simple_switchable_processor\nimport\n(\nSimpleSwitchableProcessor\n,\n)\nfrom\ngem5.isas\nimport\nISA\nfrom\ngem5.resources.resource\nimport\nobtain_resource\nfrom\ngem5.simulate.exit_event\nimport\nExitEvent\nfrom\ngem5.simulate.simulator\nimport\nSimulator\nfrom\ngem5.utils.requires\nimport\nrequires\nrequires\n(\nisa_required\n=\nISA\n.\nX86\n,\ncoherence_protocol_required\n=\nCoherenceProtocol\n.\nMESI_TWO_LEVEL\n,\nkvm_required\n=\nTrue\n,\n)\ncache_hierarchy\n=\nMESITwoLevelCacheHierarchy\n(\nl1d_size\n=\n\"32KiB\"\n,\nl1d_assoc\n=\n8\n,\nl1i_size\n=\n\"32KiB\"\n,\nl1i_assoc\n=\n8\n,\nl2_size\n=\n\"256KiB\"\n,\nl2_assoc\n=\n16\n,\nnum_l2_banks\n=\n1\n,\n)\nmemory\n=\nSingleChannelDDR3_1600\n(\nsize\n=\n\"2GiB\"\n)\nprocessor\n=\nSimpleSwitchableProcessor\n(\nstarting_core_type\n=\nCPUTypes\n.\nKVM\n,\nswitch_core_type\n=\nCPUTypes\n.\nTIMING\n,\nisa\n=\nISA\n.\nX86\n,\nnum_cores\n=\n2\n,\n)\nboard\n=\nX86Board\n(\nclk_freq\n=\n\"3GHz\"\n,\nprocessor\n=\nprocessor\n,\nmemory\n=\nmemory\n,\ncache_hierarchy\n=\ncache_hierarchy\n,\n)\nworkload\n=\nobtain_resource\n(\n\"x86-ubuntu-24.04-boot-with-systemd\"\n)\nboard\n.\nset_workload\n(\nworkload\n)\ndef\nexit_event_handler\n():\nprint\n(\n\"First exit: kernel booted\"\n)\nyield\nFalse\n# gem5 is now executing systemd startup\nprint\n(\n\"Second exit: Started `after_boot.sh` script\"\n)\n# The after_boot.sh script is executed after the kernel and systemd have\n# booted.\n# Here we switch the CPU type to Timing.\nprint\n(\n\"Switching to Timing CPU\"\n)\nprocessor\n.\nswitch\n()\nyield\nFalse\n# gem5 is now executing the `after_boot.sh` script\nprint\n(\n\"Third exit: Finished `after_boot.sh` script\"\n)\n# The after_boot.sh script will run a script if it is passed via\n# readfile_contents. This is the last exit event before the simulation exits.\nyield\nTrue\nsimulator\n=\nSimulator\n(\nboard\n=\nboard\n,\non_exit_event\n=\n{\nExitEvent\n.\nEXIT\n:\nexit_event_handler\n(),\n},\n)\nsimulator\n.\nrun\n()\nTo recap what we learned in this tutorial:\nThe\nrequires\nfunction can be used to specify the gem5 and host requirements for a script.\nThe\nSimpleSwitchableProcessor\ncan be used to create a setup in which cores can be switched out for others.\nThe\nX86Board\ncan be used to set up full-system simulations.\nIts workload can be set via either\nset_workload()\nfor workload resources, or via\nset_kernel_disk_workload()\nfor separate kernel and disk image resources.\nThe\nset_kernel_disk_workload()\nfunction accepts a\nreadfile_contents\nargument.\nThis is processed as a script to be executed after the system boot is complete.\nThe\nSimulator\nmodule allows for the overriding of exit events using Python generators.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/gem5-stdlib/x86-full-system-tutorial",
            "page_title": "No Title Found",
            "parent_section": "Building an x86 full-system simulation with the gem5 standard library",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Alpha\nGem5 models a DEC Tsunami based system. \nIn addition to the normal Tsunami system that support 4 cores, we have an extension which supports 64 cores (a custom PALcode and patched Linux kernel is required). \nThe simulated system looks like an Alpha 21264 including the BWX, MVI, FIX, and CIX to user level code. \nFor historical reasons the processor executes EV5 based PALcode.\nIt can boot unmodified Linux 2.4/2.6, FreeBSD, or L4Ka::Pistachio as well as applications in syscall emulation mode. \nMany years ago it was possible to boot HP/Compaq\u2019s Tru64 5.1 operating system. \nWe no longer actively maintain that capability, however, and it does not currently work.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/architecture_support/",
            "page_title": "Architecture Support",
            "parent_section": "Alpha",
            "section_heading": "Overview"
        }
    },
    {
        "text": "ARM\nThe ARM Architecture models within gem5 support an\nARMv8-A\nprofile of the ARM\u00ae architecture with multi-processor extensions. \nThis includes both AArch32 and AArch64 state. \nIn AArch32, this include support for\nThumb\u00ae\n, Thumb-2, VFPv3 (32 double register variant) and\nNEON\u2122\n, and Large Physical Address Extensions (LPAE). \nOptional features of the architecture that are not currently supported are\nTrustZone\u00ae\n, ThumbEE,\nJazelle\u00ae\n, and\nVirtualization\n.\nIn full system mode gem5 is able to boot uni- or multi-processor Linux and bare metal applications built with ARM\u2019s compilers. \nNewer Linux versions work out of the box (if used with gem5\u2019s DTBs) we also provide gem5-specific Linux kernels with custom configurations and custom drivers Additionally, statically linked Linux binaries can be run in ARM\u2019s syscall emulation mode.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/architecture_support/",
            "page_title": "Architecture Support",
            "parent_section": "ARM",
            "section_heading": "Overview"
        }
    },
    {
        "text": "POWER\nSupport for the POWER ISA within gem5 is currently limited to syscall emulation only and is based on the\nPOWER ISA v2.06 B Book\n.\nA big-endian, 32-bit processor is modeled. \nMost common instructions are available (enough to run all the SPEC CPU2000 integer benchmarks). \nFloating point instructions are available but support may be patchy. \nIn particular, the Floating-Point Status and Control Register (FPSCR) is generally not updated at all. \nThere is no support for vector instructions.\nFull system support for POWER would require a significant amount of effort and is not currently being developed. \nHowever, if there is interest in pursuing this, a set of patches-in-progress that make a start towards this can be obtained from\nTim\n.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/architecture_support/",
            "page_title": "Architecture Support",
            "parent_section": "POWER",
            "section_heading": "Overview"
        }
    },
    {
        "text": "SPARC\nThe gem5 simulator models a single core of a UltraSPARC T1 processor (UltraSPARC Architecture 2005).\nIt can boot Solaris like the Sun T1 Architecture simulator tools do (building the hypervisor with specific defines and using the HSMID virtual disk driver). \nMultiprocessor support was never completed for full-system SPARC. \nWith syscall emulation gem5 supports running Linux or Solaris binaries. \nNew versions of Solaris no longer support generating statically compiled binaries which gem5 requires.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/architecture_support/",
            "page_title": "Architecture Support",
            "parent_section": "SPARC",
            "section_heading": "Overview"
        }
    },
    {
        "text": "x86\nX86 support within the gem5 simulator includes a generic x86 CPU with 64 bit extensions, more similar to AMD\u2019s version of the architecture than Intel\u2019s but not strictly like either. \nUnmodified versions of the Linux kernel can be booted in UP and SMP configurations, and patches are available for speeding up boot. \nSSE and 3dnow are implemented, but the majority of x87 floating point is not. \nMost effort has been focused on 64 bit mode, but compatibility mode and legacy modes have some support as well. \nReal mode works enough to bootstrap an AP, but hasn\u2019t been extensively tested. \nThe features of the architecture that are exercised by Linux and standard Linux binaries are implemented and should work, but other areas may not. \n64 and 32 bit Linux binaries are supported in syscall emulation mode.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/architecture_support/",
            "page_title": "Architecture Support",
            "parent_section": "x86",
            "section_heading": "Overview"
        }
    },
    {
        "text": "MIPS",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/architecture_support/",
            "page_title": "Architecture Support",
            "parent_section": "MIPS",
            "section_heading": "Overview"
        }
    },
    {
        "text": "RISC-V",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/architecture_support/",
            "page_title": "Architecture Support",
            "parent_section": "RISC-V",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Section: Supported features and modes\n\nSub-section: From gem5 v21.2\n\nThe best way to get a synced version of Arm architectural features is to have a look at the\nArmExtension\nenum\nused by the release object and the available example releases provided within the same file.\nA user can choose one of the following options:\nUse the default release\nUse another example release (e.g. Armv82)\nGenerate a custom release from the available ArmExtension enum values",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/architecture_support/arm_implementation/",
            "page_title": "ARM Implementation",
            "parent_section": "Supported features and modes",
            "section_heading": "From gem5 v21.2"
        }
    },
    {
        "text": "Section: Supported features and modes\n\nSub-section: Before gem5 v21.2\n\nThe best way to get a synced version of Arm architectural features is to have a look at Arm ID registers and boolean values:\nsrc/arch/arm/ArmISA.py\nsrc/arch/arm/ArmSystem.py",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/architecture_support/arm_implementation/",
            "page_title": "ARM Implementation",
            "parent_section": "Supported features and modes",
            "section_heading": "Before gem5 v21.2"
        }
    },
    {
        "text": "Section: The decode section\n\nSub-section: Specifying instruction formats\n\nWhen the ISA description file is processed, each instruction definition does in fact invoke a function call to generate the appropriate C++ code for the decode file. The function that is invoked is determined by the instruction format. The instruction format determines the number and type of the arguments given to the instruction definition, and how they are processed to generate the corresponding output. Note that the term \u201cinstruction format\u201d as used in this context refers solely to one of these definition-processing functions, and does not necessarily map one-to-one to the machine instruction formats defined by the ISA.\nThe one oversimplification in the previous example is that no instruction format was specified. As a result, the parser does not know how to process the instruction definitions.\nInstruction formats can be specified in two ways. An explicit format specification can be given before the mnemonic, separated by a double colon (::), as follows:\ndecode OPCODE {\n  0: Integer::add({{ Rc = Ra + Rb; }});\n  1: Integer::sub({{ Rc = Ra - Rb; }});\n}\nIn this example, both instruction definitions will be processed using the format Integer. A more common approach specifies the format for a set of definitions using a format block, as follows:\ndecode OPCODE {\n  format Integer {\n    0: add({{ Rc = Ra + Rb; }});\n    1: sub({{ Rc = Ra - Rb; }});\n  }\n}\nIn this example, the format \u201cInteger\u201d applies to all of the instruction definitions within the inner braces. The two examples are thus functionally equivalent. There are few restrictions on the use of format blocks. A format block may include only a subset of the statements in a decode block. Format blocks and explicit format specifications may be mixed freely, with the latter taking precedence. Format and decode blocks can be nested within each other arbitrarily. Note that a closing brace will always bind with the nearest format or decode block, making it syntactically impossible to generate format or decode blocks that do not nest fully inside the enclosing block.\nAt any point where an instruction definition occurs without an explicit format specification, the format associated with the innermost enclosing format block will be used. If a definition occurs with no explicit format and no enclosing format block, a runtime error will be raised.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/architecture_support/isa_parser/",
            "page_title": "ISA Parser",
            "parent_section": "The decode section",
            "section_heading": "Specifying instruction formats"
        }
    },
    {
        "text": "Section: The decode section\n\nSub-section: Decode block defaults\n\nDefault cases for decode blocks can be specified by\ndefault:\nlabels, as in C switch statements. However, it is common in ISA descriptions that unspecified cases correspond to unknown or illegal instruction encodings. To avoid the requirement of a\ndefault:\ncase in every decode block, the language allows an alternate default syntax that specifies a default case for the current decode block and any nested decode block with no explicit default. This alternate default is specified by giving the\ndefault\nkeyword and an instruction definition after the bitfield specification (prior to the opening brace). Specifying the outermost decode block as follows:\ndecode OPCODE default Unknown::unknown() {\n   [...]\n}\nis thus (nearly) equivalent to adding\ndefault: Unknown::unknown();\ninside every decode block that does not otherwise specify a default case.\nNote: The appropriate format definition (see _\nFormat definitions\n) is invoked each time an instruction definition is encountered.  Thus there is a semantic difference between having a single block-level default and a default within each nested block, which is that the former will invoke the format definition once, while the latter could result in multiple invocations of the format definition.  If the format definition generates header, decoder, or exec output, then that output will be included multiple times in the corresponding files, which typically leads to multiple definition errors when the C++ gets compiled.  If it is absolutely necessary to invoke the format definition for a single instruction multiple times, the format definition should be written to produce\nonly\ndecode-block output, and all needed header, decoder, and exec output should be produced once using_\noutput\nblocks (see _\nOutput blocks\n)._",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/architecture_support/isa_parser/",
            "page_title": "ISA Parser",
            "parent_section": "The decode section",
            "section_heading": "Decode block defaults"
        }
    },
    {
        "text": "Section: The decode section\n\nSub-section: Preprocessor directive handling\n\nThe decode block may also contain C preprocessor directives. These directives are not processed by the parser; instead, they are passed through to the C++ output to be processed when the C++ decoder is compiled. The parser does not recognize any specific directives; any line with a # in the first column is treated as a preprocessor directive.\nThe directives are copied to all of the output streams (the header, the decoder, and the execute files; see\nFormat definitions\n. The directives maintain their position relative to the code generated by the instruction definitions within the decode block. The net result is that, for example, #ifdef/#endif pairs that surround a set of instruction definitions will enclose both the declarations generated by those definitions and the corresponding case statements within the decode function. Thus #ifdef and similar constructs can be used to delineate instruction definitions that will be conditionally compiled into the simulator based on preprocessor symbols (e.g., FULL_SYSTEM). It should be emphasized that #ifdef does not affect the ISA description parser. In an #ifdef/#else/#endif construct, all of the instruction definitions in both parts of the conditional will be processed. Only during the subsequent C++ compilation of the decoder will one or the other set of definitions be selected.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/architecture_support/isa_parser/",
            "page_title": "ISA Parser",
            "parent_section": "The decode section",
            "section_heading": "Preprocessor directive handling"
        }
    },
    {
        "text": "Section: The declaration section\n\nSub-section: Format definitions\n\nAn instruction format is basically a Python function that takes the arguments supplied by an instruction definition (found inside a decode block) and generates up to four pieces of C++ code. The pieces of C++ code are distinguished by where they appear in the generated output.\nThe \u2018\u2018header output\u2019\u2019 goes in the header file (decoder.hh) that is included in all the generated source files (decoder.cc and all the per-CPU-model execute .cc files). The header output typically contains the C++ class declaration(s) (if any) that correspond to the instruction.\nThe \u2018\u2018decoder output\u2019\u2019 goes before the decode function in the same source file (decoder.cc). This output typically contains definitions that do not need to be visible to the\nexecute()\nmethods: inline constructor definitions, non-inline method definitions (e.g., for disassembly), etc.\nThe \u2018\u2018exec output\u2019\u2019 contains per-CPU model definitions, i.e., the\nexecute()\nmethods for the instruction class.\nThe \u2018\u2018decode block\u2019\u2019 contains a statement or block of statements that go into the decode function (in the body of the corresponding case statement). These statements take control once the bit pattern specified by the decode block is recognized, and are responsible for returning an appropriate instruction object.\nThe syntax for defining an instruction format is as follows:\ndef format FormatName(arg1, arg2) {{\n    [code omitted]\n}};\nIn this example, the format is named \u201cFormatName\u201d. (By convention, instruction format names begin with a capital letter and use mixed case.) Instruction definitions using this format will be expected to provide two arguments (\narg1\nand\narg2\n). The language also supports the Python variable-argument mechanism: if the final parameter begins with an asterisk (e.g.,\n*rest\n), it receives a list of all the otherwise unbound arguments from the call site.\nNote that the next-to-last syntactic token in the format definition (prior to the semicolon) is simply a code literal (string constant), as described above. In this case, the text within the code literal is a Python code block. This Python code will be called at each instruction definition that uses the specified format.\nIn addition to the explicit arguments, the Python code is supplied with two additional parameters:\nname\n, which is bound to the instruction mnemonic, and\nName\n, which is the mnemonic with the first letter capitalized (useful for forming C++ class names based on the mnemonic).\nThe format code block specifies the generated code by assigning strings to four special variables:\nheader_output\n,\ndecoder_output\n,\nexec_output\n, and\ndecode_block\n. Assignment is optional; for any of these variables that does not receive a value, no code will be generated for the corresponding section. These strings may be generated by whatever method is convenient. In practice, nearly all instruction formats use the support functions provided by the ISA description parser to specialize code templates based on characteristics extracted automatically from C-like code snippets. Discussion of these features is deferred to the\nCode parsing\npage.\nAlthough the ISA description is completely independent of any specific simulator CPU model, some C++ code (particularly the exec output) must be specialized slightly for each model. This specialization is handled by automatic substitution of CPU-model-specific symbols. These symbols start with\nCPU_\nand are treated specially by the parser. Currently there is only one model-specific symbol,\nCPU_exec_context\n, which evaluates to the model\u2019s execution context class name. As with templates (see\nTemplate definitions\n), references to CPU-specific symbols use Python key-based format strings; a reference to the\nCPU_exec_context\nsymbol thus appears in a string as\n%(CPU_exec_context)s\n.\nIf a string assigned to\nheader_output\n,\ndecoder_output\n, or\ndecode_block\ncontains a CPU-specific symbol reference, the string is replicated once for each CPU model, and each instance has its CPU-specific symbols substituted according to that model. The resulting strings are then concatenated to form the final output. Strings assigned to\nexec_output\nare always replicated and subsituted once for each CPU model, regardless of whether they contain CPU-specific symbol references. The instances are not concatenated, but are tracked separately, and are placed in separate per-CPU-model files (e.g., simple_cpu_exec.cc).",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/architecture_support/isa_parser/",
            "page_title": "ISA Parser",
            "parent_section": "The declaration section",
            "section_heading": "Format definitions"
        }
    },
    {
        "text": "Section: The declaration section\n\nSub-section: Template definitions\n\nAs discussed in section Format definitions above, the purpose of an instruction format is to process the arguments of an instruction definition and generate several pieces of C++ code. These code pieces are usually generated by specializing a code template. The description language provides a simple syntax for defining these templates: the keywords\ndef template\n, the template name, the template body (a code literal), and a semicolon. By convention, template names start with a capital letter, use mixed case, and end with \u201cDeclare\u201d (for declaration (header output) templates), \u201cDecode\u201d (for decode-block templates), \u201cConstructor\u201d (for decoder output templates), or \u201cExecute\u201d (for exec output templates).\nFor example, the simplest useful decode template is as follows:\ndef template BasicDecode {{\n    return new %(class_name)s(machInst);\n}};\nAn instruction format would specialize this template for a particular instruction by substituting the actual class name for\n%(class_name)s\n. (Template specialization relies on the Python string format operator\n%\n. The term\n%(class_name)s\nis an extension of the C\n%s\nformat string indicating that the value of the symbol\nclass_name\nshould be substituted.) The resulting code would then cause the C++ decode function to create a new object of the specified class when the particular instruction was recognized.\nTemplates are represented in the parser as Python objects. A template is used to generate a string typically by calling the template object\u2019s\nsubst()\nmethod. This method takes a single argument that specifies the mapping of substitution symbols in the template (e.g.,\n%(class_name)s\n) to specific values. If the argument is a dictionary, the dictionary itself specifies the mapping. Otherwise, the argument must be another Python object, and the object\u2019s attributes are used as the mapping. In practice, the argument to\nsubst()\nis nearly always an instance of the parser\u2019s InstObjParams class; see the\nInstObjParams class\n. A template may also reference other templates (e.g.,\n%(BasicDecode)s\n) in addition to symbols specified by the\nsubst()\nargument; these will be interpolated into the result by\nsubst()\nas well.\nTemplate references to CPU-model-specific symbols (see\nFormat definitions\n) are not expanded by\nsubst()\n, but are passed through intact. This feature allows them to later be expanded appropriately according to whether the result is assigned to\nexec_output\nor another output section. However, when a template containing a CPU-model-specific symbol is referenced by another template, then the former template is replicated and expanded into a single string before interpolation, as with templates assigned to\nheader_output\nor\ndecoder_output\n. This policy guarantees that only templates directly containing CPU-model-specific symbols will be replicated, never templates that contain such symbols indirectly. This last feature is used to interpolate per-CPU declarations of the\nexecute()\nmethod into the instruction class declaration template (see the\nBasicExecDeclare\ntemplate in the Alpha ISA description).",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/architecture_support/isa_parser/",
            "page_title": "ISA Parser",
            "parent_section": "The declaration section",
            "section_heading": "Template definitions"
        }
    },
    {
        "text": "Section: The declaration section\n\nSub-section: Output blocks\n\nOutput blocks allow the ISA description to include C++ code that is copied nearly verbatim to the output file. These blocks are useful for defining classes and local functions that are shared among multiple instruction objects. An output block has the following format:\noutput <destination> {{\n    [code omitted]\n}};\nThe\n<destination>\nkeyword must be one of\nheader\n,\ndecoder\n, or\nexec\n. The code within the code literal is treated as if it were assigned to the\nheader_output\ndecoder_output\n, or\nexec_output\nvariable within an instruction format, respectively, including the special processing of CPU-model-specific symbols. The only additional processing performed on the code literal is substitution of bitfield operators, as used in instruction definitions (see\nBitfield operators\n, and interpolation of references to templates.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/architecture_support/isa_parser/",
            "page_title": "ISA Parser",
            "parent_section": "The declaration section",
            "section_heading": "Output blocks"
        }
    },
    {
        "text": "Section: The declaration section\n\nSub-section: Let blocks\n\nLet blocks provide for global Python code. These blocks consist simply of the keyword\nlet\nfollowed by a code literal (double-brace delimited string) and a semicolon.\nThe code literal is executed immediately by the Python interpreter. The parser maintains the execution context across let blocks, so that variables and functions defined in one let block will be accessible in subsequent let blocks. This context is also used when executing instruction format definitions. The primary purpose of let blocks is to define shared Python data structures and functions for use in instruction formats. The parser exports a limited set of definitions into this execution context, including the set of defined templates (see\nTemplate definitions\n, the\nInstObjParams\nand\nCodeBlock\nclasses (see\nCode parsing\n), and the standard Python\nstring\nand\nre\n(regular expression) modules.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/architecture_support/isa_parser/",
            "page_title": "ISA Parser",
            "parent_section": "The declaration section",
            "section_heading": "Let blocks"
        }
    },
    {
        "text": "Section: The declaration section\n\nSub-section: Bitfield definitions\n\nA bitfield definition provides a name for a bitfield within a machine instruction. These names are typically used as the bitfield specifications in decode blocks. The names are also used within other C++ code in the decoder file, including instruction class definitions and decode code.\nThe bitfield definition syntax is demonstrated in these examples:\ndef bitfield OPCODE <31:26>;\ndef bitfield IMM <12>;\ndef signed bitfield MEMDISP <15:0>;\nThe specified bit range is inclusive on both ends, and bit 0 is the least significant bit; thus the OPCODE bitfield in the example extracts the most significant six bits from a 32-bit instruction. A single index value extracts a one-bit field, IMM. The extracted value is zero-extended by default; with the additional signed keyword, as in the MEMDISP example, the extracted value will be sign extended. The implementation of bitfields is based on preprocessor macros and C++ template functions, so the size of the resulting value will depend on the context.\nTo fully understand where bitfield definitions can be used, we need to go under the hood a bit. A bitfield definition simply generates a C++ preprocessor macro that extracts the specified bitfield from the implicit variable\nmachInst\n. The machine instruction parameter to the decode function is also called\nmachInst\n; thus any use of a bitfield name that ends up inside the decode function (such as the argument of a decode block or the decode piece of an instruction format\u2019s output) will implicitly reference the instruction currently being decoded. The binary machine instruction stored in the\nStaticInst\nobject is also named\nmachInst\n, so any use of a bitfield name in a member function of an instruction object will reference this stored value. This data member is initialized in the\nStaticInst\nconstructor, so it is safe to use bitfield names even in the constructors of derived objects.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/architecture_support/isa_parser/",
            "page_title": "ISA Parser",
            "parent_section": "The declaration section",
            "section_heading": "Bitfield definitions"
        }
    },
    {
        "text": "Section: The declaration section\n\nSub-section: Operand and operand type definitions\n\nThese statements specify the operand types that can be used in the code blocks that express the functional operation of instructions. See\nOperand type qualifiers\nand\nInstruction parsing\n.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/architecture_support/isa_parser/",
            "page_title": "ISA Parser",
            "parent_section": "The declaration section",
            "section_heading": "Operand and operand type definitions"
        }
    },
    {
        "text": "Section: The declaration section\n\nSub-section: Namespace declaration\n\nThe final component of the declaration section is the namespace declaration, consisting of the keyword\nnamespace\nfollowed by an identifier and a semicolon. Exactly one namespace declaration must appear in the declarations section. The resulting C++ decode function, the declarations resulting from the instruction definitions in the decode block, and the contents of any\ndeclare\nstatements occurring after then namespace declaration will be placed in a C++ namespace with the specified name. The contents of\ndeclare\nstatements occurring before the namespace declaration will be outside the namespace.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/architecture_support/isa_parser/",
            "page_title": "ISA Parser",
            "parent_section": "The declaration section",
            "section_heading": "Namespace declaration"
        }
    },
    {
        "text": "Section: ISA parser\n\nSub-section: Formats",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/architecture_support/isa_parser/",
            "page_title": "ISA Parser",
            "parent_section": "ISA parser",
            "section_heading": "Formats"
        }
    },
    {
        "text": "Section: ISA parser\n\nSub-section: operands",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/architecture_support/isa_parser/",
            "page_title": "ISA Parser",
            "parent_section": "ISA parser",
            "section_heading": "operands"
        }
    },
    {
        "text": "Section: ISA parser\n\nSub-section: decode tree",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/architecture_support/isa_parser/",
            "page_title": "ISA Parser",
            "parent_section": "ISA parser",
            "section_heading": "decode tree"
        }
    },
    {
        "text": "Section: ISA parser\n\nSub-section: let blocks",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/architecture_support/isa_parser/",
            "page_title": "ISA Parser",
            "parent_section": "ISA parser",
            "section_heading": "let blocks"
        }
    },
    {
        "text": "Section: ISA parser\n\nSub-section: microcode assembler\n\nmicroops\nmacroops\ndirectives\nrom object",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/architecture_support/isa_parser/",
            "page_title": "ISA Parser",
            "parent_section": "ISA parser",
            "section_heading": "microcode assembler"
        }
    },
    {
        "text": "Section: ISA parser\n\nSub-section: Lots more stuff\n\nCode parsing\nTo a large extent, the power and flexibility of the ISA description mechanism stem from the fact that the mapping from a brief instruction definition provided in the decode block to the resulting C++ code is performed in a general-purpose programming language (Python). (This function is performed by the \u201cinstruction format\u201d definition described above in\nFormat definitions\n. Technically, the ISA description language allows any arbitrary Python code to perform this mapping. However, the parser provides a library of Python classes and functions designed to automate the process of deducing an instruction\u2019s characteristics from a brief description of its operation, and generating the strings required to populate declaration and decode templates. This library represents roughly half of the code in isa_parser.py.\nInstruction behaviors are described using C++ with two extensions: bitfield operators and operand type qualifiers. To avoid building a full C++ parser into the ISA description system (or conversely constraining the C++ that could be used for instruction descriptions), these extensions are implemented using regular expression matching and substitution. As a result, there are some syntactic constraints on their usage. The following two sections discuss these extensions in turn. The third section discusses operand parsing, the technique by which the parser automatically infers most instruction characteristics. The final two sections discuss the Python classes through which instruction formats interact with the library:\nCodeBlock\n, which analyzes and encapsulates instruction description code; and the instruction object parameter class,\nInstObjParams\n, which encapsulates the full set of parameters to be substituted into a template.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/architecture_support/isa_parser/",
            "page_title": "ISA Parser",
            "parent_section": "ISA parser",
            "section_heading": "Lots more stuff"
        }
    },
    {
        "text": "Section: ISA parser\n\nSub-section: Bitfield operators\n\nSimple bitfield extraction can be performed on rvalues using the\n<:>\npostfix operator. Bit numbering matches that used in global bitfield definitions (see\nBitfield definitions\n). For example,\nRa<7:0>\nextracts the low 8 bits of register\nRa\n. Single-bit fields can be specified by eliminating the latter operand, e.g.\nRb<31:>\n. Unlike in global bitfield definitions, the colon cannot be eliminated, as it becomes too difficult to distinguish bitfield operators from template arguments. In addition, the bit index parameters must be either identifiers or integer constants; expressions are not allowed. The bit operator will apply either to the syntactic token on its left, or, if that token is a closing parenthesis, to the parenthesized expression.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/architecture_support/isa_parser/",
            "page_title": "ISA Parser",
            "parent_section": "ISA parser",
            "section_heading": "Bitfield operators"
        }
    },
    {
        "text": "Section: ISA parser\n\nSub-section: Operand type qualifiers\n\nThe effective type of an instruction operand (e.g., a register) may be specified by appending a period and a type qualifier to the operand name. The list of type qualifiers is architecture-specific; the\ndef operand_types\nstatement in the ISA description is used to specify it. The specification is in the form of a Python dictionary which maps a type extension to type name. For example, the Alpha ISA definition is as follows:\ndef operand_types {{\n    'sb' : 'int8_t',\n    'ub' : 'uint8_t',\n    'sw' : 'int16_t',\n    'uw' : 'uint16_t',\n    'sl' : 'int32_t',\n    'ul' : 'uint32_t',\n    'sq' : 'int64_t',\n    'uq' : 'uint64_t',\n    'sf' : 'float',\n    'df' : 'double'\n}};\nThus the Alpha 32-bit add instruction addl could be defined as:\nRc.sl = Ra.sl + Rb.sl;\nThe operations are performed using the types specified; the result will be converted from the specified type to the appropriate register value (in this case by sign-extending the 32-bit result to 64 bits, since Alpha integer registers are 64 bits in size).\nType qualifiers are allowed only on recognized instruction operands (see\nInstruction operands\n).",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/architecture_support/isa_parser/",
            "page_title": "ISA Parser",
            "parent_section": "ISA parser",
            "section_heading": "Operand type qualifiers"
        }
    },
    {
        "text": "Section: ISA parser\n\nSub-section: Instruction operands\n\nMost of the automation provided by the parser is based on its recognition of the operands used in the instruction definition code. Most relevant instruction characteristics can be inferred from the operands: floating-point vs. integer instructions can be recognized by the registers used, an instruction that reads from a memory location is a load, etc. In combination with the bitfield operands and type qualifiers described above, most instructions can be described in a single line of code. In addition, most of the differences between simulator CPU models lies in the operand access mechanisms; by generating the code for these accesses automatically, a single description suffices for a variety of situations.\nThe ISA description provides a list of recognized instruction operands and their characteristics via the\ndef operands\nstatement. This statement specifies a Python dictionary that maps operand strings to a five-element tuple.  The elements of the tuple specify the operand as follows:\nthe operand class, which must be one of the strings \u201cIntReg\u201d, \u201cFloatReg\u201d, \u201cMem\u201d, \u201cNPC\u201d, or \u201cControlReg\u201d, indicating an integer register, floating-point register, memory location, the next program counter (NPC), or a control register, respectively.\nthe default type of the operand (an extension string defined in the\ndef operand_types\nblock),\na specifier indicating how specific instances of the operand are decoded (e.g., a bitfield name),\na string or triple of strings indicating the instruction flags that can be inferred when the operand is used, and\na sort priority used to control the order of operands in disassembly.\nFor example, a simplified subset of the Alpha ISA operand traits map is as follows:\ndef operands {{\n    'Ra': ('IntReg', 'uq', 'RA', 'IsInteger', 1),\n    'Rb': ('IntReg', 'uq', 'RB', 'IsInteger', 2),\n    'Rc': ('IntReg', 'uq', 'RC', 'IsInteger', 3),\n    'Fa': ('FloatReg', 'df', 'FA', 'IsFloating', 1),\n    'Fb': ('FloatReg', 'df', 'FB', 'IsFloating', 2),\n    'Fc': ('FloatReg', 'df', 'FC', 'IsFloating', 3),\n    'Mem': ('Mem', 'uq', None, ('IsMemRef', 'IsLoad', 'IsStore'), 4),\n    'NPC': ('NPC', 'uq', None, ( None, None, 'IsControl'), 4)\n}};\nThe operand named\nRa\nis an integer register, default type\nuq\n(unsigned quadword), uses the\nRA\nbitfield from the instruction, implies the\nIsInteger\ninstruction flag, and has a sort priority of 1 (placing it first in any list of operands).\nFor the instruction flag element, a single string (such as\n'IsInteger'\nimplies an unconditionally inferred instruction flag. If the flag operand is a triple, the first element is unconditional, the second is inferred when the operand is a source, and the third when it is a destination. Thus the\n('IsMemRef', 'IsLoad', 'IsStore')\nelement for memory references indicates that any instruction with a memory operand is marked as a memory reference. In addition, if the memory operand is a source, the instruction is marked as a load, while if the operand is a destination, the instruction is marked a store. Similarly, the\n(None, None, 'IsControl')\ntuple for the NPC operand indicates that any instruction that writes to the NPC is a control instruction, but instructions which merely reference NPC as a source do not receive any default flags.\nNote that description code parsing uses regular expressions, which limits the ability of the parser to infer the nature of a partciular operand.  In particular, destination operands are distinguished from source operands solely by testing whether the operand appears on the left-hand side of an assignment operator (\n=\n). Destination operands that are assigned to in a different fashion, e.g. by being passed by reference to other functions, must still appear on the left-hand side of an assignment to be properly recognized as destinations.  The parser also does not recognize C compound assignments, e.g.,\n+=\n.  If an operand is both a source and a destination, it must appear on both the left- and right-hand sides of\n=\n.\nAnother limitation of regular-expression-based code parsing is that control flow in the code block is not recognized.  Combined with the details of how register updates are performed in the CPU models, this means that destinations cannot be updated conditionally.  If a particular register is recognized as a destination register, that register will always be updated at the end of the\nexecute()\nmethod, and thus the code must assign a valid value to that register along each possible code path within the block.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/architecture_support/isa_parser/",
            "page_title": "ISA Parser",
            "parent_section": "ISA parser",
            "section_heading": "Instruction operands"
        }
    },
    {
        "text": "Section: ISA parser\n\nSub-section: The CodeBlock class\n\nAn instruction format requests processing of a string containing instruction description code by passing the string to the CodeBlock constructor. The constructor performs all of the needed analysis and processing, storing the results in the returned object. Among the CodeBlock fields are:\norig_code\n: the original code string.\ncode\n: a processed string containing legal C++ code, derived from the original code by substituting in the bitfield operators and munging operand type qualifiers (s/./_/) to make valid C++ identifiers.\nconstructor\n: code for the constructor of an instruction object, initializing various C++ object fields including the number of operands and the register indices of the operands.\nexec_decl\n: code to declare the C++ variables corresponding to the operands, for use in an execution emulation function.\n*_rd\n: code to read the actual operand values into the corresponding C++ variables for source operands. The first part of the name indicates the relevant CPU model (currently simple and dtld are supported).\n*_wb\n: code to write the C++ variable contents back to the appropriate register or memory location. Again, the first part of the name reflects the CPU model.\n*_mem_rd\n,\n*_nonmem_rd\n,\n*_mem_wb\n,\n*_nonmem_wb\n: as above, but with memory and non-memory operands segregated.\nflags\n: the set of instruction flags implied by the operands.\nop_class\n: a basic guess at the instruction\u2019s operation class (see OpClass) based on the operand types alone.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/architecture_support/isa_parser/",
            "page_title": "ISA Parser",
            "parent_section": "ISA parser",
            "section_heading": "The CodeBlock class"
        }
    },
    {
        "text": "Section: ISA parser\n\nSub-section: The InstObjParams class\n\nInstances of the InstObjParams class encapsulate all of the parameters needed to substitute into a code template, to be used as the argument to a template\u2019s\nsubst()\nmethod (see Template definitions).\nclass\nInstObjParams\n(\nobject\n):\ndef\n__init___\n(\nself\n,\nparser\n,\nmem\n,\nclass_name\n,\nbase_class\n=\n''\n,\nsnippets\n=\n{},\nopt_args\n=\n[]):\nThe first three constructor arguments populate the object\u2019s\nmnemonic\n,\nclass_name\n, and (optionally)\nbase_class\nmembers. The fourth (optional) argument is a CodeBlock object; all of the members of the provided CodeBlock object are copied to the new object, making them accessible for template substitution. Any remaining arguments are interpreted as either additional instruction flags (appended to the\nflags\nlist inherited from the CodeBlock argument, if any), or as an operation class (overriding any\nop_class\nfrom the CodeBlock).",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/architecture_support/isa_parser/",
            "page_title": "ISA Parser",
            "parent_section": "ISA parser",
            "section_heading": "The InstObjParams class"
        }
    },
    {
        "text": "last edited:\n2025-08-21 21:40:17 +0000\nRegister Ops\nThese microops typically take two sources and produce one result. Most have a version that operates on only registers and a version which operates on registers and an immediate value. Some optionally set flags according to their operation. Some of them can be predicated.\nAdd\nAddition.\nadd Dest, Src1, Src2\nDest # Dest <- Src1 + Src2\nAdds the contents of the Src1 and Src2 registers and puts the result in the Dest register.\naddi Dest, Src1, Imm\nDest # Dest <- Src1 + Imm\nAdds the contents of the Src1 register and the immediate Imm and puts the result in the Dest register.\nFlags\nThis microop optionally sets the CF, ECF, ZF, EZF, PF, AF, SF, and OF flags.\nFlag\nMeaning\nCF and ECF\nThe carry out of the most significant bit.\nZF and EZF\nWhether the result was zero.\nPF\nThe parity of the result.\nAF\nThe carry from the fourth to fifth bit positions.\nSF\nThe sign of the result.\nOF\nWhether there was an overflow.\nAdc\nAdd with carry.\nadc Dest, Src1, Src2\nDest # Dest <- Src1 + Src2 + CF\nAdds the contents of the Src1 and Src2 registers and the carry flag and puts the result in the Dest register.\nadci Dest, Src1, Imm\nDest # Dest <- Src1 + Imm + CF\nAdds the contents of the Src1 register, the immediate Imm, and the carry flag and puts the result in the Dest register.\nFlags\nThis microop optionally sets the CF, ECF, ZF, EZF, PF, AF, SF, and OF flags.\nFlag\nMeaning\nCF and ECF\nThe carry out of the most significant bit.\nZF and EZF\nWhether the result was zero.\nPF\nThe parity of the result.\nAF\nThe carry from the fourth to fifth bit positions.\nSF\nThe sign of the result.\nOF\nWhether there was an overflow.\nSub\nSubtraction.\nsub Dest, Src1, Src2\nDest # Dest <- Src1 - Src2\nSubtracts the contents of the Src2 register from the Src1 register and puts the result in the Dest register.\nsubi Dest, Src1, Imm\nDest # Dest <- Src1 - Imm\nSubtracts the contents of the immediate Imm from the Src1 register and puts the result in the Dest register.\nFlags\nThis microop optionally sets the CF, ECF, ZF, EZF, PF, AF, SF, and OF flags.\nFlag\nMeaning\nCF and ECF\nThe borrow into of the most significant bit.\nZF and EZF\nWhether the result was zero.\nPF\nThe parity of the result.\nAF\nThe borrow from the fourth to fifth bit positions.\nSF\nThe sign of the result.\nOF\nWhether there was an overflow.\nSbb\nSubtract with borrow.\nsbb Dest, Src1, Src2\nDest # Dest <- Src1 - Src2 - CF\nSubtracts the contents of the Src2 register and the carry flag from the Src1 register and puts the result in the Dest register.\nsbbi Dest, Src1, Imm\nDest # Dest <- Src1 - Imm - CF\nSubtracts the immediate Imm and the carry flag from the Src1 register and puts the result in the Dest register.\nFlags\nThis microop optionally sets the CF, ECF, ZF, EZF, PF, AF, SF, and OF flags.\nFlag\nMeaning\nCF and ECF\nThe borrow into of the most significant bit.\nZF and EZF\nWhether the result was zero.\nPF\nThe parity of the result.\nAF\nThe borrow from the fourth to fifth bit positions.\nSF\nThe sign of the result.\nOF\nWhether there was an overflow.\nMul1s\nSigned multiply.\nmul1s Src1, Src2\nProdHi:ProdLo # Src1 * Src2\nMultiplies the unsigned contents of the Src1 and Src2 registers and puts the high and low portions of the product into the internal registers ProdHi and ProdLo, respectively.\nmul1si Src1, Imm\nProdHi:ProdLo # Src1 * Imm\nMultiplies the unsigned contents of the Src1 register and the immediate Imm and puts the high and low portions of the product into the internal registers ProdHi and ProdLo, respectively.\nFlags\nThis microop does not set any flags.\nMul1u\nUnsigned multiply.\nmul1u Src1, Src2\nProdHi:ProdLo # Src1 * Src2\nMultiplies the unsigned contents of the Src1 and Src2 registers and puts the high and low portions of the product into the internal registers ProdHi and ProdLo, respectively.\nmul1ui Src1, Imm\nProdHi:ProdLo # Src1 * Imm\nMultiplies the unsigned contents of the Src1 register and the immediate Imm and puts the high and low portions of the product into the internal registers ProdHi and ProdLo, respectively.\nFlags\nThis microop does not set any flags.\nMulel\nUnload multiply result low.\nmulel Dest\nDest # Dest <- ProdLo\nMoves the value of the internal ProdLo register into the Dest register.\nFlags\nThis microop does not set any flags.\nMuleh\nUnload multiply result high.\nmuleh Dest\nDest # Dest <- ProdHi\nMoves the value of the internal ProdHi register into the Dest register.\nFlags\nThis microop optionally sets the CF, ECF, and OF flags.\nFlag\nMeaning\nCF and ECF\nWhether ProdHi is non-zero.\nOF\nWhether ProdHi is zero.\nDiv1\nFirst stage of division.\ndiv1 Src1, Src2\nQuotient * Src2 + Remainder # Src1\nDivisor # Src2\nBegins a division operation where the contents of SrcReg1 is the high part of the dividend and the contents of SrcReg2 is the divisor. The remainder from this partial division is put in the internal register Remainder. The quotient is put in the internal register Quotient. The divisor is put in the internal register Divisor.\ndiv1i Src1, Imm:\nQuotient * Imm + Remainder # Src1\nDivisor # Imm\nBegins a division operation where the contents of SrcReg1 is the high part of the dividend and the immediate Imm is the divisor. The remainder from this partial division is put in the internal register Remainder. The quotient is put in the internal register Quotient. The divisor is put in the internal register Divisor.\nFlags\nThis microop does not set any flags.\nDiv2\nSecond and later stages of division.\ndiv2 Dest, Src1, Src2\nQuotient * Divisor + Remainder # original Remainder with bits shifted in from Src1\nDest # Dest <- Src2 - number of bits shifted in above\nPerforms subsequent steps of division following a div1 instruction. The contents of the register Src1 is the low portion of the dividend. The contents of the register Src2 denote the number of bits in Src1 that have not yet been used before this step in the division. Dest is set to the number of bits in Src1 that have not been used after this step. The internal registers Quotient, Divisor, and Remainder are updated by this instruction.\nIf there are no remaining bits in Src1, this instruction does nothing except optionally compute flags.\ndiv2i Dest, Src1, Imm\nQuotient * Divisor + Remainder # original Remainder with bits shifted in from Src1\nDest # Dest <- Imm - number of bits shifted in above\nPerforms subsequent steps of division following a div1 instruction. The contents of the register Src1 is the low portion of the dividend. The immediate Imm denotes the number of bits in Src1 that have not yet been used before this step in the division. Dest is set to the number of bits in Src1 that have not been used after this step. The internal registers Quotient, Divisor, and Remainder are updated by this instruction.\nIf there are no remaining bits in Src1, this instruction does nothing except optionally compute flags.\nFlags\nThis microop optionally sets the EZF flag.\nFlag\nMeaning\nEZF\nWhether there are any remaining bits in Src1 after this step.\nDivq\nUnload division quotient.\ndivq Dest\nDest # Dest <- Quotient\nMoves the value of the internal Quotient register into the Dest register.\nFlags\nThis microop does not set any flags.\nDivr\nUnload division remainder.\ndivr Dest\nDest # Dest <- Remainder\nMoves the value of the internal Remainder register into the Dest register.\nFlags\nThis microop does not set any flags.\nOr\nLogical or.\nor Dest, Src1, Src2\nDest # Dest <- Src1 | Src2\nComputes the bitwise or of the contents of the Src1 and Src2 registers and puts the result in the Dest register.\nori Dest, Src1, Imm\nDest # Dest <- Src1 | Imm\nComputes the bitwise or of the contents of the Src1 register and the immediate Imm and puts the result in the Dest register.\nFlags\nThis microop optionally sets the CF, ECF, ZF, EZF, PF, AF, SF, and OF flags.\nThere is nothing that prevents computing a value for the AF flag, but it\u2019s value will be meaningless.\nFlag\nMeaning\nCF and ECF\nCleared.\nZF and EZF\nWhether the result was zero.\nPF\nThe parity of the result.\nAF\nUndefined.\nSF\nThe sign of the result.\nOF\nCleared.\nAnd\nLogical And\nand Dest, Src1, Src2\nDest # Dest <- Src1 & Src2\nComputes the bitwise and of the contents of the Src1 and Src2 registers and puts the result in the Dest register.\nandi Dest, Src1, Imm\nDest # Dest <- Src1 & Imm\nComputes the bitwise and of the contents of the Src1 register and the immediate Imm and puts the result in the Dest register.\nFlags\nThis microop optionally sets the CF, ECF, ZF, EZF, PF, AF, SF, and OF flags.\nThere is nothing that prevents computing a value for the AF flag, but it\u2019s value will be meaningless.\nFlag\nMeaning\nCF and ECF\nCleared.\nZF and EZF\nWhether the result was zero.\nPF\nThe parity of the result.\nAF\nUndefined.\nSF\nThe sign of the result.\nOF\nCleared.\nXor\nLogical exclusive or.\nxor Dest, Src1, Src2\nDest # Dest <- Src1 | Src2\nComputes the bitwise xor of the contents of the Src1 and Src2 registers and puts the result in the Dest register.\nxori Dest, Src1, Imm\nDest # Dest <- Src1 | Imm\nComputes the bitwise xor of the contents of the Src1 register and the immediate Imm and puts the result in the Dest register.\nFlags\nThis microop optionally sets the CF, ECF, ZF, EZF, PF, AF, SF, and OF flags.\nThere is nothing that prevents computing a value for the AF flag, but it\u2019s value will be meaningless.\nFlag\nMeaning\nCF and ECF\nCleared.\nZF and EZF\nWhether the result was zero.\nPF\nThe parity of the result.\nAF\nUndefined.\nSF\nThe sign of the result.\nOF\nCleared.\nSll\nLogical left shift.\nsll Dest, Src1, Src2\nDest # Dest <- Src1 \u00ab\u00a0Src2\nShifts the contents of the Src1 register to the left by the value in the Src2 register and writes the result into the Dest register. The shift amount is truncated to either 5 or 6 bits, depending on the operand size.\nslli Dest, Src1, Imm\nDest # Dest <- Src1 \u00ab\u00a0Imm\nShifts the contents of the Src1 register to the left by the value in the immediate Imm and writes the result into the Dest register. The shift amount is truncated to either 5 or 6 bits, depending on the operand size.\nFlags\nThis microop optionally sets the CF, ECF, and OF flags. If the shift amount is zero, no flags are modified.\nFlag\nMeaning\nCF and ECF\nThe last bit shifted out of the result.\nOF\nThe exclusive OR of what this instruction would set the CF flag to, if requested, and the most significant bit of the result.\nSrl\nLogical right shift.\nsrl Dest, Src1, Src2\nDest # Dest <- Src1\u00a0\u00bb(logical) Src2\nShifts the contents of the Src1 register to the right by the value in the Src2 register and writes the result into the Dest register. Bits which are shifted in sign extend the result. The shift amount is truncated to either 5 or 6 bits, depending on the operand size.\nsrli Dest, Src1, Imm\nDest # Dest <- Src1\u00a0\u00bb(logical) Imm\nShifts the contents of the Src1 register to the right by the value in the immediate Imm and writes the result into the Dest register. Bits which are shifted in sign extend the result. The shift amount is truncated to either 5 or 6 bits, depending on the operand size.\nFlags\nThis microop optionally sets the CF, ECF, and OF flags. If the shift amount is zero, no flags are modified.\nFlag\nMeaning\nCF and ECF\nThe last bit shifted out of the result.\nSF\nThe most significant bit of the original value to shift.\nSra\nArithmetic right shift.\nsra Dest, Src1, Src2\nDest # Dest <- Src1\u00a0\u00bb(arithmetic) Src2\nShifts the contents of the Src1 register to the right by the value in the Src2 register and writes the result into the Dest register. Bits which are shifted in zero extend the result. The shift amount is truncated to either 5 or 6 bits, depending on the operand size.\nsrai Dest, Src1, Imm\nDest # Dest <- Src1\u00a0\u00bb(arithmetic) Imm\nShifts the contents of the Src1 register to the right by the value in the immediate Imm and writes the result into the Dest register. Bits which are shifted in zero extend the result. The shift amount is truncated to either 5 or 6 bits, depending on the operand size.\nFlags\nThis microop optionally sets the CF, ECF, and OF flags. If the shift amount is zero, no flags are modified.\nFlag\nMeaning\nCF and ECF\nThe last bit shifted out of the result.\nOF\nCleared.\nRor\nRotate right.\nror Dest, Src1, Src2\nRotates the contents of the Src1 register to the right by the value in the Src2 register and writes the result into the Dest register. The rotate amount is truncated to either 5 or 6 bits, depending on the operand size.\nrori Dest, Src1, Imm\nRotates the contents of the Src1 register to the right by the value in the immediate Imm and writes the result into the Dest register. The rotate amount is truncated to either 5 or 6 bits, depending on the operand size.\nFlags\nThis microop optionally sets the CF, ECF, and OF flags. If the rotate amount is zero, no flags are modified.\nFlag\nMeaning\nCF and ECF\nThe most significant bit of the result.\nOF\nThe exclusive OR of the most two significant bits of the original value.\nRcr\nRotate right through carry.\nrcr Dest, Src1, Src2\nRotates the contents of the Src1 register through the carry flag and to the right by the value in the Src2 register and writes the result into the Dest register. The rotate amount is truncated to either 5 or 6 bits, depending on the operand size.\nrcri Dest, Src1, Imm\nRotates the contents of the Src1 register through the carry flag and to the right by the value in the immediate Imm and writes the result into the Dest register. The rotate amount is truncated to either 5 or 6 bits, depending on the operand size.\nFlags\nThis microop optionally sets the CF, ECF, and OF flags. If the rotate amount is zero, no flags are modified.\nFlag\nMeaning\nCF and ECF\nThe last bit shifted out of the result.\nOF\nThe exclusive OR of the CF flag before the rotate and the most significant bit of the original value.\nRol\nRotate left.\nrol Dest, Src1, Src2\nRotates the contents of the Src1 register to the left by the value in the Src2 register and writes the result into the Dest register. The rotate amount is truncated to either 5 or 6 bits, depending on the operand size.\nroli Dest, Src1, Imm\nRotates the contents of the Src1 register to the left by the value in the immediate Imm and writes the result into the Dest register. The rotate amount is truncated to either 5 or 6 bits, depending on the operand size.\nFlags\nThis microop optionally sets the CF, ECF, and OF flags. If the rotate amount is zero, no flags are modified.\nFlag\nMeaning\nCF and ECF\nThe least significant bit of the result.\nOF\nThe exclusive OR of the most and least significant bits of the result.\nRcl\nRotate left through carry.\nrcl Dest, Src1, Src2\nRotates the contents of the Src1 register through the carry flag and to the left by the value in the Src2 register and writes the result into the Dest register. The rotate amount is truncated to either 5 or 6 bits, depending on the operand size.\nrcli Dest, Src1, Imm\nRotates the contents of the Src1 register through the carry flag and to the left by the value in the immediate Imm and writes the result into the Dest register. The rotate amount is truncated to either 5 or 6 bits, depending on the operand size.\nFlags\nThis microop optionally sets the CF, ECF, and OF flags. If the rotate amount is zero, no flags are modified.\nFlag\nMeaning\nCF and ECF\nThe last bit rotated out of the result.\nOF\nThe exclusive OR of CF before the rotate and the most significant bit of the result.\nMov\nMove.\nmov Dest, Src1, Src2\nDest # Src1 <- Src2\nMerge the contents of the Src2 register into the contents of Src1 and put the result into the Dest register.\nmovi Dest, Src1, Imm\nDest # Src1 <- Imm\nMerge the contents of the immediate Imm into the contents of Src1 and put the results into the Dest register.\nFlags\nThis microop does not set any flags. It is optionally predicated.\nSext\nSign extend.\nsext Dest, Src1, Imm\nDest # Dest <- sign_extend(Src1, Imm)\nSign extend the value in the Src1 register starting at the bit position in the immediate Imm, and put the result in the Dest register.\nFlags\nThis microop does not set any flags.\nZext\nZero extend.\nzext Dest, Src1, Imm\nDest # Dest <- zero_extend(Src1, Imm)\nZero extend the value in the Src1 register starting at the bit position in the immediate Imm, and put the result in the Dest register.\nFlags\nThis microop does not set any flags.\nRuflag\nRead user flag.\nruflag Dest, Imm\nReads the user level flag stored in the bit position specified by the immediate Imm and stores it in the register Dest.\nThe mapping between values of Imm and user level flags is show in the following table.\nImm\nFlag\n0\nCF (carry flag)\n2\nPF (parity flag)\n3\nECF (emulation carry flag)\n4\nAF (auxiliary flag)\n5\nEZF (emulation zero flag)\n6\nZF (zero flag)\n7\nCF (sign flag)\n10\nCF (direction flag)\n11\nCF (overflow flag)\nFlags\nThe EZF flag is always set. In the future this may become optional.\nRuflags\nRead all user flags.\nruflags Dest\nDest # user flags\nStore the user level flags into the Dest register.\nFlags\nThis microop does not set any flags.\nWruflags\nWrite all user flags.\nwruflags Src1, Src2\nuser flags # Src1 ^ Src2\nSet the user level flags to the exclusive or of the Src1 and Src2 registers.\nwruflagsi Src1, Imm\nuser flags # Src1 ^ Imm\nSet the user level flags to the exclusive or of the Src1 register and the immediate Imm.\nFlags\nSee above.\nRdip\nRead the instruction pointer.\nrdip Dest\nDest # rIP\nSet the Dest register to the current value of rIP.\nFlags\nThis microop does not set any flags.\nWrip\nWrite the instruction pointer.\nwrip Src1, Src2\nrIP # Src1 + Src2\nSet the rIP to the sum of the Src1 and Src2 registers. This causes a macroop branch at the end of the current macroop.\nwripi Src1, Imm\nmicropc # Src1 + Imm\nSet the rIP to the sum of the Src1 register and immediate Imm. This causes a macroop branch at the end of the current macroop.\nFlags\nThis microop does not set any flags. It is optionally predicated.\nChks\nCheck selector.\nNot yet implemented.\nLoad/Store Ops\nLd\nLoad.\nld Data, Seg, Sib, Disp\nLoads the integer register Data from memory.\nLdf\nLoad floating point.\nldf Data, Seg, Sib, Disp\nLoads the floating point register Data from memory.\nLdm\nLoad multimedia.\nldm Data, Seg, Sib, Disp\nLoad the multimedia register Data from memory.\nThis is not implemented and may never be.\nLdst\nLoad with store check.\nLdst Data, Seg, Sib, Disp\nLoad the integer register Data from memory while also checking if a store to that location would succeed.\nThis is not implemented currently.\nLdstl\nLoad with store check, locked.\nLdst Data, Seg, Sib, Disp\nLoad the integer register Data from memory while also checking if a store to that location would succeed, and also provide the semantics of the \u201cLOCK\u201d instruction prefix.\nThis is not implemented currently.\nSt\nStore.\nst Data, Seg, Sib, Disp\nStores the integer register Data to memory.\nStf\nStore floating point.\nstf Data, Seg, Sib, Disp\nStores the floating point register Data to memory.\nStm\nStore multimedia.\nstm Data, Seg, Sib, Disp\nStore the multimedia register Data to memory.\nThis is not implemented and may never be.\nStupd\nStore with base update.\nStupd Data, Seg, Sib, Disp\nStore the integer register Data to memory and update the base register.\nLea\nLoad effective address.\nlea Data, Seg, Sib, Disp\nCalculates the address for this combination of parameters and stores it in Data.\nCda\nCheck data address.\ncda Seg, Sib, Disp\nCheck whether the data address is valid.\nThis is not implemented currently.\nCdaf\nCDA with cache line flush.\ncdaf Seg, Sib, Disp\nCheck whether the data address is valid, and flush cache lines\nThis is not implemented currently.\nCia\nCheck instruction address.\ncia Seg, Sib, Disp\nCheck whether the instruction address is valid.\nThis is not implemented currently.\nTia\nTLB invalidate address\ntia Seg, Sib, Disp\nInvalidate the tlb entry which corresponds to this address.\nThis is not implemented currently.\nLoad immediate Op\nLimm\nlimm Dest, Imm\nStores the 64 bit immediate Imm into the integer register Dest.\nFloating Point Ops\nMovfp\nmovfp Dest, Src\nDest # Src\nMove the contents of the floating point register Src into the floating point register Dest.\nThis instruction is predicated.\nXorfp\nxorfp Dest, Src1, Src2\nDest # Src1 ^ Src2\nCompute the bitwise exclusive or of the floating point registers Src1 and Src2 and put the result in the floating point register Dest.\nSqrtfp\nsqrtfp Dest, Src\nDest # sqrt(Src)\nCompute the square root of the floating point register Src and put the result in floating point register Dest.\nAddfp\naddfp Dest, Src1, Src2\nDest # Src1 + Src2\nCompute the sum of the floating point registers Src1 and Src2 and put the result in the floating point register Dest.\nSubfp\nsubfp Dest, Src1, Src2\nDest # Src1 - Src2\nCompute the difference of the floating point registers Src1 and Src2 and put the result in the floating point register Dest.\nMulfp\nmulfp Dest, Src1, Src2\nDest # Src1 * Src2\nCompute the product of the floating point registers Src1 and Src2 and put the result in the floating point register Dest.\nDivfp\ndivfp Dest, Src1, Src2\nDest # Src1 / Src2\nDivide Src1 by Src2 and put the result in the floating point register Dest.\nCompfp\ncompfp Src1, Src2\nCompare floating point registers Src1 and Src2.\nCvtf_i2d\ncvtf_i2d Dest, Src\nConvert integer register Src into a double floating point value and store the result in the lower part of Dest.\nCvtf_i2d_hi\ncvtf_i2d_hi Dest, Src\nConvert integer register Src into a double floating point value and store the result in the upper part of Dest.\nCvtf_d2i\ncvtf_d2i Dest, Src\nConvert floating point register Src into an integer value and store the result in the integer register Dest.\nSpecial Ops\nFault\nGenerate a fault.\nfault fault_code\nUses the C++ code fault_code to allocate a Fault object to return.\nLddha\nSet the default handler for a fault.\nThis is not implemented currently.\nLdaha\nSet the alternate handler for a fault\nThis is not implemented currently.\nSequencing Ops\nThese microops are used for control flow withing microcode\nBr\nMicrocode branch. This is never considered the last microop of a sequence. If it appears at the end of a macroop, it is assumed that it branches to microcode in the ROM.\nbr target\nmicropc # target\nSet the micropc to the 16 bit immediate target.\nFlags\nThis microop does not set any flags. It is optionally predicated.\nEret\nReturn from emulation. This instruction is always considered the last microop in a sequence. When executing from the ROM, it is the only way to return to normal instruction decoding.\neret\nReturn from emulation.\nFlags\nThis microop does not set any flags. It is optionally predicated.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/architecture_support/x86_microop_isa/",
            "page_title": "Register Ops",
            "parent_section": "Overview",
            "section_heading": "x86_microop_isa"
        }
    },
    {
        "text": "Supported operating systems and environments\ngem5 has been designed with a Linux environment in mind. We test regularly\non\nUbuntu 22.04\nand\nUbuntu 24.04\nto ensure gem5 functions well in\nthese environments. Though\nany Linux based OS should function if the correct\ndependencies are installed\n. We ensure that gem5 is compilable with both gcc\nand clang (see\nDependencies\nbelow for compiler version\ninformation).\nAs of gem5 21.0,\nwe support building and running gem5 with Python 3.6+\nonly\n. gem5 20.0 was our last version of gem5 to provide support for Python\n2.\nIf running gem5 in a suitable OS/environment is not possible, we have provided\npre-prepared\nDocker\nimages which may be used to\ncompile and run gem5. Please see our\nDocker\nsection below for more\ninformation on this.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/building",
            "page_title": "Building gem5",
            "parent_section": "Supported operating systems and environments",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Section: Dependencies\n\nSub-section: Setup on Ubuntu 24.04 (gem5 >= v24.0)\n\nIf compiling gem5 on Ubuntu 24.04, or related Linux distributions, you may\ninstall all these dependencies using APT:\nsudo\napt\ninstall\nbuild-essential scons python3-dev git pre-commit zlib1g zlib1g-dev\n\\\nlibprotobuf-dev protobuf-compiler libprotoc-dev libgoogle-perftools-dev\n\\\nlibboost-all-dev  libhdf5-serial-dev python3-pydot python3-venv python3-tk mypy\n\\\nm4 libcapstone-dev libpng-dev libelf-dev pkg-config wget cmake doxygen clang-format",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/building",
            "page_title": "Building gem5",
            "parent_section": "Dependencies",
            "section_heading": "Setup on Ubuntu 24.04 (gem5 >= v24.0)"
        }
    },
    {
        "text": "Section: Dependencies\n\nSub-section: Setup on Ubuntu 22.04 (gem5 >= v21.1)\n\nIf compiling gem5 on Ubuntu 22.04, or related Linux distributions, you may\ninstall all these dependencies using APT:\nsudo\napt\ninstall\nbuild-essential git m4 scons zlib1g zlib1g-dev\n\\\nlibprotobuf-dev protobuf-compiler libprotoc-dev libgoogle-perftools-dev\n\\\npython3-dev libboost-all-dev pkg-config python3-tk clang-format-15\nYou may need to configure\nclang-format-15\nas the default\nclang-format\nfor your system.\n# Configure clang-format-15 and git-clang-format-15 as the system defaults.\nsudo\nupdate-alternatives\n--install\n/usr/bin/clang-format clang-format /usr/bin/clang-format-15 150\n\\\n--slave\n/usr/bin/clang-format-diff clang-format-diff /usr/bin/clang-format-diff-15\n\\\n--slave\n/usr/bin/git-clang-format git-clang-format /usr/bin/git-clang-format-15\n# [Optional] Add other alternative versions, and select version 15 as the default version.\nsudo\nupdate-alternatives\n--config\nclang-format",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/building",
            "page_title": "Building gem5",
            "parent_section": "Dependencies",
            "section_heading": "Setup on Ubuntu 22.04 (gem5 >= v21.1)"
        }
    },
    {
        "text": "Section: Dependencies\n\nSub-section: Setup on Ubuntu 20.04 (gem5 >= v21.0)\n\nIf compiling gem5 on Ubuntu 20.04, or related Linux distributions, you may\ninstall all these dependencies using APT:\nsudo\napt\ninstall\nbuild-essential git m4 scons zlib1g zlib1g-dev\n\\\nlibprotobuf-dev protobuf-compiler libprotoc-dev libgoogle-perftools-dev\n\\\npython3-dev python-is-python3 libboost-all-dev pkg-config gcc-10 g++-10\n\\\npython3-tk clang-format-18\nYou may need to configure\nclang-format-18\nas the default\nclang-format\nfor your system.\n# Configure clang-format-18 and git-clang-format-18 as the system defaults.\nsudo\nupdate-alternatives\n--install\n/usr/bin/clang-format clang-format /usr/bin/clang-format-18 180\n\\\n--slave\n/usr/bin/clang-format-diff clang-format-diff /usr/bin/clang-format-diff-18\n\\\n--slave\n/usr/bin/git-clang-format git-clang-format /usr/bin/git-clang-format-18\n# [Optional] Add other alternative versions, and select version 18 as the default version.\nsudo\nupdate-alternatives\n--config\nclang-format",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/building",
            "page_title": "Building gem5",
            "parent_section": "Dependencies",
            "section_heading": "Setup on Ubuntu 20.04 (gem5 >= v21.0)"
        }
    },
    {
        "text": "Section: Dependencies\n\nSub-section: Docker\n\nFor users struggling to setup an environment to build and run gem5, we provide\nthe following Docker Images:\nUbuntu 24.04 with all optional dependencies:\nghcr.io/gem5/ubuntu-24.04_all-dependencies:v24-0\n(\nsource Dockerfile\n).\nUbuntu 24.04 with minimum dependencies:\nghcr.io/gem5/ubuntu-24.04_min-dependencies:v24-0\n(\nsource Dockerfile\n).\nUbuntu 22.04 with all optional dependencies:\nghcr.io/gem5/ubuntu-22.04_all-dependencies:v23-0\n(\nsource Dockerfile\n).\nUbuntu 20.04 with all optional dependencies:\nghcr.io/gem5/ubuntu-20.04_all-dependencies:v23-0\n(\nsource Dockerfile\n).\nUbuntu 18.04 with all optional dependencies:\nghcr.io/gem5/ubuntu-18.04_all-dependencies:v23-0\n(\nsource Dockerfile\n).\nTo obtain a docker image:\ndocker pull <image>\nE.g., for Ubuntu 20.04 with all optional dependencies:\ndocker pull ghcr.io/gem5/ubuntu-20.04_all-dependencies:v23-0\nThen, to work within this environment, we suggest using the following:\ndocker run\n-u\n$UID\n:\n$GID\n--volume\n<gem5 directory>:/gem5\n--rm\n-it\n<image>\nWhere\n<gem5 directory>\nis the full path of the gem5 in your file system, and\n<image>\nis the image pulled (e.g.,\nghcr.io/gem5/ubuntu-22.04_all-dependencies:v23-0`).\nFrom this environment, you will be able to build and run gem5 from the\n/gem5\ndirectory.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/building",
            "page_title": "Building gem5",
            "parent_section": "Dependencies",
            "section_heading": "Docker"
        }
    },
    {
        "text": "Getting the code\ngit clone https://github.com/gem5/gem5",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/building",
            "page_title": "Building gem5",
            "parent_section": "Getting the code",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Building with SCons\ngem5\u2019s build system is based on SCons, an open source build system implemented\nin Python. You can find more information about scons at\nhttp://www.scons.org\n.\nThe main scons file is called SConstruct and is found in the root of the source\ntree. Additional scons files are named SConscript and are found throughout the\ntree, usually near the files they\u2019re associated with.\nWithin the root of the gem5 directory, gem5 can be built with SCons using:\nscons build/\n{\nISA\n}\n/gem5.\n{\nvariant\n}\n-j\n{\ncpus\n}\nwhere\n{ISA}\nis the target (guest) Instruction Set Architecture, and\n{variant}\nspecifies the compilation settings. For most intents and purposes\nopt\nis a good target for compilation. The\n-j\nflag is optional and allows\nfor parallelization of compilation with\n{cpus}\nspecifying the number of\nthreads. A single-threaded compilation from scratch can take up to 2 hours on\nsome systems. We therefore strongly advise allocating more threads if possible.\nHowever, compilation of gem5 is compute and memory intensive and increasing the\nnumber of threads also increases memory usage. If using a machine with less\nmemory, it is recommended to use fewer threads (e.g.\n-j 1\nor\n-j 2\n).\nThe valid ISAs are:\nALL - recommended, as it has all ISAs and all Ruby protocols as of gem5 v24.1\nARM\nNULL\nMIPS\nPOWER\nRISCV\nSPARC\nX86\nThe valid build variants are:\ndebug\nhas optimizations turned off. This ensures that variables won\u2019t be\noptimized out, functions won\u2019t be unexpectedly inlined, and control flow will\nnot behave in surprising ways. That makes this version easier to work with in\ntools like gdb, but without optimizations this version is significantly slower\nthan the others. You should choose it when using tools like gdb and valgrind\nand don\u2019t want any details obscured, but other wise more optimized versions are\nrecommended.\nopt\nhas optimizations turned on and debugging functionality like asserts\nand DPRINTFs left in. This gives a good balance between the speed of the\nsimulation and insight into what\u2019s happening in case something goes wrong. This\nversion is best in most circumstances.\nfast\nhas optimizations turned on and debugging functionality compiled\nout. This pulls out all the stops performance wise, but does so at the expense\nof run time error checking and the ability to turn on debug output. This\nversion is recommended if you\u2019re very confident everything is working correctly\nand want to get peak performance from the simulator.\nThese versions are summarized in the following table.\nBuild variant\nOptimizations\nRun time debugging support\ndebug\nX\nopt\nX\nX\nfast\nX\nFor example, to build gem5 on 4 threads with\nopt\nand with all ISAs:\nscons build/ALL/gem5.opt\n-j\n4\nIn addition, users may make use of the \u201cgprof\u201d and \u201cpperf\u201d build options to\nenable profiling:\ngprof\nallows gem5 to be used with the gprof profiling tool. It can be\nenabled by compiling with the\n--gprof\nflag. E.g.,\nscons build/ALL/gem5.debug --gprof\n.\npprof\nallows gem5 to be used with the pprof profiling tool. It can be\nenabled by compiling with the\n--pprof\nflag. E.g.,\nscons build/ALL/gem5.debug --pprof\n.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/building",
            "page_title": "Building gem5",
            "parent_section": "Building with SCons",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Build with Kconfig\nPlease see\nhere",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/building",
            "page_title": "Building gem5",
            "parent_section": "Build with Kconfig",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Usage\nOnce compiled, gem5 can then be run using:\n./build/{ISA}/gem5.{variant} [gem5 options] {simulation script} [script options]\nIf you are building gem5 from a pre-compiled binary, gem5 can be run with the following command:\ngem5 [gem5 options] {simulation script} [script options]\nRunning with the\n--help\nflag will display all the available options:\nUsage\n=====\n  gem5.opt [gem5 options] script.py [script options]\n\ngem5 is copyrighted software; use the --copyright option for details.\n\nOptions\n=======\n--help, -h              show this help message and exit\n--build-info, -B        Show build information\n--copyright, -C         Show full copyright information\n--readme, -R            Show the readme\n--outdir=DIR, -d DIR    Set the output directory to DIR [Default: m5out]\n--redirect-stdout, -r   Redirect stdout (& stderr, without -e) to file\n--redirect-stderr, -e   Redirect stderr to file\n--silent-redirect       Suppress printing a message when redirecting stdout or\n                        stderr\n--stdout-file=FILE      Filename for -r redirection [Default: simout.txt]\n--stderr-file=FILE      Filename for -e redirection [Default: simerr.txt]\n--listener-mode={on,off,auto}\n                        Port (e.g., gdb) listener mode (auto: Enable if\n                        running interactively) [Default: auto]\n--allow-remote-connections\n                        Port listeners will accept connections from anywhere\n                        (0.0.0.0). Default is only localhost.\n--interactive, -i       Invoke the interactive interpreter after running the\n                        script\n--pdb                   Invoke the python debugger before running the script\n--path=PATH[:PATH], -p PATH[:PATH]\n                        Prepend PATH to the system path when invoking the\n                        script\n--quiet, -q             Reduce verbosity\n--verbose, -v           Increase verbosity\n-m mod                  run library module as a script (terminates option\n                        list)\n-c cmd                  program passed in as string (terminates option list)\n-P                      Don't prepend the script directory to the system path.\n                        Mimics Python 3's `-P` option.\n-s                      IGNORED, only for compatibility with python. don'tadd\n                        user site directory to sys.path; also PYTHONNOUSERSITE\n\nStatistics Options\n------------------\n--stats-file=FILE       Sets the output file for statistics [Default:\n                        stats.txt]\n--stats-help            Display documentation for available stat visitors\n\nConfiguration Options\n---------------------\n--dump-config=FILE      Dump configuration output file [Default: config.ini]\n--json-config=FILE      Create JSON output of the configuration [Default:\n                        config.json]\n--dot-config=FILE       Create DOT & pdf outputs of the configuration\n                        [Default: config.dot]\n--dot-dvfs-config=FILE  Create DOT & pdf outputs of the DVFS configuration\n                        [Default: none]\n\nDebugging Options\n-----------------\n--debug-break=TICK[,TICK]\n                        Create breakpoint(s) at TICK(s) (kills process if no\n                        debugger attached)\n--debug-help            Print help on debug flags\n--debug-flags=FLAG[,FLAG]\n                        Sets the flags for debug output (-FLAG disables a\n                        flag)\n--debug-start=TICK      Start debug output at TICK\n--debug-end=TICK        End debug output at TICK\n--debug-file=FILE       Sets the output file for debug. Append '.gz' to the\n                        name for it to be compressed automatically [Default:\n                        cout]\n--debug-activate=EXPR[,EXPR]\n                        Activate EXPR sim objects\n--debug-ignore=EXPR     Ignore EXPR sim objects\n--remote-gdb-port=REMOTE_GDB_PORT\n                        Remote gdb base port (set to 0 to disable listening)\n\nHelp Options\n------------\n--list-sim-objects      List all built-in SimObjects, their params and default\n                        values",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/building",
            "page_title": "Building gem5",
            "parent_section": "Usage",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Using EXTRAS\nThe\nEXTRAS\nscons variable can be\nused to build additional directories of source files into gem5 by setting it to\na colon delimited list of paths to these additional directories. EXTRAS is a\nhandy way to build on top of the gem5 code base without mixing your new source\nwith the upstream source. You can then manage your new body of code however you\nneed to independently from the main code base.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/building",
            "page_title": "Building gem5",
            "parent_section": "Using EXTRAS",
            "section_heading": "Overview"
        }
    },
    {
        "text": "last edited:\n2025-08-21 21:40:17 +0000\nBuilding EXTRAS\nThe\nEXTRAS\nSCons option is a way to add functionality in gem5 without adding your files to the gem5 source tree. Specifically, it allows you to identify one or more directories that will get compiled in with gem5 as if they appeared under the \u2018src\u2019 part of the gem5 tree, without requiring the code to be actually located under \u2018src\u2019. It\u2019s present to allow user to compile in additional functionality (typically additional SimObject classes) that isn\u2019t or can\u2019t be distributed with gem5. This is useful for maintaining local code that isn\u2019t suitable for incorporating into the gem5 source tree, or third-party code that can\u2019t be incorporated due to an incompatible license. Because the EXTRAS location is completely independent of the gem5 repository, you can keep the code under a different version control system as well.\nThe main drawback of the EXTRAS feature is that, by itself, it only supports adding code to gem5, not modifying any of the base gem5 code.\nOne use of the EXTRAS feature is to support EIO traces. The trace reader for EIO is licensed under the SimpleScalar license, and due to the incompatibility of that license with gem5\u2019s BSD license, the code to read these traces is not included in the gem5 distribution. Instead, the EIO code is distributed via a separate \u201cencumbered\u201d\nrepository\n.\nThe following examples show how to compile the EIO code. By adding to or modifying the extras path, any other suitable extra could be compiled in. To compile in code using EXTRAS simply execute the following\nscons\nEXTRAS\n=\n/path/\nto\n/\nencumbered\nbuild\n/<\nISA\n>\n/gem5.op\nt\nIn the root of this directory you should have a SConscript that uses the\nSource()\nand\nSimObject()\nscons functions that are used in the rest of M5 to compile the appropriate sources and add any SimObjects of interest. If you want to add more than one directory, you can set EXTRAS to a colon-separated list of paths.\nNote that EXTRAS is a \u201csticky\u201d parameter, so after a value is provided to scons once, the value will be reused for future scons invocations targeting the same build directory (\nbuild/<ISA>\nin this case) as long as it is not overridden. Thus you only need to specify EXTRAS the first time you build a particular configuration or if you want to override a previously specified value.\nTo run a regression with EXTRAS use a command line similar to the following:\n.\n/\nutil\n/\nregress\n--\nscons\n-\nopts\n=\n\"\nEXTRAS=/path/to/encumbered\n\"\n-\nj\n2\nquick",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/building/EXTRAS",
            "page_title": "Building EXTRAS",
            "parent_section": "Overview",
            "section_heading": "EXTRAS"
        }
    },
    {
        "text": "Creation\nFirst of all, you need to create a checkpoint. Each checkpoint as saved in a new directory named \u2018cpt.TICKNUMBER\u2019, where TICKNUMBER refers to the tick value at which this checkpoint was created. There are several ways in which a checkpoint can be created:\nAfter booting the gem5 simulator, execute the command m5 checkpoint. One can execute the command manually using m5term, or include it in a run script to do this automatically after the Linux kernel has booted up.\nThere is a pseudo instruction that can be used for creating checkpoints. For example, one may include this pseudo instruction in an application program, so that the checkpoint is created when the application has reached a certain state.\nThe option\n-\n-take-checkpoints\ncan be provided to the python scripts (fs.py, ruby_fs.py) so that checkpoints are dumped periodically. The option\n-\n-checkpoint-at-end\ncan be used for creating the checkpoint at the end of the simulation. Take a look at the file\nconfigs/common/Options.py\nfor these options.\nWhile creating checkpoints with Ruby memory model, it is necessary to use the MOESI hammer protocol. This is because checkpointing the correct memory state requires that the caches are flushed to the memory. This flushing operation is currently supported only with the MOESI hammer protocol.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/checkpoints",
            "page_title": "Checkpoints",
            "parent_section": "Creation",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Restoring\nRestoring from a checkpoint can usually be easily done from the command line, e.g.:\nbuild/ALL/gem5.debug configs/example/fs.py -r N\n  OR\n  build/ALL/gem5.debug configs/example/fs.py --checkpoint-restore=N\nThe number N is integer that represents checkpoint number which usually starts from 1 then increases incrementally to 2,3,4\u2026\nBy default, gem5 assumes that the checkpoint is to be restored using Atomic CPUs. This may not work if the checkpoint was recorded using Timing / Detailed / Inorder CPU. One can mention the option\n-\n-restore-with-cpu <CPU Type>\non the command line. The cpu type supplied with this option is then used for restoring from the checkpoint.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/checkpoints",
            "page_title": "Checkpoints",
            "parent_section": "Restoring",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Section: Detailed example: Parsec\n\nSub-section: Annotating workloads\n\nAnnotation is required for two purposes: for defining region of program beyond the initialization section of a program and for defining logical units of work in each of the workloads.\nWorkloads in PARSEC benchmark suite, already has annotating demarcating start and end of portion of program without program initialization section and program finalization section. We just use gem5 specific annotation for start of Region of Interest. The start of the Region of Interest (ROI) is marked by\nm5_roi_begin()\nand the end of ROI is demarcated by\nm5_roi_end()\n.\nDue to large simulation time its not always possible to simulate whole program. Moreover, unlike single threaded programs, simulating for a given number instructions in multi-threaded workloads is not a correct way to simulate portion of a program due to possible presence of instructions spinning on synchronization variable. Thus it is important define semantically meaningful logical units of work in each workload. Simulating for a given number of workuints in a multi-threaded workloads gives a reasonable way of simulating portion of workloads as the problem of instructions spinning on synchronization variables.\nSwitchover/Fastforwarding",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/checkpoints",
            "page_title": "Checkpoints",
            "parent_section": "Detailed example: Parsec",
            "section_heading": "Annotating workloads"
        }
    },
    {
        "text": "Sampling\nSampling (switching between functional and detailed models) can be implemented via your Python script. In your script you can direct the simulator to switch between two sets of CPUs. To do this, in your script setup a list of tuples of (oldCPU, newCPU). If there are multiple CPUs you wish to switch simultaneously, they can all be added to that list. For example:\nrun_cpu1\n=\nSimpleCPU\n()\nswitch_cpu1\n=\nDetailedCPU\n(\nswitched_out\n=\nTrue\n)\nrun_cpu2\n=\nSimpleCPU\n()\nswitch_cpu2\n=\nFooCPU\n(\nswitched_out\n=\nTrue\n)\nswitch_cpu_list\n=\n[(\nrun_cpu1\n,\nswitch_cpu1\n),(\nrun_cpu2\n,\nswitch_cpu2\n)]\nNote that the CPU that does not immediately run should have the parameter \u201cswitched_out=True\u201d. This keeps those CPUs from adding themselves to the list of CPUs to run; they will instead get added when you switch them in.\nIn order for gem5 to instantiate all of your CPUs, you must make the CPUs that will be switched in a child of something that is in the configuration hierarchy. Unfortunately at the moment some configuration limitations force the switch CPU to be placed outside of the System object. The Root object is the next most convenient place to place the CPU, as shown below:\nm5\n.\nsimulate\n(\n500\n)\n# simulate for 500 cycles\nm5\n.\nswitchCpus\n(\nswitch_cpu_list\n)\nm5\n.\nsimulate\n(\n500\n)\n# simulate another 500 cycles after switching\nNote that gem5 may have to simulate for a few cycles prior to switching CPUs due to any outstanding state that may be present in the CPUs being switched out.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/checkpoints",
            "page_title": "Checkpoints",
            "parent_section": "Sampling",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Cross Compilers\nA cross compiler is a compiler set up to run on one ISA but generate binaries which run on another. \nYou may need one if you intend to simulate a system which uses a particular ISA, Alpha for instance, but don\u2019t have access to actual Alpha hardware.\nThere are various sources for cross compilers. The following are some of them.\nARM\n.\nRISC-V\n.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/compiling_workloads/",
            "page_title": "Compiling Workloads",
            "parent_section": "Cross Compilers",
            "section_heading": "Overview"
        }
    },
    {
        "text": "QEMU\nAlternatively, you can use QEMU and a disk image to run the desired ISA in emulation. \nTo create more recent disk images, see\nthis page\n. \nThe following is a youtube video of working with image files using qemu on Ubuntu 12.04 64bit.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/compiling_workloads/",
            "page_title": "Compiling Workloads",
            "parent_section": "QEMU",
            "section_heading": "Overview"
        }
    },
    {
        "text": "gem5 bootcamp 2022 module on using CPU models\ngem5 bootcamp (2022) had a session on learning the use of different gem5 CPU models.\nThe slides presented in the session can be found\nhere\n.\nThe youtube video of the recorded bootcamp module on gem5 CPU models is available\nhere\n.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/cpu_models/",
            "page_title": "No Title Found",
            "parent_section": "gem5 bootcamp 2022 module on using CPU models",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Pipeline stages\nFetch\nFetches instructions each cycle, selecting which thread to fetch from based on the policy selected. This stage is where the DynInst is first created. Also handles branch prediction.\nDecode\nDecodes instructions each cycle. Also handles early resolution of PC-relative unconditional branches.\nRename\nRenames instructions using a physical register file with a free list. Will stall if there are not enough registers to rename to, or if back-end resources have filled up. Also handles any serializing instructions at this point by stalling them in rename until the back-end drains.\nIssue/Execute/Writeback\nOur simulator model handles both execute and writeback when the execute() function is called on an instruction, so we have combined these three stages into one stage. This stage (IEW) handles dispatching instructions to the instruction queue, telling the instruction queue to issue instruction, and executing and writing back instructions.\nCommit\nCommits instructions each cycle, handling any faults that the instructions may have caused. Also handles redirecting the front-end in the case of a branch misprediction.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/cpu_models/O3CPU",
            "page_title": "O3CPU",
            "parent_section": "Pipeline stages",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Execute-in-execute model\nFor the O3CPU, we\u2019ve made efforts to make it highly timing accurate. In order to do this, we use a model that actually executes instructions at the execute stage of the pipeline. Most simulator models will execute instructions either at the beginning or end of the pipeline; SimpleScalar and our old detailed CPU model both execute instructions at the beginning of the pipeline and then pass it to a timing backend. This presents two potential problems: first, there is the potential for error in the timing backend that would not show up in program results. Second, by executing at the beginning of the pipeline, the instructions are all executed in order and out-of-order load interaction is lost. Our model is able to avoid these deficiencies and provide an accurate timing model.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/cpu_models/O3CPU",
            "page_title": "O3CPU",
            "parent_section": "Execute-in-execute model",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Template Policies\nThe O3CPU makes heavy use of template policies to obtain a level of polymorphism without having to use virtual functions. It uses template policies to pass in an \u201cImpl\u201d to almost all of the classes used within the O3CPU. This Impl has defined within it all of the important classes for the pipeline, such as the specific Fetch class, Decode class, specific DynInst types, the CPU class, etc. It allows any class that uses it as a template parameter to be able to obtain full type information of any of the classes defined within the Impl. By obtaining full type information, there is no need for the traditional virtual functions/base classes which are normally used to provide polymorphism. The main drawback is that the CPU must be entirely defined at compile time, and that the templated classes require manual instantiation. See\nsrc/cpu/o3/impl.hh\nand\nsrc/cpu/o3/cpu_policy.hh\nfor example Impl classes.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/cpu_models/O3CPU",
            "page_title": "O3CPU",
            "parent_section": "Template Policies",
            "section_heading": "Overview"
        }
    },
    {
        "text": "ISA independence\nThe O3CPU has been designed to try to separate code that is ISA dependent and code that is ISA independent. The pipeline stages and resources are all mainly ISA independent, as well as the lower level CPU code. The ISA dependent code implements ISA-specific functions. For example, the AlphaO3CPU implements Alpha-specific functions, such as hardware return from error interrupt (hwrei()) or reading the interrupt flags. The lower level CPU, the FullO3CPU, handles orchestrating all of the pipeline stages and handling other ISA-independent actions. We hope this separation makes it easier to implement future ISAs, as hopefully only the high level classes will have to be redefined.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/cpu_models/O3CPU",
            "page_title": "O3CPU",
            "parent_section": "ISA independence",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Interaction with ThreadContext\nThe\nThreadContext\nprovides interface for external objects to access thread state within the CPU. However, this is slightly complicated by the fact that the O3CPU is an out-of-order CPU. While it is well defined what the architectural state is at any given cycle, it is not well defined what happens if that architectural state is changed. Thus it is feasible to do reads to the ThreadContext without much effort, but doing writes to the ThreadContext and altering register state requires the CPU to flush the entire pipeline. This is because there may be in flight instructions that depend on the register that has been changed, and it is unclear if they should or should not view the register update. Thus accesses to the ThreadContext have the potential to cause slowdown in the CPU simulation.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/cpu_models/O3CPU",
            "page_title": "O3CPU",
            "parent_section": "Interaction with ThreadContext",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Section: Backend Pipeline\n\nSub-section: Compute Instructions\n\nCompute instructions are simpler as they do not access memory and\ndo not interact with the LSQ. Included below is a high-level calling chain\n(only important functions) with a description about the functionality of each.\nRename\n::\ntick\n()\n->\nRename\n::\nRenameInsts\n()\nIEW\n::\ntick\n()\n->\nIEW\n::\ndispatchInsts\n()\nIEW\n::\ntick\n()\n->\nInstructionQueue\n::\nscheduleReadyInsts\n()\nIEW\n::\ntick\n()\n->\nIEW\n::\nexecuteInsts\n()\nIEW\n::\ntick\n()\n->\nIEW\n::\nwritebackInsts\n()\nCommit\n::\ntick\n()\n->\nCommit\n::\ncommitInsts\n()\n->\nCommit\n::\ncommitHead\n()\nRename (\nRename::renameInsts()\n).\nAs suggested by the name, registers are renamed and the instruction\nis pushed to the IEW stage. It checks that the IQ/LSQ can hold the new\ninstruction.\nDispatch (\nIEW::dispatchInsts()\n).\nThis function inserts the renamed instruction into the IQ and LSQ.\nSchedule (\nInstructionQueue::scheduleReadyInsts()\n)\nThe IQ manages the ready instructions (operands ready) in a ready list,\nand schedules them to an available FU. The latency of the FU is set here,\nand instructions are sent to execution when the FU done.\nExecute (\nIEW::executeInsts()\n).\nHere\nexecute()\nfunction of the compute instruction is invoked and\nsent to commit. Please note\nexecute()\nwill write results to the destiniation\nregister.\nWriteback (\nIEW::writebackInsts()\n).\nHere\nInstructionQueue::wakeDependents()\nis invoked. Dependent\ninstructions will be added to the ready list for scheduling.\nCommit (\nCommit::commitInsts()\n).\nOnce the instruction reaches the head of ROB, it will be committed and\nreleased from ROB.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/cpu_models/O3CPU",
            "page_title": "O3CPU",
            "parent_section": "Backend Pipeline",
            "section_heading": "Compute Instructions"
        }
    },
    {
        "text": "Section: Backend Pipeline\n\nSub-section: Load Instruction\n\nLoad instructions share the same path as compute instructions until\nexecution.\nIEW\n::\ntick\n()\n->\nIEW\n::\nexecuteInsts\n()\n->\nLSQUnit\n::\nexecuteLoad\n()\n->\nStaticInst\n::\ninitiateAcc\n()\n->\nLSQ\n::\npushRequest\n()\n->\nLSQUnit\n::\nread\n()\n->\nLSQRequest\n::\nbuildPackets\n()\n->\nLSQRequest\n::\nsendPacketToCache\n()\n->\nLSQUnit\n::\ncheckViolation\n()\nDcachePort\n::\nrecvTimingResp\n()\n->\nLSQRequest\n::\nrecvTimingResp\n()\n->\nLSQUnit\n::\ncompleteDataAccess\n()\n->\nLSQUnit\n::\nwriteback\n()\n->\nStaticInst\n::\ncompleteAcc\n()\n->\nIEW\n::\ninstToCommit\n()\nIEW\n::\ntick\n()\n->\nIEW\n::\nwritebackInsts\n()\nLSQUnit::executeLoad()\nwill initiate the access by invoking the\ninstruction\u2019s\ninitiateAcc()\nfunction. Through the execution context interface,\ninitiateAcc()\nwill call\ninitiateMemRead()\nand eventually be directed\nto\nLSQ::pushRequest()\n.\nLSQ::pushRequest()\nwill allocate a\nLSQRequest\nto track all states, and\nstart translation. When the translation completes, it will\nrecord the virtual address and invoke\nLSQUnit::read()\n.\nLSQUnit::read()\nwill check if the load is aliased with any previous\nstore.\nIf can it can forward, then it will schedule\nWritebackEvent\nfor the next\ncycle.\nIf it is aliased but cannot forward, it calls\nInstructionQueue::rescheduleMemInst()\nand\nLSQReuqest::discard()\n.\nOtherwise, it send packets to the cache.\nLSQUnit::writeback()\nwill invoke\nStaticInst::completeAcc()\n, which\nwill write a loaded value to the destination register. The\ninstruction is then pushed to the commit queue.\nIEW::writebackInsts()\nwill then mark it done and wake up its dependents. Starting from here it\nshares same path as compute instructions.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/cpu_models/O3CPU",
            "page_title": "O3CPU",
            "parent_section": "Backend Pipeline",
            "section_heading": "Load Instruction"
        }
    },
    {
        "text": "Section: Backend Pipeline\n\nSub-section: Store Instruction\n\nStore instructions are similar to load instructions, but only writeback\nto cache after committed.\nIEW\n::\ntick\n()\n->\nIEW\n::\nexecuteInsts\n()\n->\nLSQUnit\n::\nexecuteStore\n()\n->\nStaticInst\n::\ninitiateAcc\n()\n->\nLSQ\n::\npushRequest\n()\n->\nLSQUnit\n::\nwrite\n()\n->\nLSQUnit\n::\ncheckViolation\n()\nCommit\n::\ntick\n()\n->\nCommit\n::\ncommitInsts\n()\n->\nCommit\n::\ncommitHead\n()\nIEW\n::\ntick\n()\n->\nLSQUnit\n::\ncommitStores\n()\nIEW\n::\ntick\n()\n->\nLSQUnit\n::\nwritebackStores\n()\n->\nLSQRequest\n::\nbuildPackets\n()\n->\nLSQRequest\n::\nsendPacketToCache\n()\n->\nLSQUnit\n::\nstorePostSend\n()\nDcachePort\n::\nrecvTimingResp\n()\n->\nLSQRequest\n::\nrecvTimingResp\n()\n->\nLSQUnit\n::\ncompleteDataAccess\n()\n->\nLSQUnit\n::\ncompleteStore\n()\nUnlike\nLSQUnit::read()\n,\nLSQUnit::write()\nwill only copy the store\ndata, but not send the packet to cache, as the store is not committed yet.\nAfter the store is committed,\nLSQUnit::commitStores()\nwill mark the SQ\nentry as\ncanWB\nso that\nLSQUnit::writebackStores()\nwill send\nthe store request to cache.\nFinally, when the response comes back,\nLSQUnit::completeStore()\nwill\nrelease the SQ entries.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/cpu_models/O3CPU",
            "page_title": "O3CPU",
            "parent_section": "Backend Pipeline",
            "section_heading": "Store Instruction"
        }
    },
    {
        "text": "Section: Backend Pipeline\n\nSub-section: Branch Misspeculation\n\nBranch misspeculation is handled in\nIEW::executeInsts()\n. It will\nnotify the commit stage to start squashing all instructions in the ROB\non the misspeculated branch.\nIEW\n::\ntick\n()\n->\nIEW\n::\nexecuteInsts\n()\n->\nIEW\n::\nsquashDueToBranch\n()",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/cpu_models/O3CPU",
            "page_title": "O3CPU",
            "parent_section": "Backend Pipeline",
            "section_heading": "Branch Misspeculation"
        }
    },
    {
        "text": "Section: Backend Pipeline\n\nSub-section: Memory Order Misspeculation\n\nThe\nInstructionQueue\nhas a\nMemDepUnit\nto track memory order dependence.\nThe IQ will not schedule an instruction if MemDepUnit states there is\ndependency.\nIn\nLSQUnit::read()\n, the LSQ will search for possible aliasing store and\nforward if possible. Otherwise, the load is blocked and rescheduled for when\nthe blocking store completes by notifying the MemDepUnit.\nBoth\nLSQUnit::executeLoad/Store()\nwill call\nLSQUnit::checkViolation()\nto search the LQ for possible misspeculation. If found, it will set\nLSQUnit::memDepViolator\nand\nIEW::executeInsts()\nwill start later to\nsquash the misspeculated instructions.\nIEW\n::\ntick\n()\n->\nIEW\n::\nexecuteInsts\n()\n->\nLSQUnit\n::\nexecuteLoad\n()\n->\nStaticInst\n::\ninitiateAcc\n()\n->\nLSQUnit\n::\ncheckViolation\n()\n->\nIEW\n::\nsquashDueToMemOrder\n()",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/cpu_models/O3CPU",
            "page_title": "O3CPU",
            "parent_section": "Backend Pipeline",
            "section_heading": "Memory Order Misspeculation"
        }
    },
    {
        "text": "BaseSimpleCPU\nThe BaseSimpleCPU serves several purposes:\nHolds architected state, stats common across the SimpleCPU models.\nDefines functions for checking for interrupts, setting up a fetch request, handling pre-execute setup, handling post-execute actions, and advancing the PC to the next instruction. These functions are also common across the SimpleCPU models.\nImplements the ExecContext interface.\nThe BaseSimpleCPU can not be run on its own. You must use one of the classes that inherits from BaseSimpleCPU, either AtomicSimpleCPU or TimingSimpleCPU.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/cpu_models/SimpleCPU",
            "page_title": "SimpleCPU",
            "parent_section": "BaseSimpleCPU",
            "section_heading": "Overview"
        }
    },
    {
        "text": "AtomicSimpleCPU\nThe AtomicSimpleCPU is the version of SimpleCPU that uses atomic memory accesses (see\nMemory systems\nfor details). It uses the latency estimates from the atomic accesses to estimate overall cache access time. The AtomicSimpleCPU is derived from BaseSimpleCPU, and implements functions to read and write memory, and also to tick, which defines what happens every CPU cycle. It defines the port that is used to hook up to memory, and connects the CPU to the cache.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/cpu_models/SimpleCPU",
            "page_title": "SimpleCPU",
            "parent_section": "AtomicSimpleCPU",
            "section_heading": "Overview"
        }
    },
    {
        "text": "TimingSimpleCPU\nThe TimingSimpleCPU is the version of SimpleCPU that uses timing memory accesses (see\nMemory systems\nfor details). It stalls on cache accesses and waits for the memory system to respond prior to proceeding. Like the AtomicSimpleCPU, the TimingSimpleCPU is also derived from BaseSimpleCPU, and implements the same set of functions. It defines the port that is used to hook up to memory, and connects the CPU to the cache. It also defines the necessary functions for handling the response from memory to the accesses sent out.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/cpu_models/SimpleCPU",
            "page_title": "SimpleCPU",
            "parent_section": "TimingSimpleCPU",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Overview\nThe Trace CPU model plays back elastic traces, which are dependency and timing annotated traces generated by the Elastic Trace Probe attached to the O3 CPU model. The focus of the Trace CPU model is to achieve memory-system (cache-hierarchy, interconnects and main memory) performance exploration in a fast and reasonably accurate way instead of using the detailed but slow O3 CPU model. The traces have been developed for single-threaded benchmarks simulating in both SE and FS mode. They have been correlated for 15 memory-sensitive SPEC 2006 benchmarks and a handful of HPC proxy apps by interfacing the Trace CPU with classic memory system and varying cache design parameters and DRAM memory type. In general, elastic traces can be ported to other simulation environments.\nPublication\n:\nExploring System Performance using Elastic Traces: Fast, Accurate and Portable\u201d\nRadhika Jagtap, Stephan Diestelhorst, Andreas Hansson, Matthias Jung and Norbert Wehn SAMOS 2016\nTrace generation and replay methodology",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/cpu_models/TraceCPU",
            "page_title": "TraceCPU",
            "parent_section": "Overview",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Section: Elastic Trace Generation\n\nSub-section: Trace file formats\n\nThe elastic data memory trace and fetch request trace are both encoded using google protobuf.\nElastic Trace fields in protobuf format\nFields\nDiscritption\nrequired uint64 seq_num\nInstruction number used as an id for tracking dependencies\nrequired RecordType type\nRecordType enum has values: INVALID, LOAD, STORE, COMP\noptional uint64 p_addr\nPhysical memory address if instruction is a load/store\noptional uint32 size\nSize in bytes of data if instruction is a load/store\noptional uint32 flags\nFlags or attributes of the access, ex. Uncacheable\nrequired uint64 rob_dep\nPast instruction number on which there is order (ROB) dependency\nrequired uint64 comp_delay\nExecution delay between the completion of the last dependency and the execution of the instruction\nrepeated uint64 reg_dep\nPast instruction number on which there is RAW data dependency\noptional uint32 weight\nTo account for committed instructions that were filtered out\noptional uint64 pc\nInstruction address, i.e. the program counter\noptional uint64 v_addr\nVirtual memory address if instruction is a load/store\noptional uint32 asid\nAddress Space ID\nA decode script in Python is available at\nutil/decode_inst_dep_trace.py\nthat outputs the trace in ASCII format.\nExample of a trace in ASCII\n1,356521,COMP,8500::\n\n2,35656,1,COMP,0:,1:\n\n3,35660,1,LOAD,1748752,4,74,500:,2:\n\n4,35660,1,COMP,0:,3:\n\n5,35664,1,COMP,3000::,4\n\n6,35666,1,STORE,1748752,4,74,1000:,3:,4,5\n\n7,35666,1,COMP,3000::,4\n\n8,35670,1,STORE,1748748,4,74,0:,6,3:,7\n\n9,35670,1,COMP,500::,7\nEach record in the instruction fetch trace has the following fields.\nFields\nDiscritption\nrequired uint64 tick\nTimestamp of the access\nrequired uint32 cmd\nRead or Write (in this case always Read)\nrequired uint64 addr\nPhysical memory address\nrequired uint32 size\nSize in bytes of data\noptional uint32 flags\nFlags or attributes of the access\noptional uint64 pkt_id\nId of the access\noptional uint64 pc\nInstruction address, i.e. the program counter\nThe decode script in Python at\nutil/decode_packet_trace.py\ncan be used to output the trace in ASCII format.\nCompile dependencies\n:\nYou need to install google protocol buffer as the traces are recorded using this.\nsudo\napt-get\ninstall\nprotobuf-compiler\nsudo\napt-get\ninstall\nlibprotobuf-dev",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/cpu_models/TraceCPU",
            "page_title": "TraceCPU",
            "parent_section": "Elastic Trace Generation",
            "section_heading": "Trace file formats"
        }
    },
    {
        "text": "Section: Elastic Trace Generation\n\nSub-section: Scripts and options\n\nSE mode\nbuild/ARM/gem5.opt configs/example/arm/etrace_se.py \\\n    --inst-trace-file fetchtrace.proto.gz \\\n    --data-trace-file deptrace.proto.gz \\\n    [WORKLOAD]\nFS mode\nCreate a checkpoint for your region of interest and resume from the checkpoint but with O3 CPU model and tracing enabled.\n# Checkpoint generation\n# NOTE: fs.py is deprecated and will be removed. Do not rely too much on it\nbuild/ARM/gem5.opt --outdir=m5out/bbench \\\n    ./configs/deprecated/example/fs.py [fs.py options] \\\n    --benchmark bbench-ics\n# Checkpoint restore\n# NOTE: fs.py is deprecated and will be removed. Do not rely too much on it\nbuild/ARM/gem5.opt --outdir=m5out/bbench/capture_10M \\\n    ./configs/deprecated/example/fs.py [fs.py options] \\\n    --cpu-type=arm_detailed --caches \\\n    --elastic-trace-en --data-trace-file=deptrace.proto.gz --inst-trace-file=fetchtrace.proto.gz \\\n    --mem-type=SimpleMemory \\\n    --checkpoint-dir=m5out/bbench -r 0 --benchmark bbench-ics -I 10000000",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/cpu_models/TraceCPU",
            "page_title": "TraceCPU",
            "parent_section": "Elastic Trace Generation",
            "section_heading": "Scripts and options"
        }
    },
    {
        "text": "Section: Replay with Trace CPU\n\nSub-section: Scripts and options\n\nA trace replay script in the examples folder can be used to play back SE and FS generated traces\nbuild/ARM/gem5.opt [gem5.opt options] -d bzip_10Minsts_replay configs/example/etrace_replay.py [options] --caches --data-trace-file=bzip_10Minsts/deptrace.proto.gz --inst-trace-file=bzip_10Minsts/fetchtrace.proto.gz --mem-size=4GB\nFields\nDiscritption\nrequired uint64 seq_num\nTimestamp of the access\nrequired RecordType type\nRead or Write (in this case always Read)\noptional uint64 p_addr\nPhysical memory address if instruction is a load/store\noptional uint32 size\nSize in bytes of data if instruction is a load/store\noptional uint32 flags\nFlags or attributes of the access, ex. Uncacheable\nrequired uint64 rob_dep\nPast instruction number on which there is order (ROB) dependency\nrequired uint64 comp_delay\nExecution delay between the completion of the last dependency and the execution of the instruction\nrepeated uint64 reg_dep\nPast instruction number on which there is RAW data dependency\noptional uint32 weight\nTo account for committed instructions that were filtered out\noptional uint64 pc\nInstruction address, i.e. the program counter\noptional uint64 v_addr\nVirtual memory address if instruction is a load/store\noptional uint32 asid\nAddress Space ID",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/cpu_models/TraceCPU",
            "page_title": "TraceCPU",
            "parent_section": "Replay with Trace CPU",
            "section_heading": "Scripts and options"
        }
    },
    {
        "text": "gem5 bootcamp 2022 module on instruction execution\ngem5 bootcamp (2022) had a session on learning how instructions work in gem5 and how to add new instructions in gem5.\nThe slides presented in the session can be found\nhere\n.\nThe youtube video of the recorded bootcamp module on gem5 instructions is available\nhere\n.\nStaticInsts\nThe StaticInst provides all static information and methods for a binary instruction.\nIt holds the following information/methods:\nFlags to tell what kind of instruction it is (integer, floating point, branch, memory barrier, etc.)\nThe op class of the instruction\nThe number of source and destination registers\nThe number of integer and FP registers used\nMethod to decode a binary instruction into a StaticInst\nVirtual function execute(), which defines how the specific architectural actions taken for an instruction (e.g. read r1, r2, add them and store in r3.)\nVirtual functions to handle starting and completing memory operations\nVirtual functions to execute the address calculation and memory access separately for models that split memory operations into two operations\nMethod to disassemble the instruction, printing it out in a human readable format. (e.g. addq r1 r2 r3)\nIt does not have dynamic information, such as the PC of the instruction or the values of the source registers or the result. This allows a 1 to 1 mapping of StaticInst to unique binary machine instructions. We take advantage of this fact by caching the mapping of a binary instruction to a StaticInst in a hash_map, allowing us to decode a binary instruction only once, and directly using the StaticInst the rest of the time.\nEach ISA instruction derives from StaticInst and implements its own constructor, the execute() function, and, if it is a memory instruction, the memory access functions. See ISA_description_system for details about how these ISA instructions are specified.\nDynInsts\nThe DynInst is used to hold dynamic information about instructions. This is necessary for more detailed models or out-of-order models, both of which may need extra information beyond the\nStaticInsts\nin order to correctly execute instructions.\nSome of the dynamic information that it stores includes:\nThe PC of the instruction\nThe renamed register indices of the source and destination registers\nThe predicted next-PC\nThe instruction result\nThe thread number of the instruction\nThe CPU the instruction is executing on\nWhether or not the instruction is squashed\nAdditionally the DynInst provides the ExecContext interface. When ISA instructions are executed, the DynInst is passed in as the ExecContext, handling all accesses of the ISA to CPU state.\nDetailed CPU models can derive from DynInst and create their own specific DynInst subclasses that implement any additional state or functions that might be needed. See src/cpu/o3/alpha/dyn_inst.hh for an example of this.\nMicrocode support\nExecContext\nThe ExecContext describes the interface that the ISA uses to access CPU state. Although there is a file\nsrc/cpu/exec_context.hh\n, it is purely for documentation purposes and classes do not derive from it. Instead, ExecContext is an implicit interface that is assumed by the ISA.\nThe ExecContext interface provides methods to:\nRead and write PC information\nRead and write integer, floating point, and control registers\nRead and write memory\nRecord and return the address of a memory access, prefetching, and trigger a system call\nTrigger some full-system mode functionality\nExample implementations of the ExecContext interface include:\nSimpleCPU\nDynInst\nSee the ISA description page for more details on how an instruction set is implemented.\nThreadContext\nThreadContext is the interface to all state of a thread for anything outside of the CPU. It provides methods to read or write state that might be needed by external objects, such as the PC, next PC, integer and FP registers, and IPRs. It also provides functions to get pointers to important thread-related classes, such as the ITB, DTB, System, kernel statistics, and memory ports. It is an abstract base class; the CPU must create its own ThreadContext by either deriving from it, or using the templated ProxyThreadContext class.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/cpu_models/execution_basics",
            "page_title": "Execution basic",
            "parent_section": "gem5 bootcamp 2022 module on instruction execution",
            "section_heading": "Overview"
        }
    },
    {
        "text": "ProxyThreadContext\nThe ProxyThreadContext class provides a way to implement a ThreadContext without having to derive from it. ThreadContext is an abstract class, so anything that derives from it and uses its interface will pay the overhead of virtual function calls. This class is created to enable a user-defined Thread object to be used wherever ThreadContexts are used, without paying the overhead of virtual function calls when it is used by itself. The user-defined object must simply provide all the same functions as the normal ThreadContext, and the ProxyThreadContext will forward all calls to the user-defined object. See the code of\nSimpleThread\nfor an example of using the ProxyThreadContext.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/cpu_models/execution_basics",
            "page_title": "Execution basic",
            "parent_section": "ProxyThreadContext",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Difference vs. ExecContext\nThe ThreadContext is slightly different than the ExecContext. The ThreadContext provides access to an individual thread\u2019s state; an ExecContext provides ISA access to the CPU (meaning it is implicitly multithreaded on SMT systems). Additionally the ThreadState is an abstract class that exactly defines the interface; the ExecContext is a more implicit interface that must be implemented so that the ISA can access whatever state it needs. The function calls to access state are slightly different between the two. The ThreadContext provides read/write register methods that take in an architectural register index. The ExecContext provides read/write register methdos that take in a StaticInst and an index, where the index refers to the i\u2019th source or destination register of that\nStaticInsts\n. Additionally the ExecContext provides read and write methods to access memory, while the ThreadContext does not provide any methods to access memory.\nThreadState\nThe ThreadState class is used to hold thread state that is common across CPU models, such as the thread ID, thread status, kernel statistics, memory port pointers, and some statistics of number of instructions completed. Each CPU model can derive from ThreadState and build upon it, adding in thread state that is deemed appropriate. An example of this is\nSimpleThread\n, where all of the thread\u2019s architectural state has been added in. However, it is not necessary (or even feasible in some cases) for all of the thread\u2019s state to be centrally located in a ThreadState derived class. The DetailedCPU keeps register values and rename maps in its own classes outside of ThreadState. ThreadState is only used to provide a more convenient way to centrally locate some state, and provide sharing across CPU models.\nFaults\nRegisters",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/cpu_models/execution_basics",
            "page_title": "Execution basic",
            "parent_section": "Difference vs. ExecContext",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Register types - float, int, misc",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/cpu_models/execution_basics",
            "page_title": "Execution basic",
            "parent_section": "Register types - float, int, misc",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Indexing - register spaces stuff\nSee\nRegister Indexing\nfor a more thorough treatment.\nA \u201cnickle tour\u201d of flattening and register indexing in the CPU models.\nFirst, an instruction has identified that it needs register such and such as determined by its encoding (or the fact that it always uses a certain register, or \u2026). For the sake of argument, lets say we\u2019re talking about SPARC, the register is %g1, and the second bank of globals is active. From the instructions point of view, the unflattened register is %g1, which, likely, is just represented by the index 1.\nNext, we need to map from the instruction\u2019s view of the register file(s) down to actual storage locations. Think of this like virtual memory. The instruction is working within an index space which is like a virtual address space, and it needs to be mapped down to the flattened space which is like physical memory. Here, the index 1 is likely mapped to, say, 9, where 0-7 is the first bank of globals and 8-15 is the second.\nThis is the point where the CPU gets involved. The index 9 refers to an actual register the instruction expects to access, and it\u2019s the CPU\u2019s job to make that happen. Before this point, all the work was done by the ISA with no insight available to the CPU, and beyond this point all the work is done by the CPU with no insight available to the ISA.\nThe CPU is free to provide a register directly like the simple CPU by having an array and just reading and writing the 9th element on behalf of the instruction. The CPU could, alternatively, do something complicated like renaming and mapping the flattened index further into a physical register like O3.\nOne important property of all this, which makes sense if you think about the virtual memory analogy, is that the size of the index space before flattening has nothing to do with the size after. The virtual memory space could be very large (presumably with gaps) and map to a smaller physical space, or it could be small and map to a larger physical space where the extra is for, say, other virtual spaces used at other times. You need to make sure you\u2019re using the right size (post flattening) to size your tables because that\u2019s the space of possible options.\nOne other tricky part comes from the fact that we add offsets into the indices to distinguish ints from floats from miscs. Those offsets might be one thing in the preflattening world, but then need to be something else in the post flattening world to keep things from landing on top of each other without leaving gaps. It\u2019s easy to make a mistake here, and it\u2019s one of the reasons I don\u2019t like this offset idea as a way to keep the different types separate. I\u2019d rather see a two dimensional index where the second coordinate was a register type. But in the world as it exists today, this is something you have to keep track of.\nPCs\nRegister Indexing\nCPU register indexing in gem5 is a complicated by the need to support multiple ISAs with sometimes very different register semantics (register windows, condition codes, mode-based alternate register sets, etc.). In addition, this support has evolved gradually as new ISAs have been added, so older code may not take advantage of newer features or terminology.\nTypes of Register Indices\nThere are three types of register indices used internally in the CPU models: relative, unified, and flattened.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/cpu_models/execution_basics",
            "page_title": "Execution basic",
            "parent_section": "Indexing - register spaces stuff",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Relative\nA relative register index is the index that is encoded in a machine instruction. There is a separate index space for each class of register (integer, floating point, etc.), starting at 0. The register class is implied by the opcode. Thus a value of \u201c1\u201d in a source register field may mean integer register 1 (e.g., \u201c%r1\u201d) or floating point register 1 (e.g., \u201c%f1\u201d) depending on the type of the instruction.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/cpu_models/execution_basics",
            "page_title": "Execution basic",
            "parent_section": "Relative",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Unified\nWhile relative register indices are good for keeping instruction encodings compact, they are ambiguous, and thus not convenient for things like managing dependencies. To avoid this ambiguity, the decoder maps the relative register indices into a unified register space by adding class-specific offsets to relocate each relative index range into a unique position. Integer registers are unmodified, and continue to start at zero. Floating-point register indices are offset by (at least) the number of integer registers, so that the first FP register (e.g., \u201c%f0\u201d) gets a unified index that is greater than that of the last integer register. Similarly, miscellaneous (a.k.a. control) registers are mapped past the end of the FP register index space.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/cpu_models/execution_basics",
            "page_title": "Execution basic",
            "parent_section": "Unified",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Flattened\nUnified register indices provide an unambiguous description of all the registers that are accessible as instruction operands at a given point in the execution. Unfortunately, due to the complex features of some ISAs, they do not always unambiguously identify the actual state that the instruction is referencing. For example, in ISAs with register windows (notably SPARC), a particular register identifier such as \u201c%o0\u201d will refer to a different register after a \u201csave\u201d or \u201crestore\u201d operation than it did previously. Several ISAs have registers that are hidden in normal operation, but get mapped on top of ordinary registers when an interrupt occurs (e.g., ARM\u2019s mode-specific registers), or under explicit supervisor control (e.g., SPARC\u2019s \u201calternate globals\u201d).\nWe solve this problem by maintaining a flattened register space which provides a distinct index for every unique register storage location. For example, the integer portion of the SPARC flattened register space has distinct indices for the globals and the alternate globals, as well as for each of the available register windows. The \u201cflattening\u201d process of translating from a unified or relative register index to a flattened register index varies by ISA. On some ISAs, the mapping is trivial, while others use table lookups to do the translation.\nA key distinction between the generation of unified and flattened register indices is that the former can always be done statically while the latter often depends on dynamic processor state. That is, the translation from relative to unified indices depends only on the context provided by the instruction itself (which is convenient as the translation is done in the decoder). In contrast, the mapping to a flattened register index may depend on processor state such as the interrupt level or the current window pointer on SPARC.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/cpu_models/execution_basics",
            "page_title": "Execution basic",
            "parent_section": "Flattened",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Combining Register Index Types\nAlthough the typical progression for modifying register indices is relative -> unified -> flattened, it turns out that relative vs. unified and flattened vs. unflattened are orthogonal attributes. Relative vs. unified indicates whether the index is relative to the base register for its register class (integer, FP, or misc) or has the base offset for its class added in. Flattened vs. unflattened indicates whether the index has been adjusted to account for runtime context such as register window adjustments or alternate register file modes. Thus a relative flattened register index is one in which the runtime context has been accounted for, but is still expressed relative to the base offset for its class.\nA single set of class-specific offsets is used to generate unified indices from relative indices regardless of whether the indices are flattened or unflattened. Thus the offsets must be large enough to separate the register classes even when flattened addresses are being used. As a result, the unflattened unified register space is often discontiguous.\nIllustrations\nAs an illustration, consider a hypothetical architecture with four integer registers (%r0-%r4), three FP registers (%f0-%f2), and two misc/control registers (%msr0-%msr1). In addition, the architecture supports a complete set of alternate integer and FP registers for fast context switching.\nThe resulting register file layout, along with the unified flattened register file indices, is shown at right. Although the indices in the picture range from 0 to 15, the actual set of valid indices depends on the type of index and (for relative indices) the register class as well:\nRelative unflattened\nInt: 0-3; FP: 0-2; Misc: 0-1\nUnified unflattened\n0-3, 8-10, 14-15\nRelative flattened\nInt: 0-7; FP: 0-5; Misc: 0-1\nUnified flattened\n0-15\nIn this example, register %f1 in the alternate FP register file could be referred to via the relative flattened index 4 as well as the relative unflattened index 1, the unified unflattened index 9, or the unified flattened index 12. Note that the difference between the relative and unified indices is always 8 (regardless of flattening), and the difference between the unflattened and flattened indices is 3 (regardless of relative vs. unified status).\nCaveats\nAlthough the gem5 code is unfortunately not always clear about which type of register index is expected by a particular function, functions whose name incorporates a register class (e.g., readIntReg()) expect a relative register index, and functions that expect a flattened index often have \u201cflat\u201d in the function name.\nAlthough the general case is complicated, the common case can be deceptively simple. For example, because integer registers start at the beginning of the unified register space, relative and unified register indices are identical for integer registers. Furthermore, in an architecture with no (or rarely-used) alternate integer registers, the unflattened and flattened indices are (almost always) the same as well, meaning that all four types of register indices are interchangeable in this case. While this situation seems to be a simplification, it also tends to hide bugs where the wrong register index type is used.\nThe description above is intended to illustrate the typical usage of these index types. There may be exceptions that don\u2019t precisely   follow this description, but I got tired of writing \u201ctypically\u201d in every sentence.\nThe terms \u2018relative\u2019 and \u2018unified\u2019 were invented for use in this documentation, so you are unlikely see them in the code until the code starts catching up with this page.\nThis discussion pertains only to the architectural registers. An out-of-order CPU model such as O3 adds another layer of complexity by renaming these architectural registers (using the flattened register indices) to an underlying physical register file.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/cpu_models/execution_basics",
            "page_title": "Execution basic",
            "parent_section": "Combining Register Index Types",
            "section_heading": "Overview"
        }
    },
    {
        "text": "ISA and CPU Independence\ngem5 tries to keep CPU models ISA independent to make it easier to use any ISA with different CPU models. gem5 relies on two generic interfaces to make this independence possible: static instructions and execution context (both are discussed above).\nStatic instructions allow CPU to manage instructions and the execution context allow ISA or instructions to interact with the CPU. Following picture provides a high level overview of\nwhat components in gem5 are ISA dependent or independent:\n\nSource of the above figure:\nModular ISA-Independent Full-System Simulation (Ch 5 of Processor and System-on-Chip Simulation), G. Black, N. Binkert, and S. Reinhardt, A. Saidi.\nLink\n.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/cpu_models/execution_basics",
            "page_title": "Execution basic",
            "parent_section": "ISA and CPU Independence",
            "section_heading": "Overview"
        }
    },
    {
        "text": "What is Minor?\nMinor\nis an in-order\nprocessor model with a fixed pipeline but configurable data structures and\nexecute behaviour. It is intended to be used to model processors with strict\nin-order execution behaviour and allows visualisation of an instruction\u2019s\nposition in the pipeline through the MinorTrace/minorview.py format/tool. The\nintention is to provide a framework for micro-architecturally correlating the\nmodel with a particular, chosen processor with similar capabilities.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/cpu_models/minor_cpu",
            "page_title": "No Title Found",
            "parent_section": "What is Minor?",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Section: Design Philosophy\n\nSub-section: Multithreading\n\nThe model isn\u2019t currently capable of multithreading but there are THREAD\ncomments in key places where stage data needs to be arrayed to support\nmultithreading.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/cpu_models/minor_cpu",
            "page_title": "No Title Found",
            "parent_section": "Design Philosophy",
            "section_heading": "Multithreading"
        }
    },
    {
        "text": "Section: Design Philosophy\n\nSub-section: Data structures\n\nDecorating data structures with large amounts of life-cycle information is\navoided. Only instructions\n(\nMinorDynInst\n) contain a\nsignificant proportion of their data content whose values are not set at\nconstruction.\nAll internal structures have fixed sizes on construction. Data held in queues\nand FIFOs (\nMinorBuffer\n,\nFUPipeline\n) should have\na\nBubbleIF\ninterface to allow a distinct \u2018bubble\u2019/no data value option for each type.\nInter-stage \u2018struct\u2019 data is packaged in structures which are passed by value.\nOnly\nMinorDynInst\n, the line\ndata in\nForwardLineData\nand the memory-interfacing objects\nFetch1::FetchRequest\nand\nLSQ::LSQRequest\nare\n::new\nallocated while running the model.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/cpu_models/minor_cpu",
            "page_title": "No Title Found",
            "parent_section": "Design Philosophy",
            "section_heading": "Data structures"
        }
    },
    {
        "text": "Model structure\nObjects of class\nMinorCPU\nare provided by the\nmodel to gem5.\nMinorCPU\nimplements the\ninterfaces of (cpu.hh) and can provide data and instruction interfaces for\nconnection to a cache system. The model is configured in a similar way to other\ngem5 models through Python. That configuration is passed on to\nMinorCPU::pipeline\n(of class\nPipeline\n) which\nactually implements the processor pipeline.\nThe hierarchy of major unit ownership from\nMinorCPU\ndown looks like this:\nMinorCPU\n--- Pipeline - container for the pipeline, owns the cyclic 'tick' event mechanism and the idling (cycle skipping) mechanism.\n--- --- Fetch1 - instruction fetch unit responsible for fetching cache lines (or parts of lines from the I-cache interface).\n--- --- --- Fetch1::IcachePort - interface to the I-cache from Fetch1.\n--- --- Fetch2 - line to instruction decomposition.\n--- --- Decode - instruction to micro-op decomposition.\n--- --- Execute - instruction execution and data memory interface.\n--- --- --- LSQ - load store queue for memory ref. instructions.\n--- --- --- LSQ::DcachePort - interface to the D-ache from Execute.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/cpu_models/minor_cpu",
            "page_title": "No Title Found",
            "parent_section": "Model structure",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Section: Key data structures\n\nSub-section: Instruction and line identity: Instld (dyn_inst.hh)\n\n- T/S.P/L - for fetched cache lines\n- T/S.P/L/F - for instructions before Decode\n- T/S.P/L/F.E - for instructions from Decode onwards\nfor example:\n- 0/10.12/5/6.7\nInstId\nfields\nare:\nField\nSymbol\nGenerated by\nChecked by\nFunction\nInstId::threadId\nT\nFetch1\nEverywhere the thread number is needed\nThread number (currently always 0).\nInstId::streamSeqNum\nS\nExecute\nFetch1, Fetch2, Execute (to discard lines/insts)\nStream sequence number as chosen by Execute. Stream sequence numbers change after changes of PC (branches, exceptions) in Execue and are used to separate pre and post brnach instrucion streams.\nInstId::predictionSeqNum\nFetch2\nFetch2 (while discarding lines after prediction)\nPrediction sequence numbers represent branch prediction decisions. This is used by Fetch2 to mark lines/instructions/ according to the last followed branch prediction made by Fetch2. Fetch2 can signal to Fetch1 that it should change its fetch address and mark lines with a new prediction sequence number (which it will only do if the stream sequence number Fetch1 expects matches that of the request).\nInstId::lineSeqNum\nFetch1\n(just for debugging)\nLine fetch sequence number of this cache line or the line this instruction was extracted from.\nInstId::fetchSeqNum\nFetch2\nFetch2 (as the inst. sequence number for branches)\nInstruction fetch order assigned by Fetch2 when lines are decomposed into instructions.\nInstId::execSeqNum\nDecode\nExecute (to check instruction identify in queues/FUs/LSQ\nInstruction order after micro-op decomposition\nThe sequence number fields are all independent of each other and although, for\ninstance,\nInstId::execSeqNum\nfor an instruction will always be >=\nInstId::fetchSeqNum\n,\nthe comparison is not useful.\nThe originating stage of each sequence number field keeps a counter for that\nfield which can be incremented in order to generate new, unique numbers.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/cpu_models/minor_cpu",
            "page_title": "No Title Found",
            "parent_section": "Key data structures",
            "section_heading": "Instruction and line identity: Instld (dyn_inst.hh)"
        }
    },
    {
        "text": "Section: Key data structures\n\nSub-section: Instructi ns: MinorDynInst (dyn_inst.hh)\n\nMinorDynInst\nrepresents\nan instruction\u2019s progression through the pipeline. An instruction can be three\nthings:\nThings\nPredicate\nExplanation\nA bubble\nMinorDynInst::isBubble()\nno instruction at all, just a space-filler\nA fault\nMinorDynInst::isFault()\na fault to pass down the pipeline in an insturction\u2019s clothing\nA decoded instruction\nMinorDynInst::isInst()\ninstructions are actually passed to the gem5 decoder in Fetch2 and so are created fully decoded. MinorDynInst::staticInst is the decoded instruction form.\nInstructions are reference counted using the gem5\nRefCountingPtr\n(\nbase/refcnt.hh\n)\nwrapper. They therefore usually appear as MinorDynInstPtr in code. Note that as\nRefCountingPtr\ninitialises as nullptr rather than an object that supports\nBubbleIF::isBubble\npassing raw MinorDynInstPtrs to\nQueues\nand other similar\nstructures from stage.hh without boxing is dangerous.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/cpu_models/minor_cpu",
            "page_title": "No Title Found",
            "parent_section": "Key data structures",
            "section_heading": "Instructi ns: MinorDynInst (dyn_inst.hh)"
        }
    },
    {
        "text": "Section: Key data structures\n\nSub-section: ForwardLineData (pipe_data.hh)\n\nForwardLineData is used to pass cache lines from Fetch1 to Fetch2. Like\nMinorDynInsts, they can be bubbles (\nForwardLineData::isBubble()\n),\nfault-carrying or can contain a line (partial line) fetched by Fetch1. The data\ncarried by ForwardLineData is owned by a Packet object returned from memory and\nis explicitly memory managed and do must be deleted once processed (by Fetch2\ndeleting the Packet).",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/cpu_models/minor_cpu",
            "page_title": "No Title Found",
            "parent_section": "Key data structures",
            "section_heading": "ForwardLineData (pipe_data.hh)"
        }
    },
    {
        "text": "Section: Key data structures\n\nSub-section: ForwardInstData (pipe_data.hh)\n\nForwardInstData can contain up to\nForwardInstData::width()\ninstructions in its\nForwardInstData::insts\nvector. This structure is used to carry instructions between Fetch2, Decode and\nExecute and to store input buffer vectors in Decode and Execute.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/cpu_models/minor_cpu",
            "page_title": "No Title Found",
            "parent_section": "Key data structures",
            "section_heading": "ForwardInstData (pipe_data.hh)"
        }
    },
    {
        "text": "Section: Key data structures\n\nSub-section: Fetch1::FetchRequest (fetch1.hh)\n\nFetchRequests represent I-cache line fetch requests. The are used in the memory\nqueues of Fetch1 and are pushed into/popped from\nPacket::senderState\nwhile traversing the memory system.\nFetchRequests contain a memory system Request (\nmem/request.hh\n) for that fetch access, a\npacket (Packet,\nmem/packet.hh\n), if the request gets to\nmemory, and a fault field that can be populated with a TLB-sourced prefetch\nfault (if any).",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/cpu_models/minor_cpu",
            "page_title": "No Title Found",
            "parent_section": "Key data structures",
            "section_heading": "Fetch1::FetchRequest (fetch1.hh)"
        }
    },
    {
        "text": "Section: Key data structures\n\nSub-section: LSQ::LSQRequest (execute.hh)\n\nLSQRequests are similar to FetchRequests but for D-cache accesses. They carry\nthe instruction associated with a memory access.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/cpu_models/minor_cpu",
            "page_title": "No Title Found",
            "parent_section": "Key data structures",
            "section_heading": "LSQ::LSQRequest (execute.hh)"
        }
    },
    {
        "text": "Section: The pipeline\n\nSub-section: Event handling: MinorActivityRecorder (activity.hh,pipeline.hh)\n\nMinor is essentially a cycle-callable model with some ability to skip cycles\nbased on pipeline activity. External events are mostly received by callbacks\n(e.g.\nFetch1::IcachePort::recvTimingResp\n)\nand cause the pipeline to be woken up to service advancing request queues.\nTicked\n(sim/ticked.hh)\nis a base class bringing together an evaluate member function and a provided\nSimObject\n. It\nprovides a\nTicked::start\n/stop\ninterface to start and pause clock events from being periodically issued.\nPipeline\nis\na derived class of Ticked.\nDuring evaluate calls, stages can signal that they still have work to do in the\nnext cycle by calling either\nMinorCPU::activityRecorder\n->activity()\n(for non-callable related activity) or MinorCPU::wakeupOnEvent(\n) (for\nstage callback-related 'wakeup' activity).\nPipeline::evaluate\ncontains calls to evaluate for each unit and a test for pipeline idling which\ncan turns off the clock tick if no unit has signalled that it may become active\nnext cycle.\nWithin Pipeline (\npipeline.hh\n), the stages are\nevaluated in reverse order (and so will ::evaluate in reverse order) and their\nbackwards data can be read immediately after being written in each cycle\nallowing output decisions to be \u2018perfect\u2019 (allowing synchronous stalling of the\nwhole pipeline). Branch predictions from Fetch2 to Fetch1 can also be\ntransported in 0 cycles making fetch1ToFetch2BackwardDelay the only\nconfigurable delay which can be set as low as 0 cycles.\nThe\nMinorCPU::activateContext\nand\nMinorCPU::suspendContext\ninterface can be called to start and pause threads (threads in the MT sense)\nand to start and pause the pipeline. Executing instructions can call this\ninterface (indirectly through the ThreadContext) to idle the CPU/their threads.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/cpu_models/minor_cpu",
            "page_title": "No Title Found",
            "parent_section": "The pipeline",
            "section_heading": "Event handling: MinorActivityRecorder (activity.hh,pipeline.hh)"
        }
    },
    {
        "text": "Section: The pipeline\n\nSub-section: Each pipeline stage\n\nIn general, the behaviour of a stage (each cycle) is:\nevaluate:\n        push input to inputBuffer\n        setup references to input/output data slots\n\n        do 'every cycle' 'step' tasks\n\n        if there is input and there is space in the next stage:\n            process and generate a new output\n            maybe re-activate the stage\n\n        send backwards data\n\n        if the stage generated output to the following FIFO:\n            signal pipe activity\n\n        if the stage has more processable input and space in the next stage:\n            re-activate the stage for the next cycle\n\n        commit the push to the inputBuffer if that data hasn't all been used\nThe Execute stage differs from this model as its forward output (branch) data\nis unconditionally sent to Fetch1 and Fetch2. To allow this behaviour, Fetch1\nand Fetch2 must be unconditionally receptive to that data.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/cpu_models/minor_cpu",
            "page_title": "No Title Found",
            "parent_section": "The pipeline",
            "section_heading": "Each pipeline stage"
        }
    },
    {
        "text": "Section: The pipeline\n\nSub-section: Fetch1 stage\n\nFetch1\nis\nresponsible for fetching cache lines or partial cache lines from the I-cache\nand passing them on to\nFetch2\nto be decomposed\ninto instructions. It can receive \u2018change of stream\u2019 indications from both\nExecute\nand\nFetch2\nto\nsignal that it should change its internal fetch address and tag newly fetched\nlines with new stream or prediction sequence numbers. When both Execute and\nFetch2\nsignal\nchanges of stream at the same time,\nFetch1\ntakes\nExecute\n\u2019s\nchange.\nEvery line issued by\nFetch1\nwill bear a\nunique line sequence number which can be used for debugging stream changes.\nWhen fetching from the I-cache,\nFetch1\nwill ask for\ndata from the current fetch address (Fetch1::pc) up to the end of the \u2018data\nsnap\u2019 size set in the parameter fetch1LineSnapWidth. Subsequent autonomous line\nfetches will fetch whole lines at a snap boundary and of size fetch1LineWidth.\nFetch1\nwill\nonly initiate a memory fetch if it can reserve space in\nFetch2\ninput buffer.\nThat input buffer serves an the fetch queue/LFL for the system.\nFetch1\ncontains two queues: requests and transfers to handle the stages of translating\nthe address of a line fetch (via the TLB) and accommodating the\nrequest/response of fetches to/from memory.\nFetch requests from\nFetch1\nare pushed into\nthe requests queue as newly allocated FetchRequest objects once they have been\nsent to the ITLB with a call to itb->translateTiming.\nA response from the TLB moves the request from the requests queue to the\ntransfers queue. If there is more than one entry in each queue, it is possible\nto get a TLB response for request which is not at the head of the requests\nqueue. In that case, the TLB response is marked up as a state change to\nTranslated in the request object, and advancing the request to transfers (and\nthe memory system) is left to calls to\nFetch1::stepQueues\nwhich is called in the cycle following any event is received.\nFetch1::tryToSendToTransfers\n\u2014\nlayout: documentation\ntitle: Execution Basics\ndoc: gem5 documentation\nparent: cpu_models\npermalink: /documentation/general_docs/cpu_models/execution_basics\n\u2014\nis responsible for moving requests between the two queues and issuing requests\nto memory. Failed TLB lookups (prefetch aborts) continue to occupy space in the\nqueues until they are recovered at the head of transfers.\nResponses from memory change the request object state to Complete and\nFetch1::evaluate\ncan pick up response data, package it in the\nForwardLineData\nobject,\nand forward it to\nFetch2\n\u2019s input buffer.\nAs space is always reserved in\nFetch2::inputBuffer\n,\nsetting the input buffer\u2019s size to 1 results in non-prefetching behaviour.\nWhen a change of stream occurs, translated requests queue members and completed\ntransfers queue members can be unconditionally discarded to make way for new\ntransfers.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/cpu_models/minor_cpu",
            "page_title": "No Title Found",
            "parent_section": "The pipeline",
            "section_heading": "Fetch1 stage"
        }
    },
    {
        "text": "Section: The pipeline\n\nSub-section: Fetch2 stage\n\nFetch2 receives a line from Fetch1 into its input buffer. The data in the head\nline in that buffer is iterated over and separated into individual instructions\nwhich are packed into a vector of instructions which can be passed to\nDecode\n.\nPacking instructions can be aborted early if a fault is found in either the\ninput line as a whole or a decomposed instruction.\nBranch prediction\nFetch2 contains the branch prediction mechanism. This is a wrapper around the branch predictor interface provided by gem5 (cpu/pred/\u2026).\nBranches are predicted for any control instructions found. If prediction is\nattempted for an instruction, the\nMinorDynInst::triedToPredict\nflag is set on that instruction.\nWhen a branch is predicted to take, the\nMinorDynInst::predictedTaken\nflag is set and\nMinorDynInst::predictedTarget\nis set to the predicted target PC value. The predicted branch instruction is then packed into Fetch2\u2019s output vector, the prediction sequence number is incremented, and the branch is communicated to Fetch1.\nAfter signalling a prediction, Fetch2 will discard its input buffer contents\nand will reject any new lines which have the same stream sequence number as\nthat branch but have a different prediction sequence number. This allows\nfollowing sequentially fetched lines to be rejected without ignoring new lines\ngenerated by a change of stream indicated from a \u2018real\u2019 branch from Execute\n(which will have a new stream sequence number).\nThe program counter value provided to Fetch2 by Fetch1 packets is only updated\nwhen there is a change of stream. Fetch2::havePC indicates whether the PC will\nbe picked up from the next processed input line. Fetch2::havePC is necessary to\nallow line-wrapping instructions to be tracked through decode.\nBranches (and instructions predicted to branch) which are processed by Execute\nwill generate BranchData (\npipe_data.hh\n) data explaining the\noutcome of the branch which is sent forwards to Fetch1 and Fetch2. Fetch1 uses\nthis data to change stream (and update its stream sequence number and address\nfor new lines). Fetch2 uses it to update the branch predictor. Minor does not\ncommunicate branch data to the branch predictor for instructions which are\ndiscarded on the way to commit.\nBranchData::BranchReason (\npipe_data.hh\n) encodes the possible\nbranch scenarios:\nBranch enum val.\nIn Execute\nFetch1 reaction\nFetch2 reaction\nNo Branch\n(output bubble data)\n-\n-\nCorrectlyPredictedBranch\nPredicted, taken\n-\nUpdate BP as taken branch\nUnpredictedBranch\nNot predicted, taken and was taken\nNew stream\nUpdate BP as taken branch\nBadlyPredictedBranch\nPredicted, not taken\nNew stream to restore to old Inst. source\nUpdate BP as not taken branch\nBadlyPredictedBranchTarget\nPredicted, taken, but to a different target than predicted one\nNew stream\nUpdate BTB to new target\nSuspendThread\nHint to suspend fetch\nSuspend fetch for this thread (branch to next inst. as wakeup fetch addr\n-\nInterrupt\nInterrupt detected\nNew stream\n-\n\nlayout: documentation\ntitle: Execution Basics\ndoc: gem5 documentation\nparent: cpu_models\npermalink: /documentation/general_docs/cpu_models/execution_basics\n\u2014",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/cpu_models/minor_cpu",
            "page_title": "No Title Found",
            "parent_section": "The pipeline",
            "section_heading": "Fetch2 stage"
        }
    },
    {
        "text": "Section: The pipeline\n\nSub-section: Decode Stage\n\nDecode\ntakes a\nvector of instructions from\nFetch2\n(via its input\nbuffer) and decomposes those instructions into micro-ops (if necessary) and\npacks them into its output instruction vector.\nThe parameter executeInputWidth sets the number of instructions which can be\npacked into the output per cycle. If the parameter decodeCycleInput is true,\nDecode\ncan try\nto take instructions from more than one entry in its input buffer per cycle.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/cpu_models/minor_cpu",
            "page_title": "No Title Found",
            "parent_section": "The pipeline",
            "section_heading": "Decode Stage"
        }
    },
    {
        "text": "Section: The pipeline\n\nSub-section: Execute Stage\n\nExecute provides all the instruction execution and memory access mechanisms. An\ninstructions passage through Execute can take multiple cycles with its precise\ntiming modelled by a functional unit pipeline FIFO.\nA vector of instructions (possibly including fault \u2018instructions\u2019) is provided\nto Execute by Decode and can be queued in the Execute input buffer before being\nissued. Setting the parameter executeCycleInput allows execute to examine more\nthan one input buffer entry (more than one instruction vector). The number of\ninstructions in the input vector can be set with executeInputWidth and the\ndepth of the input buffer can be set with parameter executeInputBufferSize.\nFunctional units\nThe Execute stage contains pipelines for each functional unit comprising the\ncomputational core of the CPU. Functional units are configured via the\nexecuteFuncUnits parameter. Each functional unit has a number of instruction\nclasses it supports, a stated delay between instruction issues, and a delay\nfrom instruction issue to (possible) commit and an optional timing annotation\ncapable of more complicated timing.\nEach active cycle,\nExecute::evaluate\nperforms this action:\nExecute::evaluate:\n        push input to inputBuffer\n        setup references to input/output data slots and branch output slot\n\n        step D-cache interface queues (similar to Fetch1)\n\n        if interrupt posted:\n            take interrupt (signalling branch to Fetch1/Fetch2)\n        else\n            commit instructions\n            issue new instructions\n\n        advance functional unit pipelines\n\n        reactivate Execute if the unit is still active\n\n        commit the push to the inputBuffer if that data hasn't all been used\nFunctional unit FIFOs\nFunctional units are implemented as SelfStallingPipelines (stage.hh). These are\nTimeBuffer\nFIFOs\nwith two distinct \u2018push\u2019 and \u2018pop\u2019 wires. They respond to\nSelfStallingPipeline::advance\nin the same way as TimeBuffers unless there is data at the far, \u2018pop\u2019, end of\nthe FIFO. A \u2018stalled\u2019 flag is provided for signalling stalling and to allow a\nstall to be cleared. The intention is to provide a pipeline for each functional\nunit which will never advance an instruction out of that pipeline until it has\nbeen processed and the pipeline is explicitly unstalled.\nThe actions \u2018issue\u2019, \u2018commit\u2019, and \u2018advance\u2019 act on the functional units.\nIssue\nIssuing instructions involves iterating over both the input buffer instructions\nand the heads of the functional units to try and issue instructions in order.\nThe number of instructions which can be issued each cycle is limited by the\nparameter executeIssueLimit, how executeCycleInput is set, the availability of\n\u2014\nlayout: documentation\ntitle: Execution Basics\ndoc: gem5 documentation\nparent: cpu_models\npermalink: /documentation/general_docs/cpu_models/execution_basics\n\u2014\npipeline space and the policy used to choose a pipeline in which the\ninstruction can be issued.\nAt present, the only issue policy is strict round-robin visiting of each\npipeline with the given instructions in sequence. For greater flexibility,\nbetter (and more specific policies) will need to be possible.\nMemory operation instructions traverse their functional units to perform their\nEA calculations. On \u2018commit\u2019, the\nExecContext\n::initiateAcc\nexecution phase is performed and any memory access is issued (via.\nExecContext::{read,write}Mem calling\nLSQ::pushRequest\n)\nto the\nLSQ\n.\nNote that faults are issued as if they are instructions and can (currently) be\nissued to any functional unit.\nEvery issued instruction is also pushed into the Execute::inFlightInsts queue.\nMemory ref. instructions are pushing into Execute::inFUMemInsts queue.\nCommit\nInstructions are committed by examining the head of the Execute::inFlightInsts\nqueue (which is decorated with the functional unit number to which the\ninstruction was issued). Instructions which can then be found in their\nfunctional units are executed and popped from Execute::inFlightInsts.\nMemory operation instructions are committed into the memory queues (as\ndescribed above) and exit their functional unit pipeline but are not popped\nfrom the Execute::inFlightInsts queue. The Execute::inFUMemInsts queue provides\nordering to memory operations as they pass through the functional units\n(maintaining issue order). On entering the LSQ, instructions are popped from\nExecute::inFUMemInsts.\nIf the parameter executeAllowEarlyMemoryIssue is set, memory operations can be\nsent from their FU to the LSQ before reaching the head of\nExecute::inFlightInsts but after their dependencies are met.\nMinorDynInst::instToWaitFor\nis marked up with the latest dependent instruction execSeqNum required to be\ncommitted for a memory operation to progress to the LSQ.\nOnce a memory response is available (by testing the head of\nExecute::inFlightInsts against\nLSQ::findResponse\n),\ncommit will process that response (ExecContext::completeAcc) and pop the\ninstruction from Execute::inFlightInsts.\nAny branch, fault or interrupt will cause a stream sequence number change and\nsignal a branch to Fetch1/Fetch2. Only instructions with the current stream\nsequence number will be issued and/or committed.\nAdvance\nAll non-stalled pipeline are advanced and may, thereafter, become stalled.\nPotential activity in the next cycle is signalled if there are any instructions\nremaining in any pipeline.\nScoreboard\nThe scoreboard (\nScoreboard\n) is used to\ncontrol instruction issue. It contains a count of the number of in flight\ninstructions which will write each general purpose CPU integer or float\nregister. Instructions will only be issued where the scoreboard contains a\ncount of 0 instructions which will write to one of the instructions source\nregisters.\nOnce an instruction is issued, the scoreboard counts for each destination\nregister for an instruction will be incremented.\nThe estimated delivery time of the instruction\u2019s result is marked up in the scoreboard by adding the length of the issued-to FU to the current time. The timings parameter on each FU provides a list of additional rules for calculating the delivery time. These are documented in the parameter comments in MinorCPU.py.\nOn commit, (for memory operations, memory response commit) the scoreboard counters for an instruction\u2019s source registers are decremented. will be decremented.\nExecute::inFlightInsts\nThe Execute::inFlightInsts queue will always contain all instructions in flight\nin\nExecute\nin\nthe correct issue order.\nExecute::issue\nis the only process which will push an instruction into the queue.\nExecute::commit\nis the only process that can pop an instruction.\nLSQ\nThe\nLSQ\ncan\nsupport multiple outstanding transactions to memory in a number of conservative\ncases.\nThere are three queues to contain requests: requests, transfers and the store\nbuffer. The requests and transfers queue operate in a similar manner to the\nqueues in Fetch1. The store buffer is used to decouple the delay of completing\nstore operations from following loads.\nRequests are issued to the DTLB as their instructions leave their functional\nunit. At the head of requests, cacheable load requests can be sent to memory\nand on to the transfers queue. Cacheable stores will be passed to transfers\nunprocessed and progress that queue maintaining order with other transactions.\nThe conditions in\nLSQ::tryToSendToTransfers\ndictate when requests can be sent to memory.\nAll uncacheable transactions, split transactions and locked transactions are\nprocessed in order at the head of requests. Additionally, store results\nresiding in the store buffer can have their data forwarded to cacheable loads\n(removing the need to perform a read from memory) but no cacheable load can be\nissue to the transfers queue until that queue\u2019s stores have drained into the\nstore buffer.\nAt the end of transfers, requests which are\nLSQ::LSQRequest::Complete\n(are faulting, are cacheable stores, or have been sent to memory and received a\nresponse) can be picked off by Execute and either committed\n(ExecContext::completeAcc) and, for stores, be sent to the store buffer.\nBarrier instructions do not prevent cacheable loads from progressing to memory\nbut do cause a stream change which will discard that load. Stores will not be\ncommitted to the store buffer if they are in the shadow of the barrier but\nbefore the new instruction stream has arrived at Execute. As all other memory\ntransactions are delayed at the end of the requests queue until they are at the\nhead of Execute::inFlightInsts, they will be discarded by any barrier stream\nchange.\nAfter commit,\nLSQ::BarrierDataRequest\nrequests are inserted into the store buffer to track each barrier until all\npreceding memory transactions have drained from the store buffer. No further\nmemory transactions will be issued from the ends of FUs until after the barrier\nhas drained.\nDraining\nDraining is mostly handled by the\nExecute\nstage. When\ninitiated by calling\nMinorCPU::drain\n,\nPipeline::evaluate\nchecks the draining status of each unit each cycle and keeps the pipeline\nactive until draining is complete. It is Pipeline that signals the completion\nof draining. Execute is triggered by\nMinorCPU::drain\nand starts stepping through its\nExecute::DrainState\nstate machine, starting from state Execute::NotDraining, in this order:\nState\nMeaning\nExecute::NotDraining\nNot trying to drain, normal execution\nExecute::DrainCurrentInst\nDraining micro-ops to complete inst.\nExecute::DrainHaltFetch\nHalt fetching instructions\nExecute::DrainAllInsts\nDiscarding all instructions presented\nWhen complete, a drained Execute unit will be in the\nExecute::DrainAllInsts\nstate where it will continue to discard instructions but has no knowledge of\nthe drained state of the rest of the model.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/cpu_models/minor_cpu",
            "page_title": "No Title Found",
            "parent_section": "The pipeline",
            "section_heading": "Execute Stage"
        }
    },
    {
        "text": "Debug options\nThe model provides a number of debug flags which can be passed to gem5 with the\n\u2013debug-flags\noption.\nThe available flags are:\nDebug flag\nUnit which will generate debugging output\nActivity\nDebug\nActivityMonitor actions\nBranch\nFetch2\nand\nExecute\nbranch prediction decisions\nMinorCPU\nCPU global actions such as wakeup/thread suspension\nDecode\nDecode\nMinorExec\nExecute\nbehaviour\nFetch\nFetch1\nand\nFetch2\nMinorInterrupt\nExecute\ninterrupt handling\nMinorMem\nExecute\nmemory interactions\nMinorScoreboard\nExecute\nscoreboard activity\nMinorTrace\nGenerate MinorTrace cyclic state trace output (see below)\nMinorTiming\nMinorTiming instruction timing modification operations\nThe group flag\nMinor\nenables all the flags beginning with\nMinor\n.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/cpu_models/minor_cpu",
            "page_title": "No Title Found",
            "parent_section": "Debug options",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Section: MinorTrace and minorview.py\n\nSub-section: MinorTrace format\n\nThere are three types of line outputted by MinorTrace:\nMinorTrace - Ticked unit cycle state\nFor example:\n110000: system.cpu.dcachePort: MinorTrace: state=MemoryRunning in_tlb_mem=0/0\nFor each time step, the MinorTrace flag will cause one MinorTrace line to be\nprinted for every named element in the model.\nMinorInst - summaries of instructions issued by Decode\nDecode\nFor example:\n140000: system.cpu.execute: MinorInst: id=0/1.1/1/1.1 addr=0x5c \\\n                             inst=\"  mov r0, #0\" class=IntAlu\nMinorInst lines are currently only generated for instructions which are committed.\nMinorLine - summaries of line fetches issued by Fetch1\nFetch1\nFor example:\n92000: system.cpu.icachePort: MinorLine: id=0/1.1/1 size=36 \\\n                                vaddr=0x5c paddr=0x5c",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/cpu_models/minor_cpu",
            "page_title": "No Title Found",
            "parent_section": "MinorTrace and minorview.py",
            "section_heading": "MinorTrace format"
        }
    },
    {
        "text": "Section: MinorTrace and minorview.py\n\nSub-section: minorview.py\n\nMinorview (util/minorview.py) can be used to visualise the data created by\nMinorTrace.\nusage: minorview.py [-h] [--picture picture-file] [--prefix name]\n                   [--start-time time] [--end-time time] [--mini-views]\n                   event-file\n\nMinor visualiser\n\npositional arguments:\n  event-file\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --picture picture-file\n                        markup file containing blob information (default:\n                        <minorview-path>/minor.pic)\n  --prefix name         name prefix in trace for CPU to be visualised\n                        (default: system.cpu)\n  --start-time time     time of first event to load from file\n  --end-time time       time of last event to load from file\n  --mini-views          show tiny views of the next 10 time steps\nRaw debugging output can be passed to minorview.py as the event-file. It will\npick out the MinorTrace lines and use other lines where units in the simulation\nare named (such as system.cpu.dcachePort in the above example) will appear as\n\u2018comments\u2019 when units are clicked on the visualiser.\nClicking on a unit which contains instructions or lines will bring up a speech\nbubble giving extra information derived from the MinorInst/MinorLine lines.\n\u2013start-time\nand\n\u2013end-time\nallow only sections of debug files to be loaded.\n\u2013prefix\nallows the name prefix of the CPU to be inspected to be supplied.\nThis defaults to\nsystem.cpu\n.\nIn the visualiser, The buttons Start, End, Back, Forward, Play and Stop can be\nused to control the displayed simulation time.\nThe diagonally striped coloured blocks are showing the\nInstId\nof the\ninstruction or line they represent. Note that lines in\nFetch1\nand f1ToF2.F\nonly show the id fields of a line and that instructions in\nFetch2\n, f2ToD, and\ndecode.inputBuffer do not yet have execute sequence numbers. The T/S.P/L/F.E\nbuttons can be used to toggle parts of\nInstId\non and off to\nmake it easier to understand the display. Useful combinations are:\nCombination\nReason\nE\njust show the final execute sequence number\nF/E\nshow the instruction-related numbers\nS/P\nshow just the stream-related numbers (watch the stream sequence change with branches and not change with predicted branches)\nS/E\nshow instructions and their stream\nThe key to the right shows all the displayable colours (some of the colour\nchoices are quite bad!):\nSymbol\nMeaning\nU\nUknown data\nB\nBlocked stage\n-\nBubble\nE\nEmpty queue slot\nR\nReserved queue slot\nF\nFault\nr\nRead (used as the leftmost stripe on data in the dcachePort)\nw\nWrite \u201c \u201c\n0 to 9\nlast decimal digit of the corresponding data\n,---------------.         .--------------.  *U\n    | |=|->|=|->|=| |         ||=|||->||->|| |  *-  <- Fetch queues/LSQ\n    `---------------'         `--------------'  *R\n    === ======                                  *w  <- Activity/Stage activity\n                              ,--------------.  *1\n    ,--.      ,.      ,.      | ============ |  *3  <- Scoreboard\n    |  |-\\[]-\\||-\\[]-\\||-\\[]-\\| ============ |  *5  <- Execute::inFlightInsts\n    |  | :[] :||-/[]-/||-/[]-/| -. --------  |  *7\n    |  |-/[]-/||  ^   ||      |  | --------- |  *9\n    |  |      ||  |   ||      |  | ------    |\n[]->|  |    ->||  |   ||      |  | ----      |\n    |  |<-[]<-||<-+-<-||<-[]<-|  | ------    |->[] <- Execute to Fetch1,\n    '--`      `'  ^   `'      | -' ------    |        Fetch2 branch data\n             ---. |  ---.     `--------------'\n             ---' |  ---'       ^       ^\n                  |   ^         |       `------------ Execute\n  MinorBuffer ----' input       `-------------------- Execute input buffer\n                    buffer\nStages show the colours of the instructions currently being\ngenerated/processed.\nForward FIFOs between stages show the data being pushed into them at the\ncurrent tick (to the left), the data in transit, and the data available at\ntheir outputs (to the right).\nThe backwards FIFO between\nFetch2\nand\nFetch1\nshows branch\nprediction data.\nIn general, all displayed data is correct at the end of a cycle\u2019s activity at\nthe time indicated but before the inter-stage FIFOs are ticked. Each FIFO has,\ntherefore an extra slot to show the asserted new input data, and all the data\ncurrently within the FIFO.\nInput buffers for each stage are shown below the corresponding stage and show\nthe contents of those buffers as horizontal strips. Strips marked as reserved\n(cyan by default) are reserved to be filled by the previous stage. An input\nbuffer with all reserved or occupied slots will, therefore, block the previous\nstage from generating output.\nFetch queues and\nLSQ\nshow the\nlines/instructions in the queues of each interface and show the number of\nlines/instructions in TLB and memory in the two striped colours of the top of\ntheir frames.\nInside\nExecute\n, the horizontal\nbars represent the individual FU pipelines. The vertical bar to the left is the\ninput buffer and the bar to the right, the instructions committed this cycle.\nThe background of\nExecute\nshows\ninstructions which are being committed this cycle in their original FU pipeline\npositions.\nThe strip at the top of the\nExecute\nblock shows the\ncurrent streamSeqNum that\nExecute\nis committing.\nA similar stripe at the top of\nFetch1\nshows that\nstage\u2019s expected streamSeqNum and the stripe at the top of\nFetch2\nshows its\nissuing predictionSeqNum.\nThe scoreboard shows the number of instructions in flight which will commit a\nresult to the register in the position shown. The scoreboard contains slots for\neach integer and floating point register.\nThe Execute::inFlightInsts queue shows all the instructions in flight in\nExecute\nwith\nthe oldest instruction (the next instruction to be committed) to the right.\nStage activity\nshows the signalled activity (as E/1) for each stage (with CPU\nmiscellaneous activity to the left)\nActivity\nshow a count of stage and pipe activity.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/cpu_models/minor_cpu",
            "page_title": "No Title Found",
            "parent_section": "MinorTrace and minorview.py",
            "section_heading": "minorview.py"
        }
    },
    {
        "text": "Section: MinorTrace and minorview.py\n\nSub-section: minor.pic format\n\nThe minor.pic file (src/minor/minor.pic) describes the layout of the models\nblocks on the visualiser. Its format is described in the supplied minor.pic\nfile.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/cpu_models/minor_cpu",
            "page_title": "No Title Found",
            "parent_section": "MinorTrace and minorview.py",
            "section_heading": "minor.pic format"
        }
    },
    {
        "text": "O3 Pipeline Viewer\nThe o3 pipeline viewer is a text based viewer of the out-of-order CPU pipeline. It shows when instructions are fetched (f), decoded (d), renamed (n), dispatched (p), issued (i), completed (c), and retired (r). It is very useful for understanding where the pipeline is stalling or squashing in a reasonable small sequence of code. Next to the colorized viewer that wraps around is the tick the current instruction retired, the pc of that instruction, it\u2019s disassembly, and the o3 sequence number for that instruction.\n\nTo generate output line you see above you first need to run an experiment with the o3 cpu:\n./build/ARM/gem5.opt --debug-flags=O3PipeView --debug-start=<first tick of interest> --debug-file=trace.out configs/example/se.py --cpu-type=detailed --caches -c <path to binary> -m <last cycle of interest>\nThen you can run the script to generate a trace similar to the above (500 is the number of ticks per clock (2GHz) in this case):\n./util/o3-pipeview.py -c 500 -o pipeview.out --color m5out/trace.out\nYou can view the output in color by piping the file through less:\nless -r pipeview.out\nWhen CYCLE_TIME (-c) is wrong, Right square brackets in output may not aligned to the same column. Default value of CYCLE_TIME is 1000. Be careful.\nThe script has some additional integrated help: (type \u2018./util/o3-pipeview.py \u2013help\u2019 for help).",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/cpu_models/visualization",
            "page_title": "Visualization",
            "parent_section": "O3 Pipeline Viewer",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Minor Viewer\nThe\nnew page\non minor viewer is yet to be made, refer to\nold page\nfor documentation.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/cpu_models/visualization",
            "page_title": "Visualization",
            "parent_section": "Minor Viewer",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Indentation and Line Breaks\nIndentation will be 4 spaces per level, though namespaces should not increase the indentation.\nException: labels followed by colons (case and goto labels and public/private/protected modifiers) are indented two spaces from the enclosing context.\nIndentation should use spaces only (no tabs), as tab widths are not always set consistently, and tabs make output harder to read when used with tools such as diff.\nLines must be a maximum of 79 characters long.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/development/coding_style/",
            "page_title": "C/C++ Coding Style",
            "parent_section": "Indentation and Line Breaks",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Braces\nFor control blocks (if, while, etc.), opening braces must be on the same line as the control keyword with a space between the closing parenthesis and the opening brace.\nException: for multi-line expressions, the opening brace may be placed on a separate line to distinguish the control block from the statements inside the block.\nif\n(...)\n{\n...\n}\n// exception case\nfor\n(...;\n...;\n...)\n// brace could be up here\n{\n// but this is optionally OK *only* when the 'for' spans multiple lines\n...\n}\n\u2018Else\u2019 keywords should follow the closing \u2018if\u2019 brace on the same line, as follows:\nif\n(...)\n{\n...\n}\nelse\nif\n(...)\n{\n...\n}\nelse\n{\n...\n}\nBlocks that consist of a single statement that fits on a single line may optionally omit the braces. Braces are still required if the single statement spans multiple lines, or if the block is part of an else/if chain where other blocks have braces.\n// This is OK with or without braces\nif\n(\na\n>\n0\n)\n--\na\n;\n// In the following cases, braces are still required\nif\n(\na\n>\n0\n)\n{\nobnoxiously_named_function_with_lots_of_args\n(\nverbose_arg1\n,\nverbose_arg2\n,\nverbose_arg3\n);\n}\nif\n(\na\n>\n0\n)\n{\n--\na\n;\n}\nelse\n{\nunderflow\n=\ntrue\n;\nwarn\n(\n\"underflow on a\"\n);\n}\nFor function definitions or class declarations, the opening brace must be in the first column of the following line.\nIn function definitions, the return type should be on one line, followed by the function name, left-justified, on the next line. As mentioned above, the opening brace should also be on a separate line following the function name.\nSee examples below:\nint\nexampleFunc\n(...)\n{\n...\n}\nclass\nExampleClass\n{\npublic:\n...\n};\nFunctions should be preceded by a block comment describing the function.\nInline function declarations longer than one line should not be placed inside class declarations. Most functions longer than one line should not be inline anyway.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/development/coding_style/",
            "page_title": "C/C++ Coding Style",
            "parent_section": "Braces",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Spacing\nThere should be:\none space between keywords (if, for, while, etc.) and opening parentheses\none space around binary operators (+, -, <, >, etc.) including assignment operators (=, +=, etc.)\nno space around \u2018=\u2019 when used in parameter/argument lists, either to bind default parameter values (in Python or C++) or to bind keyword arguments (in Python)\nno space between function names and opening parentheses for arguments\nno space immediately inside parentheses, except for very complex expressions. Complex expressions are preferentially broken into multiple simpler expressions using temporary variables.\nFor pointer and reference argument declarations, either of the following are acceptable:\nFooType\n*\nfooPtr\n;\nFooType\n&\nfooRef\n;\nor\nFooType\n*\nfooPtr\n;\nFooType\n&\nfooRef\n;\nHowever, style should be kept consistent within a file. If you are editing an existing file, please keep consistent with the existing code. If you are writing new code in a new file, feel free to choose the style of your preference.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/development/coding_style/",
            "page_title": "C/C++ Coding Style",
            "parent_section": "Spacing",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Naming\nClass and type names are mixed case, start with an uppercase letter, and do not contain underscores (e.g., ClassName). Exception: names that are acronyms should be all upper case (e.g., CPU). Class member names (method and variables, including const variables) are mixed case, start with a lowercase letter, and do not contain underscores (e.g., aMemberVariable). Class members that have accessor methods should have a leading underscore to indicate that the user should be using an accessor. The accessor functions themselves should have the same name as the variable without the leading underscore.\nLocal variables are lower case, with underscores separating words (e.g., local_variable). Function parameters should use underscores and be lower case.\nC preprocessor symbols (constants and macros) should be all caps with underscores. However, these are deprecated, and should be replaced with const variables and inline functions, respectively, wherever possible.\nclass\nFooBarCPU\n{\nprivate:\nstatic\nconst\nint\nminLegalFoo\n=\n100\n;\n// consts are formatted just like other vars\nint\n_fooVariable\n;\n// starts with '_' because it has public accessor functions\nint\nbarVariable\n;\n// no '_' since it's internal use only\npublic:\n// short inline methods can go all on one line\nint\nfooVariable\n()\nconst\n{\nreturn\n_fooVariable\n;\n}\n// longer inline methods should be formatted like regular functions,\n// but indented\nvoid\nfooVariable\n(\nint\nnew_value\n)\n{\nassert\n(\nnew_value\n>=\nminLegalFoo\n);\n_fooVariable\n=\nnew_value\n;\n}\n};",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/development/coding_style/",
            "page_title": "C/C++ Coding Style",
            "parent_section": "Naming",
            "section_heading": "Overview"
        }
    },
    {
        "text": "#includes\nWhenever possible favor C++ includes over C include. E.g. choose cstdio, not stdio.h.\nThe block of #includes at the top of the file should be organized. We keep several sorted groups. This makes it easy to find #include and to avoid duplicate #includes.\nAlways include Python.h first if you need that header. This is mandated by the integration guide. The next header file should be your main header file (e.g., for foo.cc you\u2019d include foo.hh first). Having this header first ensures that it is independent and can be included in other places without missing dependencies.\n// Include Python.h first if you need it.\n#include\n<Python.h>\n// Include your main header file before any other non-Python headers (i.e., the one with the same name as your cc source file)\n#include\n\"main_header.hh\"\n// C includes in sorted order\n#include\n<fcntl.h>\n#include\n<sys/time.h>\n// C++ includes\n#include\n<cerrno>\n#include\n<cstdio>\n#include\n<string>\n#include\n<vector>\n// Shared headers living in include/. These are used both in the simulator and utilities such as the m5 tool.\n#include\n<gem5/asm/generic/m5ops.h>\n// M5 includes\n#include\n\"base/misc.hh\"\n#include\n\"cpu/base.hh\"\n#include\n\"params/BaseCPU.hh\"\n#include\n\"sim/system.hh\"",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/development/coding_style/",
            "page_title": "C/C++ Coding Style",
            "parent_section": "#includes",
            "section_heading": "Overview"
        }
    },
    {
        "text": "File structure and modularity\nSource files (.cc files) should never contain extern declarations; instead, include the header file associated with the .cc file in which the object is defined. This header file should contain extern declarations for all objects exported from that .cc file. This header should also be included in the defining .cc file. The key here is that we have a single external declaration in the .hh file that the compiler will automatically check for consistency with the .cc file. (This isn\u2019t as important in C++ as it was in C, since linker name mangling will now catch these errors, but it\u2019s still a good idea.)\nWhen sufficient (i.e., when declaring only pointers or references to a class), header files should use forward class declarations instead of including full header files.\nHeader files should never contain using namespace declarations at the top level. This forces all the names in that namespace into the global namespace of any source file including that header file, which basically completely defeats the point of using namespaces. It is OK to use using namespace declarations at the top level of a source (.cc) file since the effect is entirely local to that .cc file. It\u2019s also OK to use them in _impl.hh files, since for practical purposes these are source (not header) files despite their extension.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/development/coding_style/",
            "page_title": "C/C++ Coding Style",
            "parent_section": "File structure and modularity",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Section: Documenting the code\n\nSub-section: Using Doxygen\n\nThe special documentation blocks take the form of a javadoc style comment. A javadoc comment is a C style comment with 2 *\u2019s at the start, like this:\n/**\n * ...documentation...\n */\nThe intermediate asterisks are optional, but please use them to clearly delineate the documentation comments.\nThe documentation within these blocks is made up of at least a brief description of the documented structure, that can be followed by a more detailed description and other documentation. The brief description is the first sentence of the comment. It ends with a period followed by white space or a new line. For example:\n/**\n * This is the brief description. This is the start of the detailed\n * description. Detailed Description continued.\n */\nIf you need to have a period in the brief description, follow it with a backslash followed by a space.\n/**\n * e.g.\\ This is a brief description with an internal period.\n */\nBlank lines within these comments are interpreted as paragraph breaks to help you make the documentation more readble.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/development/coding_style/",
            "page_title": "C/C++ Coding Style",
            "parent_section": "Documenting the code",
            "section_heading": "Using Doxygen"
        }
    },
    {
        "text": "Section: Documenting the code\n\nSub-section: Special commands\n\nPlacing these comments before the declaration works in most cases. For files however, you need to specify that you are documenting the file. To do this you use the @file special command. To document the file that you are currently in you just need to use the command followed by your comments. To comment a separate file (we shouldn\u2019t have to do this) you can supply the name directly after the file command. There are some other special commands we will be using quite often. To document functions we will use @param and @return or @retval to document the parameters and the return value. @param takes the name of the paramter and its description. @return just describes the return value, while @retval adds a name to it. To specify pre and post conditions you can use @pre and @post.\nSome other useful commands are @todo and @sa. @todo allows you to place reminders of things to fix/implement and associate them with a specific class or member/function. @sa lets you place references to another piece of documentation (class, member, etc.). This can be useful to provide links to code that would be helpful in understanding the code being documented.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/development/coding_style/",
            "page_title": "C/C++ Coding Style",
            "parent_section": "Documenting the code",
            "section_heading": "Special commands"
        }
    },
    {
        "text": "Section: Documenting the code\n\nSub-section: Example of Simple Documentation\n\nHere is a simple header file with doxygen comments added.\n/**\n * @file\n * Contains an example of documentation style.\n */\n#include\n<vector>\n/**\n * Adds two numbers together.\n */\n#define DUMMY(a,b) (a+b)\n/**\n * A simple class description. This class does really great things in detail.\n *\n * @todo Update to new statistics model.\n */\nclass\nfoo\n{\n/** This variable stores blah, which does foo and has invariants x,y,z\n         @warning never set this to 0\n         @invariant foo\n    */\nint\nmyVar\n;\n/**\n  * This function does something.\n  * @param a The number of times to do it.\n  * @param b The thing to do it to.\n  * @return The number of times it was done.\n  *\n  * @sa DUMMY\n  */\nint\nbar\n(\nint\na\n,\nlong\nb\n);\n/**\n  * A function that does bar.\n  * @retval true if there is a problem, false otherwise.\n  */\nbool\nmanchu\n();\n};",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/development/coding_style/",
            "page_title": "C/C++ Coding Style",
            "parent_section": "Documenting the code",
            "section_heading": "Example of Simple Documentation"
        }
    },
    {
        "text": "Section: Documenting the code\n\nSub-section: Grouping\n\nDoxygen also allows for groups of classes and member (or other groups) to be declared. We can use these to create a listing of all statistics/global variables. Or just to comment about the memory hierarchy as a whole. You define a group using @defgroup and then add to it using @ingroup or @addgroup. For example:\n/**\n * @defgroup statistics Statistics group\n */\n/**\n  * @defgroup substat1 Statistitics subgroup\n  * @ingroup statistics\n  */\n/**\n *  A simple class.\n */\nclass\nfoo\n{\n/**\n   * Collects data about blah.\n   * @ingroup statistics\n   */\nStat\nstat1\n;\n/**\n   * Collects data about the rate of blah.\n   * @ingroup statistics\n   */\nStat\nstat2\n;\n/**\n   * Collects data about flotsam.\n   * @ingroup statistics\n   */\nStat\nstat3\n;\n/**\n   * Collects data about jetsam.\n   * @ingroup substat1\n   */\nStat\nstat4\n;\n};\nThis places stat1-3 in the statistics group and stat4 in the subgroup. There is a shorthand method to place objects in groups. You can use @{ and @} to mark the start and end of group inclusion. The example above can be rewritten as:\n/**\n * @defgroup statistics Statistics group\n */\n/**\n  * @defgroup substat1 Statistitics subgroup\n  * @ingroup statistics\n  */\n/**\n *  A simple class.\n */\nclass\nfoo\n{\n/**\n   * @ingroup statistics\n   * @{\n   */\n/** Collects data about blah.*/\nStat\nstat1\n;\n/** Collects data about the rate of blah. */\nStat\nstat2\n;\n/** Collects data about flotsam.*/\nStat\nstat3\n;\n/** @} */\n/**\n   * Collects data about jetsam.\n   * @ingroup substat1\n   */\nStat\nstat4\n;\n};\nIt remains to be seen what groups we can come up with.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/development/coding_style/",
            "page_title": "C/C++ Coding Style",
            "parent_section": "Documenting the code",
            "section_heading": "Grouping"
        }
    },
    {
        "text": "Section: Documenting the code\n\nSub-section: Other features\n\nNot sure what other doxygen features we want to use.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/development/coding_style/",
            "page_title": "C/C++ Coding Style",
            "parent_section": "Documenting the code",
            "section_heading": "Other features"
        }
    },
    {
        "text": "Section: M5 Status Messages\n\nSub-section: Fatal v. Panic\n\nThere are two error functions defined in\nsrc/base/logging.hh:\npanic()\nand\nfatal()\n. While these two functions have roughly similar effects (printing an error message and terminating the simulation process), they have distinct purposes and use cases. The distinction is documented in the comments in the header file, but is repeated here for convenience because people often get confused and use the wrong one.\npanic()\nshould be called when something happens that should never ever happen regardless of what the user does (i.e., an actual m5 bug).\npanic()\ncalls\nabort()\nwhich can dump core or enter the debugger.\nfatal()\nshould be called when the simulation cannot continue due to some condition that is the user\u2019s fault (bad configuration, invalid arguments, etc.) and not a simulator bug.\nfatal()\ncalls\nexit(1)\n, i.e., a \u201cnormal\u201d exit with an error code.\nThe reasoning behind these definitions is that there\u2019s no need to panic if it\u2019s just a silly user error; we only panic if m5 itself is broken. On the other hand, it\u2019s not hard for users to make errors that are fatal, that is, errors that are serious enough that the m5 process cannot continue.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/development/coding_style/",
            "page_title": "C/C++ Coding Style",
            "parent_section": "M5 Status Messages",
            "section_heading": "Fatal v. Panic"
        }
    },
    {
        "text": "Section: M5 Status Messages\n\nSub-section: Inform, Warn and Hack\n\nThe file\nsrc/base/logging.hh\nalso houses 3 functions that alert the user to various conditions happening within the simulation:\ninform()\n,\nwarn()\nand\nhack()\n. The purpose of these functions is strictly to provide simulation status to the user so none of these functions will stop the simulator from running.\ninform()\nand\ninform_once()\nshould be called for informative messages that users should know, but not worry about.\ninform_once()\nwill only display the status message generated by the\ninform_once()\nfunction the first time it is called.\nwarn()\nand\nwarn_once()\nshould be called when some functionality isn\u2019t necessarily implemented correctly, but it might work well enough. The idea behind a\nwarn()\nis to inform the user that if they see some strange behavior shortly after a\nwarn()\nthe description might be a good place to go looking for an error.\nhack()\nshould be called when some functionality isn\u2019t implemented nearly as well as it could or should be but for expediency or history sake hasn\u2019t been fixed.\ninform()\nProvides status messages and normal operating messages to the console for the user to see, without any connotations of incorrect behavior.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/development/coding_style/",
            "page_title": "C/C++ Coding Style",
            "parent_section": "M5 Status Messages",
            "section_heading": "Inform, Warn and Hack"
        }
    },
    {
        "text": "gem5 repository\nThe\ngem5 git repository\nhas two branches,\nstable\nand\ndevelop\n.\nThe HEAD of the stable branch is the latest official release of gem5 and will be tagged as such.\nUsers are not permitted to submit patches to the stable branch, and instead submit patches to the develop branch.\nAt least two weeks prior to a release a staging branch is created from the develop branch.\nThis staging branch is rigorously tested and only bug fixes or inconsequential changes (format fixes, typo fixes, etc.) are permitted to be be submitted to this branch.\nThe staging branch is updated with the following changes:\nThe\n-werror\nis removed.\nThis ensures that gem5 compiles on newer compilers as new/stricter compiler warnings are incorporated.\nFor example:\nhttps://gem5-review.googlesource.com/c/public/gem5/+/43425\n.\nThe\nDoxygen \u201cProject Number\u201d field\nis updated to the version ID.\nFor example:\nhttps://gem5-review.googlesource.com/c/public/gem5/+/47079\n.\nThe\nsrc/base/version.cc\nfile is updated to state the version ID.\nFor example:\nhttps://gem5-review.googlesource.com/c/public/gem5/+/47079\n.\nThe\next/testlib/configuration.py\nfile\u2019s\ndefault.resource_url\nfield is updated to point towards the correct Google Cloud release bucket (see\nthe Cloud Bucket release procedures\n).\nFor example:\nhttps://gem5-review.googlesource.com/c/public/gem5/+/44725\n.\nThe Resource downloader,\nsrc/python/gem5/resources/downloader.py\n, has a function\ndef _resources_json_version_required()\n. This must be updated to the correct version of the\nresources.json\nfile to use (see the\ngem5 resources repository release procedures\n) for more information on this).\nThe\ntests/weekly.sh\n,\ntests/nightly.sh\n,\ntests/compiler-tests.sh\n, and\ntests/jenkins/presubmit.sh\nshould be updated ensure they remain stable across different gem5 releases. This is achieved by:\nFix the docker pulls images by appending the version (example\nhere\n. This will be done after following the\ndocker image release procedures\n.\nEnsure the download links are downloading from the correct Google Cloud bucket for the release version.\nHardcode the\nrocm_patches/ROCclr.patch\ndownload link in\nutil/dockerfiles/gcn-gpu\nto the correct Google bucket.\nUpdate the\next/sst/README.md\nfile for the current version. This simply means updating the download links.\nSee\nhere\nfor an example of how this is done.\nWhen the staging branch is confirmed to be in a satisfactory state, it will be merged into both develop and stable.\nThere is then two additional actions:\nThe above changes to the staging branch are reverted on the develop branch.\nThe stable branch is tagged with the latest release version id at its HEAD.\nFor example,\ngit tag -a v21.1.0.0 -m \"gem5 version 21.1.0.0\" && git push --tags\nThe\nRELEASE-NOTES.md\nshould be updated to notify the community of the major changes in this release.\nThis can be done on the develop branch prior to the creation of the staging branch, or on the staging branch.\nIt has been customary to create a blog post on\nhttp://www.gem5.org\noutlining the release.\nWhile appreciated, it is not mandatory.\nImportant notes:\nYou must a member of the \u201cProject Owners\u201d or \u201cgoogle/gem5-admins@googlegroups.com\u201d Gerrit permission groups to push to the stable branch.\nPlease contact Bobby R. Bruce (bbruce@ucdavis.edu) for help pushing to the gem5 stable branch.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/development/release_procedures/",
            "page_title": "No Title Found",
            "parent_section": "gem5 repository",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Section: gem5 resources repository\n\nSub-section: gem5 resources Google Cloud Bucket\n\nThe built gem5 resources are found within the gem5 Google Cloud Bucket.\nThe\ngem5 resources git repository\ncontains sources of the gem5 resources, these are then compiled and stored in the Google Cloud Bucket.\nThe gem5 resources repo\nREADME.md\ncontains links to download the built resources from the Google Cloud Bucket.\nThe Google Cloud Bucket, like the gem5 resources repository, is versioned.\nEach resource is stored under\nhttp://dist.gem5.org/dist/{major version}\n.\nE.g., the PARSEC Benchmark image, for version 20.1, is stored at\nhttp://dist.gem5.org/dist/v20-1/images/x86/ubuntu-18-04/parsec.img.gz\n, while the image for version 21.0 is stored at\nhttp://dist.gem5.org/dist/v21-0/images/x86/ubuntu-18-04/parsec.img.gz\n(note the\n.\nsubstitution with\n-\nfor the version in the URL).\nThe build for the develop branch is found under\nhttp://dist.gem5.org/dist/develop\n.\nAs the gem5 resources staging branch is from develop, the easiest way to create a copy of the develop bucket directory:\ngsutil -m cp -r gs://dist.gem5.org/dist/develop gsutil -m cp -r gs://dist.gem5.org/dist/{major version}\nThe develop bucket\nshould\nbe in-sync with the changes on develop.\nThough this is worth checking.\nNaturally, any changes on the staging branch must be reflected in the Cloud Bucket accordingly.\nImportant notes:\nDue to legacy reason\nhttp://dist.gem5.org/dist/current\nis used to store legacy resources related to v19 of gem5.\nSpecial permissions are needed to push to the Google Cloud Bucket.\nPlease contact Bobby R. Bruce (bbruce@ucdavis.edu) for help pushing resources to the bucket.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/development/release_procedures/",
            "page_title": "No Title Found",
            "parent_section": "gem5 resources repository",
            "section_heading": "gem5 resources Google Cloud Bucket"
        }
    },
    {
        "text": "The docker images\nCurrently hosted in\nutil/dockerfiles\nin the gem5 repository, we have a series of Dockerfiles which can be built to produce environments in which gem5 can be built and run.\nThese images are mostly used for testing purposes.\nThe\nubuntu-20.04_all-dependencies\nDockerfile is the one most suitable for users who wish to build and execute gem5 in a supported environment.\nWe provide pre-built Docker images hosted at\nunder \"gem5\".\nAll the Dockerfiles found in `util/dockerfiles` have been built and stored there.\nFor instance, `ubuntu-20.04_all-dependencies` can be found at <ghcr.io/gem5/ubuntu-20.04_all-dependencies> (and can thereby be obtained with `docker pull ghcr.io/gem5/ubuntu-20.04_all-dependencies`).\nThe Docker images are continually built from the Dockerfiles found on the develop branch.\nTherefore the docker image with the\nlatest\ntag is that in-sync with the Dockerfiles found on the gem5 repo\u2019s develop branch.\nUpon a release of the latest version of gem5, when the staging branches are merged into develop, the built images hosted at\nwill be tagged with the gem5 version number.\nSo, upon the release of `v23.2`, the images will be tagged with `v23-2`\nThe purpose of this is so users of an older versions of gem5, may obtain images compatible with their release.\nI.e., a user of gem5 `v21.0` may obtain the `v21.0` version of the `ubuntu-20.04_all-dependencies` with `docker pull ghcr.io/gem5/ubuntu-20.04_all-dependencies:v21-0`.\nImportant notes:\nIf changes to the Dockerfile are done on the staging branch, then these changes will need to be pushed to\nmanually.\nSpecial permissions are needed to push to the\n.\nPlease contact Bobby R. Bruce (bbruce@ucdavis.edu) for help pushing images.\nIt is a future goal of ours to move\nthe Dockerfiles from\nutil/dockerfiles\nto gem5-resources\n.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/development/release_procedures/",
            "page_title": "No Title Found",
            "parent_section": "The docker images",
            "section_heading": "Overview"
        }
    },
    {
        "text": "gem5 website repository\nThe\ngem5 website git repository\nhas two branches,\nstable\nand\ndevelop\n.\nThe stable branch is what is built and viewable at\nhttp://www.gem5.org\n, and is up-to-date with the current gem5 release.\nE.g., if the current release of gem5, on its stable branch, is\nv20.1\n, the documentation on the stable branch will related to\nv20.1\n.\nThe develop branch contains the state of the website for the upcoming gem5 release.\nE.g., it contains the changes needed to apply to the website when the new version of gem5 is released.\nAs the stable branch may be updated at any time (as long as those updates relate to the current release), stable is merged periodically into develop.\nAs with the gem5 resources, and the main gem5 repository, a staging branch is created from the develop branch at least two weeks prior to a gem5 release.\nThe staging branch needs updated so that the documentation is up-to-date with the upcoming release.\nOf particular note, references to gem5 resources, hosted on the Google Cloud bucket should be updated.\nFor example, links to, say\nhttp://dist.gem5.org/dist/v21-0/images/x86/ubuntu-18-04/parsec.img.gz\n, would need to be updated to\nhttp://dist.gem5.org/dist/v21-1/images/x86/ubuntu-18-04/parsec.img.gz\nwhen transitioning from\nv21-0\nto\nv21-1\n.\nUpon a new major gem5 release, the develop branch is merged into stable.\nThe website repo is tagged with the preceding version prior to merging the staging branch into stable.\nThis is identical to the gem5 resources repository.\nFor example, if the current release is v21.1.0.4 and the next release is v21.2.0.0, immediately prior to the release of v21.2.0.0 the stable branch will be tagged as v21.1.0.4 then the develop branch merged into stable.\nThis ensures that a user may revert the website back to its state as of a previous release, if needed.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/development/release_procedures/",
            "page_title": "No Title Found",
            "parent_section": "gem5 website repository",
            "section_heading": "Overview"
        }
    },
    {
        "text": "gem5 Doxygen\nThe\ngem5 Doxygen website\nis created by the\nDoxygen documentation generator\n.\nIt can be created in gem5 repo as follows:\ncd src\ndoxygen\nThe html will be output to\nsrc/doxygen/html\n.\nThe gem5 Doxygen website is hosted as a static webpage in a Google Cloud Bucket.\nThe directory structure is as follows:\ndoxygen.gem5.org/\n    - develop/              # Contains the Doxygen for the gem5 develop branch.\n        - index.html\n        ...\n    - release/              # An archive of the Doxygen for every gem5 release.\n        - current/          # Doxygen for the current gem5 release.\n            - index.html\n            ...\n        - v21-0-1-0/\n            - index.html\n            ...\n        - v21-0-0-0/\n            - index.html\n            ...\n        - v20-1-0-5/\n            - index.html\n            ...\n        ...\n    - index.html           # Redirects to release/current/index.html.\nTherefore, the Doxygen for the latest release can be obtained at\nhttp://doxygen.gem5.org/\n, for the develop branch at\nhttp://doxygen.gem5.org/develop\n, and for past releases at\nhttp://doxygen.gem5.org/release/{version}\n(e.g.,\nhttp://doxygen.gem5.org/release/v20-1-0-5\n).\nAfter a gem5 release the following code is run on the gem5 repository stable branch\ncd src\ndoxygen\n\ngsutil -m rm gs://doxygen.gem5.org/release/current/*\ngsutil -m cp -r doxygen/html/* gs://doxygen.gem5.org/release/current/\ngsutil -m cp -r gs://doxygen.gem5.org/release/current gs://doxygen.gem5.org/release/{version id}\nThe final step is to add a link to this gem5 Doxygen version on the website, via the\n_data/documentation.yml\nfile\n.\nFor example:\nhttps://gem5-review.googlesource.com/c/public/gem5-website/+/43385\n.\nImportant Notes:\nThe gem5 develop branch Doxygen website is updated daily via an automated build process.\nThe footer on the Doxygen website will state when the page was generated.\nSpecial permissions are needed to push to the Google Cloud Bucket.\nPlease contact Bobby R. Bruce (bbruce@ucdavis.edu) for help pushing to the Google Cloud Bucket.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/development/release_procedures/",
            "page_title": "No Title Found",
            "parent_section": "gem5 Doxygen",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Minor and Hotfix releases\nThe previous sections have focus on major gem5 releases.\nMinor and hotfix releases of gem5 should never change any API or features in a major way.\nAs such, for minor and hotfix releases of gem5 we only carry out the release procedures for the\ngem5 code repository\nand the\ngem5 Doxygen website\n.\nThe latter may be unnecessary depending on the change/changes, but this is a low cost endeavor.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/development/release_procedures/",
            "page_title": "No Title Found",
            "parent_section": "Minor and Hotfix releases",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Overview\nTo successfully run Android in gem5, an image, a compatible kernel and a device tree blob.dtb file configured for the simulator are necessary. This guide shows how to build Android Marshmallow 32bit version using a 3.14 kernel with Mali support. An extra section will be added in the future on how to build the 4.4 kernel with Mali.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/fullsystem/building_android_m",
            "page_title": "Building Android Marshmallow",
            "parent_section": "Overview",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Pre-requisites\nThis guide assumes a 64-bit system running 14.04 LTS Ubuntu. Before starting it is important first to set up our system correctly. To do this the following packages need to be installed through shell.\nTip: Always check for the up-to-date prerequisites at the Android build page.\nUpdate and install all the dependencies. This can be done with the following commands:\nsudo apt-get update\n\nsudo apt-get install openjdk-7-jdk git-core gnupg flex bison gperf build-essential zip curl zlib1g-dev gcc-multilib g++-multilib libc6-dev-i386 lib32ncurses5-dev x11proto-core-dev libx11-dev lib32z-dev ccache libgl1-mesa-dev libxml2-utils xsltproc unzip\nAlso, make sure to have repo correctly installed\n(instructions here)\n.\nEnsure that the default JDK is OpenJDK 1.7:\njavac -version\nTo cross-compile the kernel (32bit) and for the device tree we will need the following packages to be installed:\nsudo apt-get install gcc-arm-linux-gnueabihf device-tree-compiler\nBefore getting started, as a final step make sure to have the gem5 binaries and busybox for 32-bit ARM.\nFor the gem5 binaries just do the following starting from your gem5 directory:\ncd util/m5\nmake -f Makefile.arm\ncd ../term\nmake\ncd ../../system/arm/simple_bootloader/\nmake\nFor busybox you can find the guide\nhere\n.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/fullsystem/building_android_m",
            "page_title": "Building Android Marshmallow",
            "parent_section": "Pre-requisites",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Building Android\nWe build Android Marshmallow using an AOSP running build based on the release for the Pixel C. The AOSP provides\nother builds\n, which are untested with this guide.\nTip: Synching with repo will take a long time. Use the -jN flag to speed up the make process, where N is the number of parallel jobs to run.\nMake a directory and pull the Android repository:\nmkdir android\ncd android\nrepo init --depth=1 -u https://android.googlesource.com/platform/manifest -b android-6.0.1_r63\nrepo sync -c -jN\nBefore you start the AOSP build, you will need to make one change to the build system to enable building libion.so, which is used by the Mali driver. Edit the file\naosp/system/core/libion/Android.mk\nto change\nLOCAL_MODULE_TAGS\nfor libion from \u2018optional\u2019 to \u2018debug\u2019. Here is the output of\nrepo diff\n:\n--- a/system/core/libion/Android.mk\n  +++ b/system/core/libion/Android.mk\n  @@ -3,7 +3,7 @@ LOCAL_PATH := $(call my-dir)\n  include $(CLEAR_VARS)\n  LOCAL_SRC_FILES := ion.c\n  LOCAL_MODULE := libion\n  -LOCAL_MODULE_TAGS := optional\n  +LOCAL_MODULE_TAGS := debug\n  LOCAL_SHARED_LIBRARIES := liblog\n  LOCAL_C_INCLUDES := $(LOCAL_PATH)/include $(LOCAL_PATH)/kernel-headers\n  LOCAL_EXPORT_C_INCLUDE_DIRS := $(LOCAL_PATH)/include\n  $(LOCAL_PATH)/kernel-headers\nSource the environment setup and build Android:\nTip: For root access and \u201cdebuggability\u201d [sic] we choose userdebug. Build can be done in different modes as seen\nhere\n.\nTip: Making Android will take a long time. Use the -jN flag to speed up the make process, where N is the number of parallel jobs to run.\nMake sure to do this in a bash shell.\nsource build/envsetup.sh\nlunch aosp_arm-userdebug\nmake -jN",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/fullsystem/building_android_m",
            "page_title": "Building Android Marshmallow",
            "parent_section": "Building Android",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Creating an Android image\nAfter a successful build, we create an image of Android and add the init files and binaries that configure the system for gem5. The following example creates a 3GB image.\nTip: If you want to add applications or data, make the image large enough to fit the build and anything else that is meant to be written into it.\nCreate an empty image to flash the Android build and attach the image to a loopback device:\ndd if=/dev/zero of=myimage.img bs=1M count=2560\nsudo losetup /dev/loop0 myimage.img\nWe now need to create three partitions: AndroidRoot (1.5GB), AndroidData (1GB), and AndroidCache (512MB).\nFirst, partition the device:\nsudo fdisk /dev/loop0\nUpdate the partition table:\nsudo partprobe /dev/loop0\nName the partitions / Define filesystem as ext4:\nsudo mkfs.ext4 -L AndroidRoot /dev/loop0p1\nsudo mkfs.ext4 -L AndroidData /dev/loop0p\nsudo mkfs.ext4 -L AndroidCache /dev/loop0p3\nMount the Root partition to a directory:\nsudo mkdir -p /mnt/androidRoot\nsudo mount /dev/loop0p1 /mnt/androidRoot\nLoad the build to the partition:\ncd /mnt/androidRoot\nsudo zcat <path/to/build/android>/out/target/product/generic/ramdisk.img | sudo cpio -i\nsudo mkdir cache\nsudo mkdir /mnt/tmp\nsudo mount -oro,loop <path/to/build/android>/out/target/product/generic/system.img /mnt/tmp\nsudo cp -a /mnt/tmp/* system/\nsudo umount /mnt/tmp\nDownload and unpack the\noverlays\nthat are necessary from the\ngem5 Android KitKat page\nand make the following changes to the\ninit.gem5.rc\nfile. Here is the output of\nrepo diff\n:\n--- /kitkat_overlay/init.gem5.rc\n  +++ /m_overlay/init.gem5.rc\n  @@ -1,21 +1,13 @@\n  +\n   on early-init\n       mount debugfs debugfs /sys/kernel/debug\n  \n   on init\n  -    export LD_LIBRARY_PATH ${LD_LIBRARY_PATH}:/vendor/lib/egl\n  -\n  -    # See storage config details at http://source.android.com/tech/storage/\n  -    mkdir /mnt/media_rw/sdcard 0700 media_rw media_rw\n  -    mkdir /storage/sdcard 0700 root root\n  +    # Support legacy paths\n  +    symlink /sdcard /mnt/sdcard\n       chmod 0666 /dev/mali0\n       chmod 0666 /dev/ion\n  -\n  -    export EXTERNAL_STORAGE /storage/sdcard\n  -\n  -    # Support legacy paths\n  -    symlink /storage/sdcard /sdcard\n  -    symlink /storage/sdcard /mnt/sdcard\n  \n   on fs\n       mount_all /fstab.gem5\n  @@ -60,7 +52,6 @@\n       group root\n       oneshot\n  \n  -# fusewrapped external sdcard daemon running as media_rw (1023)\n  -service fuse_sdcard /system/bin/sdcard -u 1023 -g 1023 -d\n  /mnt/media_rw/sdcard /storage/sdcard\n  +service fingerprintd /system/bin/fingerprintd\n       class late_start\n  -    disabled\n  +    user system\nAdd the Android overlays and configure their permissions:\nsudo cp -r <path/to/android/overlays>/* /mnt/androidRoot/\nsudo chmod ug+x /mnt/androidRoot/init.gem5.rc\n/mnt/androidRoot/gem5/postboot.sh\nAdd the m5 and busybox binaries under the sbin directory and make them executable:\nsudo cp <path/to/gem5>/util/m5/m5 /mnt/androidRoot/sbin\nsudo cp <path/to/busybox>/busybox /mnt/androidRoot/sbin\nsudo chmod a+x /mnt/androidRoot/sbin/busybox /mnt/androidRoot/sbin/m5\nMake the directories readable and searchable:\nsudo chmod a+rx /mnt/androidRoot/sbin/ /mnt/androidRoot/gem5/\nRemove the boot animation:\nsudo rm /mnt/androidRoot/system/bin/bootanimation\nDownload and unpack the Mali drivers, for gem5 Android 4.4, from\nhere\n. Then, make the directories for the drivers and copy them:\nsudo mkdir -p /mnt/androidRoot/system/vendor/lib/egl\nsudo mkdir -p /mnt/androidRoot/system/vendor/lib/hw\nsudo cp <path/to/userspace/Mali/drivers>/lib/egl/libGLES_mali.so /mnt/androidRoot/system/vendor/lib/egl\nsudo cp <path/to/userspace/Mali/drivers>/lib/hw/gralloc.default.so /mnt/androidRoot/system/vendor/lib/hw\nChange the permissions\nsudo chmod 0755 /mnt/androidRoot/system/vendor/lib/hw\nsudo chmod 0755 /mnt/androidRoot/system/vendor/lib/egl\nsudo chmod 0644 /mnt/androidRoot/system/vendor/lib/egl/libGLES_mali.so\nsudo chmod 0644 /mnt/androidRoot/system/vendor/lib/hw/gralloc.default.so\nUnmount and remove loopback device:\ncd /..\nsudo umount /mnt/androidRoot\nsudo losetup -d /dev/loop0",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/fullsystem/building_android_m",
            "page_title": "Building Android Marshmallow",
            "parent_section": "Creating an Android image",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Building the Kernel (3.14)\nAfter successfully setting up the image, a compatible kernel needs to be built and a .dtb file generated.\nClone the repository containing the gem5 specific kernel:\ngit clone -b ll_20140416.0-gem5 https://github.com/gem5/linux-arm-gem5.git\nMake the following changes to the kernel gem5 config file at\n<path/to/kernel/repo>/arch/arm/configs/vexpress_gem5_defconfig\n. Here is the output of\nrepo diff\n:\n--- a/arch/arm/configs/vexpress_gem5_defconfig\n  +++ b/arch/arm/configs/vexpress_gem5_defconfig\n  @@ -200,4 +200,15 @@ CONFIG_EARLY_PRINTK=y\n  CONFIG_DEBUG_PREEMPT=n\n  # CONFIG_CRYPTO_ANSI_CPRNG is not set\n  # CONFIG_CRYPTO_HW is not set\n  +CONFIG_MALI_MIDGARD=y\n  +CONFIG_MALI_MIDGARD_DEBUG_SYS=y\n  +CONFIG_ION=y\n  +CONFIG_ION_DUMMY=y\n  CONFIG_BINARY_PRINTF=y\n  +CONFIG_NET_9P=y\n  +CONFIG_NET_9P_VIRTIO=y\n  +CONFIG_9P_FS=y\n  +CONFIG_9P_FS_POSIX_ACL=y\n  +CONFIG_9P_FS_SECURITY=y\n  +CONFIG_VIRTIO_BLK=y\n  +CONFIG_VMSPLIT_3G=y\n  +CONFIG_DNOTIFY=y\n  +CONFIG_FUSE_FS=y\nFor the device tree, add the Mali GPU device and increase the memory to 1.8GB. Do this with the following changes at\n<path/to/kernel/repo>/arch/arm/boot/dts/vexpress-v2p-ca15-tc1-gem5.dts.\nHere is the output of\nrepo diff\n:\n--- a/arch/arm/boot/dts/vexpress-v2p-ca15-tc1-gem5.dts\n  +++ b/arch/arm/boot/dts/vexpress-v2p-ca15-tc1-gem5.dts\n  @@ -45,7 +45,7 @@\n  \n           memory@80000000 {\n                   device_type = \"memory\";\n  -                reg = <0 0x80000000 0 0x40000000>;\n  +                reg = <0 0x80000000 0 0x74000000>;\n           };\n  \n          hdlcd@2b000000 {\n  @@ -59,6 +59,14 @@\n  //                mode = \"3840x2160MR-16@60\"; // UHD4K mode string\n                    framebuffer = <0 0x8f000000 0 0x01000000>;\n            };\n  +\n  +    gpu@0x2d000000 {\n  +        compatible = \"arm,mali-midgard\";\n  +        reg = <0 0x2b400000 0 0x4000>;\n  +        interrupts = <0 86 4>, <0 87 4>, <0 88 4>;\n  +        interrupt-names = \"JOB\", \"MMU\", \"GPU\";\n  +    };\n  +\n  /*\n          memory-controller@2b0a0000 {\n                    compatible = \"arm,pl341\", \"arm,primecell\";\nDownload and unpack the userspace matching Mali kernel drivers for gem5 from [http://malideveloper.arm.com/resources/drivers/open-source-mali-midgard-gpu-kernel-drivers/ here]. Copy them to the gpu driver directory:\ncp -r <path/to/kernelspace/Mali/drivers>/driver/product/kernel/drivers/gpu/arm/ drivers/gpu\nChange the following in\n<path/to/kernelspace/Mali/drivers>/drivers/video/Kconfig\nand\n<path/to/kernelspace/Mali/drivers>/drivers/gpu/Makefile\nbased on the following diffs:\nHere is the output of the Kconfig\nrepo diff\n:\n--- a/drivers/video/Kconfig\n  +++ b/drivers/video/Kconfig\n  @@ -23,6 +23,8 @@ source \"drivers/gpu/host1x/Kconfig\"\n  \n  source \"drivers/gpu/drm/Kconfig\"\n  \n  +source \"drivers/gpu/arm/Kconfig\"\n  +\n   config VGASTATE\n          tristate\n          default n\nHere is the output of the drivers/gpu/Makefile\nrepo diff\n:\n--- a/drivers/gpu/Makefile\n  +++ b/drivers/gpu/Makefile\n  @@ -1,2 +1,2 @@\n  -obj-y                += drm/ vga/\n  +obj-y                += drm/ vga/ arm/\nFinally, build the kernel and the .dtb file.\nTip: Use the -jN flag to speed up the make process, where N is the number of parallel jobs to run.\nBuild the kernel:\nmake CROSS_COMPILE=arm-linux-gnueabihf- ARCH=arm vexpress_gem5_defconfig\nmake CROSS_COMPILE=arm-linux-gnueabihf- ARCH=arm vmlinux -jN\nCreate the .dtb file:\ndtc -I dts -O dtb arch/arm/boot/dts/vexpress-v2p-ca15-tc1-gem5.dts > vexpress-v2p-ca15-tc1-gem5.dtb",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/fullsystem/building_android_m",
            "page_title": "Building Android Marshmallow",
            "parent_section": "Building the Kernel (3.14)",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Testing the build\nMake the following changes to example/fs.py. Here is the output\nrepo diff\n:\n--- a/configs/example/fs.py Thu Jun 02 20:34:39 2016 +0100\n  +++ b/configs/example/fs.py Fri Jun 10 15:37:29 2016 -0700\n  @@ -144,6 +144,13 @@\n       if is_kvm_cpu(TestCPUClass) or is_kvm_cpu(FutureClass):\n           test_sys.vm = KvmVM()\n  \n  +    test_sys.gpu = NoMaliGpu(\n  +        gpu_type=\"T760\",\n  +        ver_maj=0, ver_min=0, ver_status=1,\n  +        int_job=118, int_mmu=119, int_gpu=120,\n  +        pio_addr=0x2b400000,\n  +        pio=test_sys.membus.master)\n  +\n      if options.ruby:\n          # Check for timing mode because ruby does not support atomic accesses\n          if not (options.cpu_type == \"detailed\" or options.cpu_type == \"timing\"):\nAnd the changes to FS config to either enable or disable software rendering.\n--- a/configs/common/FSConfig.py Thu Jun 02 20:34:39 2016 +0100\n  +++ b/configs/common/FSConfig.py Thu Jun 16 10:23:44 2016 -0700\n  @@ -345,7 +345,7 @@\n  \n             # release-specific tweaks\n             if 'kitkat' in mdesc.os_type():\n  -                cmdline += \" androidboot.hardware=gem5 qemu=1 qemu.gles=0 \" + \\\n  +                cmdline += \" androidboot.hardware=gem5 qemu=1 qemu.gles=1 \" + \\\n                            \"android.bootanim=0\"\n  \n         self.boot_osflags = fillInCmdline(mdesc, cmdline\nSet the following M5_PATH:\nM5_PATH=. build/ARM/gem5.opt configs/example/fs.py --cpu-type=atomic --mem-type=SimpleMemory --os-type=android-kitkat --disk-image=myimage.img --machine-type=VExpress_EMM --dtb-filename=vexpress-v2p-ca15-tc1-gem5.dtb -n 1 --mem-size=1800MB",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/fullsystem/building_android_m",
            "page_title": "Building Android Marshmallow",
            "parent_section": "Testing the build",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Building older versions of Android\ngem5 has support for running even older versions of Android like KitKat. The documentation to do so, as well as the necessary drivers and files required, can be found on the old wiki\nhere\n.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/fullsystem/building_android_m",
            "page_title": "Building Android Marshmallow",
            "parent_section": "Building older versions of Android",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Prerequisites\nThese instructions are for running headless systems. That is a more \u201cserver\u201d style system where there is no frame-buffer. The description has been created using the latest known-working tag in the repositories linked below, however the tables in each section list previous tags that are known to work. To built the kernels on an x86 host you\u2019ll need ARM cross compilers and the device tree compiler. If you\u2019re running a reasonably new version of Ubuntu or Debian you can get required software through apt:\napt-get install  gcc-arm-linux-gnueabihf gcc-aarch64-linux-gnu device-tree-compiler\nIf you can\u2019t use these pre-made compilers the next easiest way to obtain the\nrequired compilers from ARM:\nCortex A cross-compilers\nCortex RM cross-compilers\nDownload (one of) these and make sure the binaries are on your\nPATH\n.\nDepending on the exact source of your cross compilers, the compiler names used below will required small changes.\nTo actually run the kernel, you\u2019ll need to download or compile gem5\u2019s\nbootloader. See the\nbootloaders\nsection in this documents for\ndetails.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/fullsystem/building_arm_kernel",
            "page_title": "Building ARM Kernel",
            "parent_section": "Prerequisites",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Linux 4.x\nNewer gem5 kernels for ARM (v4.x and later) are based on the vanilla Linux kernel and typically have a small number of patches to make them work better with gem5. The patches are optional and you should be able to use a vanilla kernel as well. However, this requires you to configure the kernel yourself. Newer kernels all use the VExpress_GEM5_V1 gem5 platform for both AArch32 and AArch64.\nKernel Checkout\nTo checkout the kernel, execute the following command:\ngit clone https://gem5.googlesource.com/arm/linux\nThe repository contains a tag per gem5 kernel releases and working branches for major Linux revisions. Check the\nproject page\nfor a list of tags and branches. The clone command will, by default, check out the latest release branch. To checkout the v4.14 branch, execute the following in the repository:\ngit checkout -b gem5/v4.14\nKernel build\nTo compile the kernel, execute the following commands in the repository:\nmake ARCH=arm64 CROSS_COMPILE=aarch64-linux-gnu- gem5_defconfig\nmake ARCH=arm64 CROSS_COMPILE=aarch64-linux-gnu- -j `nproc`\nTesting the just built kernel:\n./build/ARM/gem5.opt configs/example/arm/starter_fs.py --kernel=/tmp/linux-arm-gem5/vmlinux \\\n    --disk-image=ubuntu-18.04-arm64-docker.img\nBootloaders\nThere are two different bootloaders for gem5. One of 32-bit kernels and one for 64-bit kernels. They can be compiled using the following command:\nmake -C system/arm/bootloader/arm\nmake -C system/arm/bootloader/arm64\nDevice Tree Blobs\nThe required DTB files to describe the hardware to the OS ship with gem5. To build them, execute this command:\nmake -C system/arm/dt\nWe recommend to use these device tree files only if you are planning to amend them. If not, we recommend you to rely on DTB autogeneration: by running a FS script without the \u2013dtb option, gem5 will automatically generate the DTB on the fly depending on the instantiated platform.\nOnce you have compiled the binaries, put them in the binaries directory in your\nM5_PATH\n.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/fullsystem/building_arm_kernel",
            "page_title": "Building ARM Kernel",
            "parent_section": "Linux 4.x",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Section: I/O Device Base Classes\n\nSub-section: PioPort\n\nThe PioPort class is a programmed I/O port that all devices that are sensitive to an address range use.\nThe port takes all the memory access types and roles them into one\nread()\nand\nwrite()\ncall that the device must respond to.\nThe device must also provide the\naddressRanges()\nfunction with which it returns the address ranges it is interested in.\nIf desired a device could have more than one PIO port.\nHowever in the normal case it would only have one port and return multiple ranges when the\naddressRange()\nfunction is called. The only time multiple PIO ports would be desirable is if your device wanted to have separate connection to two memory objects.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/fullsystem/devices",
            "page_title": "Devices in full system mode",
            "parent_section": "I/O Device Base Classes",
            "section_heading": "PioPort"
        }
    },
    {
        "text": "Section: I/O Device Base Classes\n\nSub-section: PioDevice\n\nThis is the base class which all devices senstive to an address range inherit from.\nThere are three pure virtual functions which all devices must implement\naddressRanges()\n,\nread()\n, and\nwrite()\n.\nThe magic to choose which mode we are in, etc is handled by the PioPort so the device doesn\u2019t have to bother.\nParameters for each device should be in a Params struct derived from\nPioDevice::Params\n.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/fullsystem/devices",
            "page_title": "Devices in full system mode",
            "parent_section": "I/O Device Base Classes",
            "section_heading": "PioDevice"
        }
    },
    {
        "text": "Section: I/O Device Base Classes\n\nSub-section: BasicPioDevice\n\nSince most PioDevices only respond to one address range\nBasicPioDevice\nprovides an\naddressRanges()\nand parameters for the normal pio delay and the address to which the device responds to.\nSince the size of the device normally isn\u2019t configurable a parameter is not used for this and anything that inherits from this class is expected to write it\u2019s size into pioSize in its constructor.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/fullsystem/devices",
            "page_title": "Devices in full system mode",
            "parent_section": "I/O Device Base Classes",
            "section_heading": "BasicPioDevice"
        }
    },
    {
        "text": "Section: I/O Device Base Classes\n\nSub-section: DmaPort\n\nThe DmaPort (in dma_device.hh) is used only for device mastered accesses.\nThe\nrecvTimingResp()\nmethod must be available to responses (nacked or not) to requests it makes.\nThe port has two public methods\ndmaPending()\nwhich returns if the dma port is busy (e.g. It is still trying to send out all the pieces of the last request).\nAll the code to break requests up into suitably sized chunks, collect the potentially multiple responses and respond to the device is accessed through\ndmaAction()\n.\nA command, start address, size, completion event, and possibly data is handed to the function which will then execute the completion events\nprocess()\nmethod when the request has been completed.\nInternally the code uses\nDmaReqState\nto manage what blocks it has received and to know when to execute the completion event.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/fullsystem/devices",
            "page_title": "Devices in full system mode",
            "parent_section": "I/O Device Base Classes",
            "section_heading": "DmaPort"
        }
    },
    {
        "text": "Section: I/O Device Base Classes\n\nSub-section: DmaDevice\n\nThis is the base class from which a DMA non-pci device would inherit from, however none of those exist currently within M5. The class does have some methods\ndmaWrite()\n,\ndmaRead()\nthat select the appropriate command from a DMA read or write operation.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/fullsystem/devices",
            "page_title": "Devices in full system mode",
            "parent_section": "I/O Device Base Classes",
            "section_heading": "DmaDevice"
        }
    },
    {
        "text": "Section: I/O Device Base Classes\n\nSub-section: NIC Devices\n\nThe gem5 simulator has two different Network Interface Cards (NICs) devices that can be used to connect together two simulation instances over a simulated ethernet link.\nGetting a list of packets on the ethernet link\nYou can get a list of the packet on the ethernet link by creating a Etherdump object, setting it\u2019s file parameter, and setting the dump parameter on the EtherLink to it.\nThis is easily accomplished with our fs.py example configuration by adding the command line option --etherdump=<filename>. The resulting file will be named <file> and be in a standard pcap format.\nThis file can be read with\nwireshark\nor anything else that understands the pcap format.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/fullsystem/devices",
            "page_title": "Devices in full system mode",
            "parent_section": "I/O Device Base Classes",
            "section_heading": "NIC Devices"
        }
    },
    {
        "text": "Section: I/O Device Base Classes\n\nSub-section: PCI devices\n\nTo do: Explanation of platforms and systems, how they\u2019re related, and what they\u2019re each for",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/fullsystem/devices",
            "page_title": "Devices in full system mode",
            "parent_section": "I/O Device Base Classes",
            "section_heading": "PCI devices"
        }
    },
    {
        "text": "Section: 1) Using gem5 utils to create a disk image\n\nSub-section: Creating an empty image\n\nYou can use the ./util/gem5img.py script provided with gem5 to build the disk image.\nIt\u2019s a good idea to understand how to build an image in case something goes wrong or you need to do something in an unusual way.\nHowever, in this mehtod, we are using gem5img.py script to go through the process of building and formatting an image.\nIf you want to understand the guts of what it\u2019s doing see below.\nRunning gem5img.py may require you to enter the sudo password.\nYou should never run commands as the root user that you don\u2019t understand! You should look at the file util/gem5img.py and ensure that it isn\u2019t going to do anything malicious to your computer!\nYou can use the \u201cinit\u201d option with gem5img.py to create an empty image, \u201cnew\u201d, \u201cpartition\u201d, or \u201cformat\u201d to perform those parts of init independently, and \u201cmount\u201d or \u201cumount\u201d to mount or unmount an existing image.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/fullsystem/disks",
            "page_title": "Creating disk images for full system mode",
            "parent_section": "1) Using gem5 utils to create a disk image",
            "section_heading": "Creating an empty image"
        }
    },
    {
        "text": "Section: 1) Using gem5 utils to create a disk image\n\nSub-section: Mounting an image\n\nTo mount a file system on your image file, first find a loopback device and attach it to your image with an appropriate offset as will be described further in the\nFormatting\nsection.\nmount\n-o\nloop,offset\n=\n32256 foo.img\n\nA youtube video of add file using mount on Ubuntu 12.04 64bit. Video resolution can be set to 1080",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/fullsystem/disks",
            "page_title": "Creating disk images for full system mode",
            "parent_section": "1) Using gem5 utils to create a disk image",
            "section_heading": "Mounting an image"
        }
    },
    {
        "text": "Section: 1) Using gem5 utils to create a disk image\n\nSub-section: Unmounting\n\nTo unmount an image, use the umount command like you normally would.\numount",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/fullsystem/disks",
            "page_title": "Creating disk images for full system mode",
            "parent_section": "1) Using gem5 utils to create a disk image",
            "section_heading": "Unmounting"
        }
    },
    {
        "text": "Section: 1) Using gem5 utils to create a disk image\n\nSub-section: Image Contents\n\nNow that you can create an image file and mount it\u2019s file system, you\u2019ll want to actually put some files in it.\nYou\u2019re free to use whatever files you want, but the gem5 developers have found that Gentoo stage3 tarballs are a great starting point.\nThey\u2019re essentially an almost bootable and fairly minimal Linux installation and are available for a number of architectures.\nIf you choose to use a Gentoo tarball, first extract it into your mounted image.\nThe /etc/fstab file will have placeholder entries for the root, boot, and swap devices.\nYou\u2019ll want to update this file as apporpriate, deleting any entries you aren\u2019t going to use (the boot partition, for instance).\nNext, you\u2019ll want to modify the inittab file so that it uses the m5 utility program (described elsewhere) to read in the init script provided by the host machine and to run that.\nIf you allow the normal init scripts to run, the workload you\u2019re interested in may take much longer to get started, you\u2019ll have no way to inject your own init script to dynamically control what benchmarks are started, for instance, and you\u2019ll have to interact with the simulation through a simulated terminal which introduces non-determinism.\nModifications\nBy default gem5 does not store modifications to the disk back to the underlying image file.\nAny changes you make will be stored in an intermediate COW layer and thrown away at the end of the simulation.\nYou can turn off the COW layer if you want to modify the underlying disk.\nKernel and bootloader\nAlso, generally speaking, gem5 skips over the bootloader portion of boot and loads the kernel into simulated memory itself. This means that there\u2019s no need to install a bootloader like grub to your disk image, and that you don\u2019t have to put the kernel you\u2019re going to boot from on the image either.\nThe kernel is provided separately and can be changed out easily without having to modify the disk image.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/fullsystem/disks",
            "page_title": "Creating disk images for full system mode",
            "parent_section": "1) Using gem5 utils to create a disk image",
            "section_heading": "Image Contents"
        }
    },
    {
        "text": "Section: 1) Using gem5 utils to create a disk image\n\nSub-section: Manipulating images with loopback devices\n\nLoopback devices\nLinux supports loopback devices which are devices backed by files.\nBy attaching one of these to your disk image, you can use standard Linux commands on it which normally run on real disk devices.\nYou can use the mount command with the \u201cloop\u201d option to set up a loopback device and mount it somewhere.\nUnfortunately you can\u2019t specify an offset into the image, so that would only be useful for a file system image, not a disk image which is what you need.\nYou can, however, use the lower level losetup command to set up a loopback device yourself and supply the proper offset.\nOnce you\u2019ve done that, you can use the mount command on it like you would on a disk partition, format it, etc.\nIf you don\u2019t supply an offset the loopback device will refer to the whole image, and you can use your favorite program to set up the partitions on it.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/fullsystem/disks",
            "page_title": "Creating disk images for full system mode",
            "parent_section": "1) Using gem5 utils to create a disk image",
            "section_heading": "Manipulating images with loopback devices"
        }
    },
    {
        "text": "Section: 1) Using gem5 utils to create a disk image\n\nSub-section: Working with image files\n\nTo create an empty image from scratch, you\u2019ll need to create the file itself, partition it, and format (one of) the partition(s) with a file system.\nCreate the actual file\nFirst, decide how large you want your image to be.\nIt\u2019s a good idea to make it large enough to hold everything you know you\u2019ll need on it, plus some breathing room.\nIf you find out later it\u2019s too small, you\u2019ll have to create a new larger image and move everything over.\nIf you make it too big, you\u2019ll take up actual disk space unnecessarily and make the image harder to work with.\nOnce you\u2019ve decided on a size you\u2019ll want to actually create the file.\nBasically, all you need to do is create a file of a certain size that\u2019s full of zeros.\nOne approach is to use the dd command to copy the right number of bytes from /dev/zero into the new file.\nAlternatively you could create the file, seek in it to the last byte, and write one zero byte.\nAll of the space you skipped over will become part of the file and is defined to read as zeroes, but because you didn\u2019t explicitly write any data there, most file systems are smart enough to not actually store that to disk.\nYou can create a large image that way but take up very little space on your physical disk.\nOnce you start writing to the file later that will change, and also if you\u2019re not careful, copying the file may expand it to its full size.\nPartitioning\nFirst, find an available loopback device using the losetup command with the -f option.\nlosetup\n-f\nNext, use losetup to attach that device to your image.\nIf the available device was /dev/loop0 and your image is foo.img, you would use a command like this.\nlosetup /dev/loop0 foo.img\n/dev/loop0 (or whatever other device you\u2019re using) will now refer to your entire image file.\nUse whatever partitioning program you like on it to set up one (or more) paritions.\nFor simplicity it\u2019s probably a good idea to create only one parition that takes up the entire image.\nWe say it takes up the entire image, but really it takes up all the space except for the partition table itself at the beginning of the file, and possibly some wasted space after that for DOS/bootloader compatibility.\nFrom now on we\u2019ll want to work with the new partition we created and not the whole disk, so we\u2019ll free up the loopback device using losetup\u2019s -d option\nlosetup\n-d\n/dev/loop0\nFormatting\nFirst, find an available loopback device like we did in the partitioning step above using losetup\u2019s -f option.\nlosetup\n-f\nWe\u2019ll attach our image to that device again, but this time we only want to refer to the partition we\u2019re going to put a file system on.\nFor PC and Alpha systems, that partition will typically be one track in, where one track is 63 sectors and each sector is 512 bytes, or 63 * 512 = 32256 bytes.\nThe correct value for you may be different, depending on the geometry and layout of your image.\nIn any case, you should set up the loopback device with the -o option so that it represents the partition you\u2019re interested in.\nlosetup\n-o\n32256 /dev/loop0 foo.img\nNext, use an appropriate formating command, often mke2fs, to put a file system on the partition.\nmke2fs /dev/loop0\nYou\u2019ve now successfully created an empty image file.\nYou can leave the loopback device attached to it if you intend to keep working with it (likely since it\u2019s still empty) or clean it up using losetup -d.\nlosetup\n-d\n/dev/loop0\nDon\u2019t forget to clean up the loopback device attached to your image with the losetup -d command.\nlosetup\n-d\n/dev/loop0",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/fullsystem/disks",
            "page_title": "Creating disk images for full system mode",
            "parent_section": "1) Using gem5 utils to create a disk image",
            "section_heading": "Working with image files"
        }
    },
    {
        "text": "Section: 2) Using gem5 utils and chroot to create a disk image\n\nSub-section: Creating a blank disk image\n\nThe first step is to create a blank disk image (usually a .img file).\nThis is similar to what we did in the first metod.\nWe can use the gem5img.py script provided by gem5 developers.\nTo create a blank disk image, which is formatted with ext2 by default, simply run the following.\n> util/gem5img.py init ubuntu-14.04.img 4096\nThis command creates a new image, called \u201cubuntu-14.04.img\u201d that is 4096 MB.\nThis command may require you to enter the sudo password, if you don\u2019t have permission to create loopback devices.\nYou should never run commands as the root user that you don\u2019t understand! You should look at the file util/gem5img.py and ensure that it isn\u2019t going to do anything malicious to your computer!\nWe will be using util/gem5img.py heavily throughout this section, so you may want to understand it better.\nIf you just run\nutil/gem5img.py\n, it displays all of the possible commands.\nUsage: %s [command] <command arguments>\nwhere [command] is one of\n    init: Create an image with an empty file system.\n    mount: Mount the first partition in the disk image.\n    umount: Unmount the first partition in the disk image.\n    new: File creation part of \"init\".\n    partition: Partition part of \"init\".\n    format: Formatting part of \"init\".\nWatch for orphaned loopback devices and delete them with\nlosetup -d. Mounted images will belong to root, so you may need\nto use sudo to modify their contents",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/fullsystem/disks",
            "page_title": "Creating disk images for full system mode",
            "parent_section": "2) Using gem5 utils and chroot to create a disk image",
            "section_heading": "Creating a blank disk image"
        }
    },
    {
        "text": "Section: 2) Using gem5 utils and chroot to create a disk image\n\nSub-section: Copying root files to the disk\n\nNow that we have created a blank disk, we need to populate it with all of the OS files.\nUbuntu distributes a set of files explicitly for this purpose.\nYou can find the\nUbuntu core\ndistribution for 14.04 at\nhttp://cdimage.ubuntu.com/releases/14.04/release/\n. Since we are simulating an x86 machine, we will use\nubuntu-core-14.04-core-amd64.tar.gz\n.\nDownload whatever image is appropriate for the system you are simulating.\nNext, we need to mount the blank disk and copy all of the files onto the disk.\nmkdir mnt\n../../util/gem5img.py mount ubuntu-14.04.img mnt\nwget http://cdimage.ubuntu.com/ubuntu-core/releases/14.04/release/ubuntu-core-14.04-core-amd64.tar.gz\nsudo tar xzvf ubuntu-core-14.04-core-amd64.tar.gz -C mnt\nThe next step is to copy a few required files from your working system onto the disk so we can chroot into the new disk. We need to copy\n/etc/resolv.conf\nonto the new disk.\nsudo cp /etc/resolv.conf mnt/etc/",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/fullsystem/disks",
            "page_title": "Creating disk images for full system mode",
            "parent_section": "2) Using gem5 utils and chroot to create a disk image",
            "section_heading": "Copying root files to the disk"
        }
    },
    {
        "text": "Section: 2) Using gem5 utils and chroot to create a disk image\n\nSub-section: Setting up gem5-specific files\n\nCreate a serial terminal\nBy default, gem5 uses the serial port to allow communication from the host system to the simulated system. To use this, we need to create a serial tty.\nSince Ubuntu uses upstart to control the init process, we need to add a file to /etc/init which will initialize our terminal.\nAlso, in this file, we will add some code to detect if there was a script passed to the simulated system.\nIf there is a script, we will execute the script instead of creating a terminal.\nPut the following code into a file called /etc/init/tty-gem5.conf\n# ttyS0 - getty\n#\n# This service maintains a getty on ttyS0 from the point the system is\n# started until it is shut down again, unless there is a script passed to gem5.\n# If there is a script, the script is executed then simulation is stopped.\n\nstart on stopped rc RUNLEVEL=[12345]\nstop on runlevel [!12345]\n\nconsole owner\nrespawn\nscript\n   # Create the serial tty if it doesn't already exist\n   if [ ! -c /dev/ttyS0 ]\n   then\n      mknod /dev/ttyS0 -m 660 /dev/ttyS0 c 4 64\n   fi\n\n   # Try to read in the script from the host system\n   /sbin/m5 readfile > /tmp/script\n   chmod 755 /tmp/script\n   if [ -s /tmp/script ]\n   then\n      # If there is a script, execute the script and then exit the simulation\n      exec su root -c '/tmp/script' # gives script full privileges as root user in multi-user mode\n      /sbin/m5 exit\n   else\n      # If there is no script, login the root user and drop to a console\n      # Use m5term to connect to this console\n      exec /sbin/getty --autologin root -8 38400 ttyS0\n   fi\nend script\nSetup localhost\nWe also need to set up the localhost loopback device if we are going to use any applications that use it.\nTo do this, we need to add the following to the\n/etc/hosts\nfile.\n127.0.0.1 localhost\n::1 localhost ip6-localhost ip6-loopback\nfe00::0 ip6-localnet\nff00::0 ip6-mcastprefix\nff02::1 ip6-allnodes\nff02::2 ip6-allrouters\nff02::3 ip6-allhosts\nUpdate fstab\nNext, we need to create an entry in\n/etc/fstab\nfor each partition we want to be able to access from the simulated system. Only one partition is absolutely required (\n/\n); however, you may want to add additional partitions, like a swap partition.\nThe following should appear in the file\n/etc/fstab\n.\n# /etc/fstab: static file system information.\n#\n# Use 'blkid' to print the universally unique identifier for a\n# device; this may be used with UUID= as a more robust way to name devices\n# that works even if disks are added and removed. See fstab(5).\n#\n# <file system>    <mount point>   <type>  <options>   <dump>  <pass>\n/dev/hda1      /       ext3        noatime     0 1\nCopy the\nm5\nbinary to the disk\ngem5 comes with an extra binary application that executes pseudo-instructions to allow the simulated system to interact with the host system.\nTo build this binary, run\nmake -f Makefile.<isa>\nin the\ngem5/m5\ndirectory, where\n<isa>\nis the ISA that you are simulating (e.g., x86). After this, you should have an\nm5\nbinary file.\nCopy this file to /sbin on your newly created disk.\nAfter updating the disk with all of the gem5-specific files, unless you are going on to add more applications or copying additional files, unmount the disk image.\n> util/gem5img.py umount mnt",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/fullsystem/disks",
            "page_title": "Creating disk images for full system mode",
            "parent_section": "2) Using gem5 utils and chroot to create a disk image",
            "section_heading": "Setting up gem5-specific files"
        }
    },
    {
        "text": "Section: 2) Using gem5 utils and chroot to create a disk image\n\nSub-section: Install new applications\n\nThe easiest way to install new applications on to your disk, is to use\nchroot\n.\nThis program logically changes the root directory (\u201c/\u201d) to a different directory, mnt in this case.\nBefore you can change the root, you first have to set up the special directories in your new root. To do\nthis, we use\nmount -o bind\n.\n> sudo /bin/mount -o bind /sys mnt/sys\n> sudo /bin/mount -o bind /dev mnt/dev\n> sudo /bin/mount -o bind /proc mnt/proc\nAfter binding those directories, you can now\nchroot\n:\n> sudo /usr/sbin/chroot mnt /bin/bash\nAt this point you will see a root prompt and you will be in the\n/\ndirectory of your new disk.\nYou should update your repository information.\n> apt-get update\nYou may want to add the universe repositories to your list with the\nfollowing commands.\nNote: The first command is require in 14.04.\n> apt-get install software-properties-common\n> add-apt-repository universe\n> apt-get update\nNow, you are able to install any applications you could install on a\nnative Ubuntu machine via\napt-get\n.\nRemember, after you exit you need to unmount all of the directories we\nused bind on.\n> sudo /bin/umount mnt/sys\n> sudo /bin/umount mnt/proc\n> sudo /bin/umount mnt/dev",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/fullsystem/disks",
            "page_title": "Creating disk images for full system mode",
            "parent_section": "2) Using gem5 utils and chroot to create a disk image",
            "section_heading": "Install new applications"
        }
    },
    {
        "text": "Section: 3) Using QEMU to create a disk image\n\nSub-section: Step 1: Create an empty disk\n\nUsing the qemu disk tools, create a blank raw disk image.\nIn this case, I chose to create a disk named \u201cubuntu-test.img\u201d that is 8GB.\nqemu-img create ubuntu-test.img 8G",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/fullsystem/disks",
            "page_title": "Creating disk images for full system mode",
            "parent_section": "3) Using QEMU to create a disk image",
            "section_heading": "Step 1: Create an empty disk"
        }
    },
    {
        "text": "Section: 3) Using QEMU to create a disk image\n\nSub-section: Step 2: Install ubuntu with qemu\n\nNow that we have a blank disk, we are going to use qemu to install Ubuntu on the disk.\nIt is encouraged that you use the server version of Ubuntu since gem5 does not have great support for displays.\nThus, the desktop environment isn\u2019t very useful.\nFirst, you need to download the installation CD image from the\nUbuntu website\n.\nNext, use qemu to boot off of the CD image, and set the disk in the system to be the blank disk you created above.\nUbuntu needs at least 1GB of memory to install correctly, so be sure to configure qemu to use at least 1GB memory.\nqemu-system-x86_64 -hda ../gem5-fs-testing/ubuntu-test.img -cdrom ubuntu-16.04.1-server-amd64.iso -m 1024 -enable-kvm -boot d\nWith this, you can simply follow the on-screen directions to install Ubuntu to the disk image.\nThe only gotcha in the installation is that gem5\u2019s IDE drivers don\u2019t seem to play nicely with logical paritions.\nThus, during the Ubuntu install, be sure to manually partition the disk and remove any logical partitions.\nYou don\u2019t need any swap space on the disk anyway, unless you\u2019re doing something specifically with swap space.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/fullsystem/disks",
            "page_title": "Creating disk images for full system mode",
            "parent_section": "3) Using QEMU to create a disk image",
            "section_heading": "Step 2: Install ubuntu with qemu"
        }
    },
    {
        "text": "Section: 3) Using QEMU to create a disk image\n\nSub-section: Step 3: Boot up and install needed software\n\nOnce you have installed Ubuntu on the disk, quit qemu and remove the\n-boot d\noption so that you are not booting off of the CD anymore.\nNow, you can again boot off of the main disk image you have installed Ubuntu on.\nSince we\u2019re using qemu, you should have a network connection (although\nping won\u2019t\nwork\n).\nWhen booting in qemu, you can just use\nsudo apt-get install\nand\ninstall any software you need on your disk.\nqemu-system-x86_64 -hda ../gem5-fs-testing/ubuntu-test.img -cdrom ubuntu-16.04.1-server-amd64.iso -m 1024 -enable-kvm",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/fullsystem/disks",
            "page_title": "Creating disk images for full system mode",
            "parent_section": "3) Using QEMU to create a disk image",
            "section_heading": "Step 3: Boot up and install needed software"
        }
    },
    {
        "text": "Section: 3) Using QEMU to create a disk image\n\nSub-section: Step 4: Update init script\n\nBy default, gem5 expects a modified init script which loads a script off of the host to execute in the guest.\nTo use this feature, you need to follow the steps below.\nAlternatively, you can install the precompiled binaries for x86 found on this\nwebsite\n.\nFrom qemu, you can run the following, which completes the above steps for you.\nwget http://cs.wisc.edu/~powerjg/files/gem5-guest-tools-x86.tgz\ntar xzvf gem5-guest-tools-x86.tgz\ncd gem5-guest-tools/\nsudo ./install\nNow, you can use the\nsystem.readfile\nparameter in your Python config scripts. This file will automatically be loaded (by the\ngem5init\nscript) and executed.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/fullsystem/disks",
            "page_title": "Creating disk images for full system mode",
            "parent_section": "3) Using QEMU to create a disk image",
            "section_heading": "Step 4: Update init script"
        }
    },
    {
        "text": "Section: 3) Using QEMU to create a disk image\n\nSub-section: Manually installing the gem5 init script\n\nFirst, build the m5 binary on the host.\ncd util/m5\nmake -f Makefile.x86\nThen, copy this binary to the guest and put it in\n/sbin\n. Also, create a link from\n/sbin/gem5\n.\nThen, to get the init script to execute when gem5 boots, create file /lib/systemd/system/gem5.service with the following:\n[Unit]\nDescription=gem5 init script\nDocumentation=http://gem5.org\nAfter=getty.target\n\n[Service]\nType=idle\nExecStart=/sbin/gem5init\nStandardOutput=tty\nStandardInput=tty-force\nStandardError=tty\n\n[Install]\nWantedBy=default.target\nEnable the gem5 service and\ndisable the ttyS0 service\n.\nIf your disk boots up to a login prompt, it might be caused by not disabling the ttyS0 service.\nsystemctl enable gem5.service\nFinally, create the init script that is executed by the service. In\n/sbin/gem5init\n:\n#!/bin/bash -\n\nCPU=`cat /proc/cpuinfo | grep vendor_id | head -n 1 | cut -d ' ' -f2-`\necho \"Got CPU type: $CPU\"\n\nif [ \"$CPU\" != \"M5 Simulator\" ];\nthen\n    echo \"Not in gem5. Not loading script\"\n    exit 0\nfi\n\n# Try to read in the script from the host system\n/sbin/m5 readfile > /tmp/script\nchmod 755 /tmp/script\nif [ -s /tmp/script ]\nthen\n    # If there is a script, execute the script and then exit the simulation\n    su root -c '/tmp/script' # gives script full privileges as root user in multi-user mode\n    sync\n    sleep 10\n    /sbin/m5 exit\nfi\necho \"No script found\"",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/fullsystem/disks",
            "page_title": "Creating disk images for full system mode",
            "parent_section": "3) Using QEMU to create a disk image",
            "section_heading": "Manually installing the gem5 init script"
        }
    },
    {
        "text": "Section: 3) Using QEMU to create a disk image\n\nSub-section: Problems and (some) solutions\n\nYou might run into some problems while following this method.\nSome of the issues and solutions are discussed on this\npage\n.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/fullsystem/disks",
            "page_title": "Creating disk images for full system mode",
            "parent_section": "3) Using QEMU to create a disk image",
            "section_heading": "Problems and (some) solutions"
        }
    },
    {
        "text": "Section: 4) Using Packer to create a disk image\n\nSub-section: Building a Simple Disk Image with Packer\n\na. How It Works, Briefly\nWe use\nPacker\nand\nQEMU\nto automate the process of disk creation.\nEssentially, QEMU is responsible for setting up a virtual machine and all interactions with the disk image during the building process.\nThe interactions include installing Ubuntu Server to the disk image, copying files from your machine to the disk image, and running scripts on the disk image after Ubuntu is installed.\nHowever, we will not use QEMU directly.\nPacker provides a simpler way to interact with QEMU using a JSON script, which is more expressive than using QEMU from command line.\nb. Install Required Software/Dependencies\nIf not already installed, QEMU can be installed using:\nsudo\napt-get\ninstall\nqemu\nDownload the Packer binary from\nthe official website\n.\nc. Customize the Packer Script\nThe default packer script\ntemplate.json\nshould be modified and adapted according to the required disk image and the avaiable resources for the build proces. We will rename the default template to\n[disk-name].json\n. The variables that should be modified appear at the end of\n[disk-name].json\nfile, in\nvariables\nsection.\nThe configuration files that are used to build the disk image, and the directory structure is shown below:\ndisk-image/\n[\ndisk-name].json: packer script\n    Any experiment-specific post installation script\n    post-installation.sh: generic shell script that is executed after Ubuntu is installed\n    preseed.cfg: preseeded configuration to\ninstall\nUbuntu\ni. Customizing the VM (Virtual Machine)\nIn\n[disk-name].json\n, following variables are available to customize the VM:\nVariable\nPurpose\nExample\nvm_cpus\n(should be modified)\nnumber of host CPUs used by VM\n\u201c2\u201d: 2 CPUs are used by the VM\nvm_memory\n(should be modified)\namount of VM memory, in MB\n\u201c2048\u201d: 2 GB of RAM are used by the VM\nvm_accelerator\n(should be modified)\naccelerator used by the VM e.g. Kvm\n\u201ckvm\u201d: kvm will be used\n\nii. Customizing the Disk Image\nIn\n[disk-name].json\n, disk image size can be customized using following variable:\nVariable\nPurpose\nExample\nimage_size\n(should be modified)\nsize of the disk image, in megabytes\n\u201c8192\u201d: the image has the size of 8 GB\n[image_name]\nname of the built disk image\n\u201cboot-exit\u201d\n\niii. File Transfer\nWhile building a disk image, users would need to move their files (benchmarks, data sets etc.) to\nthe disk image. In order to do this file transfer, in\n[disk-name].json\nunder\nprovisioners\n, you could add the following:\n{\n\"type\"\n:\n\"file\"\n,\n\"source\"\n:\n\"post_installation.sh\"\n,\n\"destination\"\n:\n\"/home/gem5/\"\n,\n\"direction\"\n:\n\"upload\"\n}\nThe above example copies the file\npost_installation.sh\nfrom the host to\n/home/gem5/\nin the disk image.\nThis method is also capable of copying a folder from host to the disk image and vice versa.\nIt is important to note that the trailing slash affects the copying process\n(more details)\n.\nThe following are some notable examples of the effect of using slash at the end of the paths.\nsource\ndestination\ndirection\nEffect\nfoo.txt\n/home/gem5/bar.txt\nupload\ncopy file (host) to file (image)\nfoo.txt\nbar/\nupload\ncopy file (host) to folder (image)\n/foo\n/tmp\nupload\nmkdir /tmp/foo\n(image);\ncp -r /foo/* (host) /tmp/foo/ (image)\n;\n/foo/\n/tmp\nupload\ncp -r /foo/* (host) /tmp/ (image)\nIf\ndirection\nis\ndownload\n, the files will be copied from the image to the host.\nNote\n:\nThis is a way to run script once after installing Ubuntu without copying to the disk image\n.\niv. Install Benchmark Dependencies\nTo install the dependencies, you can use a bash script\npost_installation.sh\n, which will be run after the Ubuntu installation and file copying is done.\nFor example, if we want to install\ngfortran\n, add the following in\npost_installation.sh\n:\necho\n'12345'\n|\nsudo\napt-get\ninstall\ngfortran\n;\nIn the above example, we assume that the user password is\n12345\n.\nThis is essentially a bash script that is executed on the VM after the file copying is done, you could modify the script as a bash script to fit any purpose.\nv. Running Other Scripts on Disk Image\nIn\n[disk-name].json\n, we could add more scripts to\nprovisioners\n.\nNote that the files are on the host, but the effects are on the disk image.\nFor example, the following example runs\npost_installation.sh\nafter Ubuntu is installed,\n{\n\"type\"\n:\n\"shell\"\n,\n\"execute_command\"\n:\n\"echo '{{ user\n`\nssh_password\n`\n}}' | {{.Vars}} sudo -E -S bash '{{.Path}}'\"\n,\n\"scripts\"\n:\n[\n\"post-installation.sh\"\n]\n}\nd. Build the Disk Image\ni. Build\nIn order to build a disk image, the template file is first validated using:\n./packer validate\n[\ndisk-name].json\nThen, the template file can be used to build the disk image:\n./packer build\n[\ndisk-name].json\nOn a fairly recent machine, the building process should not take more than 15 minutes to complete.\nThe disk image with the user-defined name (image_name) will be produced in a folder called [image_name]-image.\nWe recommend to use a VNC viewer in order to inspect the building process\n.\nii. Inspect the Building Process\nWhile the building of disk image takes place, Packer will run a VNC (Virtual Network Computing) server and you will be able to see the building process by connecting to the VNC server from a VNC client. There are a plenty of choices for VNC client. When you run the Packer script, it will tell you which port is used by the VNC server. For example, if it says\nqemu: Connecting to VM via VNC (127.0.0.1:5932)\n, the VNC port is 5932.\nTo connect to VNC server from the VNC client, use the address\n127.0.0.1:5932\nfor a port number 5932.\nIf you need port forwarding to forward the VNC port from a remote machine to your local machine, use SSH tunneling\nssh\n-L\n5932:127.0.0.1:5932 <username>@<host>\nThis command will forward port 5932 from the host machine to your machine, and then you will be able to connect to the VNC server using the address\n127.0.0.1:5932\nfrom your VNC viewer.\nNote\n: While Packer is installing Ubuntu, the terminal screen will display \u201cwaiting for SSH\u201d without any update for a long time.\nThis is not an indicator of whether the Ubuntu installation produces any errors.\nTherefore, we strongly recommend using VNC viewer at least once to inspect the image building process.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/fullsystem/disks",
            "page_title": "Creating disk images for full system mode",
            "parent_section": "4) Using Packer to create a disk image",
            "section_heading": "Building a Simple Disk Image with Packer"
        }
    },
    {
        "text": "last edited:\n2025-08-21 21:40:17 +0000\nm5 term\nThe m5term program allows the user to connect to the simulated console interface that full-system gem5 provides. Simply change into the util/term directory and build m5term:\n% cd gem5/util/term\n% make\ngcc  -o m5term term.c\n% make install\nsudo install -o root -m 555 m5term /usr/local/bin\nThe usage of m5term is:\n./m5term <host> <port>\n<host> is the host that is running gem5\n\n<port> is the console port to connect to. gem5 defaults to\nusing port 3456, but if the port is used, it will try the next\nhigher port until it finds one available.\n\nIf there are multiple systems running within one simulation,\nthere will be a console for each one.  (The first system's\nconsole will be on 3456 and the second on 3457 for example)\n\nm5term uses '~' as an escape character.  If you enter\nthe escape character followed by a '.', the m5term program\nwill exit.\nm5term can be used to interactively work with the simulator, though users must often set various terminal settings to get things to work\nA slightly shortened example of m5term in action:\n% m5term localhost 3456\n==== m5 slave console: Console 0 ====\nM5 console\nGot Configuration 127\nmemsize 8000000 pages 4000\nFirst free page after ROM 0xFFFFFC0000018000\nHWRPB 0xFFFFFC0000018000 l1pt 0xFFFFFC0000040000 l2pt 0xFFFFFC0000042000 l3pt_rpb 0xFFFFFC0000044000 l3pt_kernel 0xFFFFFC0000048000 l2reserv 0xFFFFFC0000046000\nCPU Clock at 2000 MHz IntrClockFrequency=1024\nBooting with 1 processor(s)\n...\n...\nVFS: Mounted root (ext2 filesystem) readonly.\nFreeing unused kernel memory: 480k freed\ninit started:  BusyBox v1.00-rc2 (2004.11.18-16:22+0000) multi-call binary\n\nPTXdist-0.7.0 (2004-11-18T11:23:40-0500)\n\nmounting filesystems...\nEXT2-fs warning: checktime reached, running e2fsck is recommended\nloading script...\nScript from M5 readfile is empty, starting bash shell...\n# ls\nbenchmarks  etc         lib         mnt         sbin        usr\nbin         floppy      lost+found  modules     sys         var\ndev         home        man         proc        tmp         z\n#",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/fullsystem/m5term",
            "page_title": "m5 term",
            "parent_section": "Overview",
            "section_heading": "m5term"
        }
    },
    {
        "text": "Section: How is the gem5 API documented?\n\nSub-section: Notes for developers\n\nIf a developer wishes to tag a new method/variable as part of the gem5 API,\nthe gem5 community should be consulted. APIs are intended to stay unaltered for\nsome time. To avoid the gem5 project becoming encumbered with \u201ctoo many APIs\u201d,\nwe strongly advise those wishing to extend the API to communicate to the\ngem5 development team as to why the API will be of value. The\ngem5 Discussion page\nis a good communication channel for this.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/gem5-apis",
            "page_title": "The gem5 API",
            "parent_section": "How is the gem5 API documented?",
            "section_heading": "Notes for developers"
        }
    },
    {
        "text": "Section: How can the API change?\n\nSub-section: Notes for Developers\n\nPrior to making any changes to the gem5 API the\ngem5-dev mailing list\nshould be consulted. Changing the API, for whatever reason,\nwill\nbe subject to higher scrutiny than other changes. Developers should\nbe prepared to provide compelling arguments as to why the API needs changed. We\nstrongly recommend API changes are discussed or they may be rejected during the\ncode review.\nWhen creating a new API the old API must be tagged as deprecated and the new\nAPI created to exist alongside the old.\nIt is of upmost importance that the\nold, deprecated API is maintained and not deleted\n.\nAs an example, take the following code:\n/**\n * @ingroup api_bitfield\n */\ninline\nuint64_t\nmask\n(\nint\nfirst\n,\nint\nlast\n)\n{\nreturn\nmbits\n((\nuint64_t\n)\n-\n1LL\n,\nfirst\n,\nlast\n);\n}\nThis function is part of the gem5 bitfield API. It is a basic mask function\nthat takes the MSB (first) and the LSB (last) for the generation of a 64-bit.\nLet us assume there is a good argument that this function should be replaced\nwith one that takes the MSB (first), and the length of the mask instead.\nTo start, the old API needs maintained (i.e., not changed) and tagged with the\n[[deprecated(<msg>)]]\ntag. The message (\n<msg>\n) Should state the new API\nto use, and the API tagging should be removed. The new API should then be\ncreated and tagged. So, using our example:\n[[\ndeprecated\n(\n\"Use mask_length instead.\"\n)]]\ninline\nuint64_t\nmask\n(\nint\nfirst\n,\nint\nlast\n)\n{\nreturn\nmbits\n((\nuint64_t\n)\n-\n1LL\n,\nfirst\n,\nlast\n);\n}\n/**\n * @ingroup api_bitfield\n */\ninline\nuint64_t\nmask_length\n(\nint\nfirst\n,\nint\nlength\n)\n{\nreturn\nmbits\n((\nuint64_t\n)\n-\n1LL\n,\nfirst\n,\nfirst\n+\nlength\n);\n}\nHere a new function,\nmask_length\n, has been created. It has been tagged\ncorrectly via Doxygen. The old API,\nmask\nexists but has the\n[[deprecated]]\nannotation added. The message provided states which API\nreplaces it.\nThe developer then needs to replace all usage of\nmask\nin the code-base with\nmask_length\n. A warning will be given at compile time if\nmask\nis used,\nstating that it is deprecated and to \u201cUse mask_length instead.\u201d.\nOccasionally there may be need to change the python API interface, which\nrelates to tagged APIs. For example, let\u2019s take the below code:\nclass\nTLBCoalescer\n(\nClockedObject\n):\ntype\n=\n'TLBCoalescer'\ncxx_class\n=\n'TLBCoalescer'\ncxx_header\n=\n'gpu-compute/tlb_coalescer.hh'\n...\nslave\n=\nVectorResponsePort\n(\n\"Port on side closer to CPU/CU\"\n)\nmaster\n=\nVectorRequestPort\n(\n\"Port on side closer to memory\"\n)\n...\nIn recent revisions\nthe terms\nmaster\nand\nslave\nhave been replaced. Though, the\nslave\nand\nmaster\nterminology are widely used, so much so we consider them part of the\nold API. We therefore wish to deprecate this API is a safe manner while\nchanging\nmaster\nand\nslave\nwith\ncpu_side_ports\nand\nmem_side_ports\n. To\ndo so we would maintain the\nmaster\nand\nslave\nvariables but utilize our\nDeprecatedParam\nClass\nto produce warnings when and if these deprecated variables are used. Working on\nour example, we would produce the following:\nclass\nTLBCoalescer\n(\nClockedObject\n):\ntype\n=\n'TLBCoalescer'\ncxx_class\n=\n'TLBCoalescer'\ncxx_header\n=\n'gpu-compute/tlb_coalescer.hh'\n...\ncpu_side_ports\n=\nVectorResponsePort\n(\n\"Port on side closer to CPU/CU\"\n)\nslave\n=\nDeprecatedParam\n(\ncpu_side_ports\n,\n'`slave` is now called `cpu_side_ports`'\n)\nmem_side_ports\n=\nVectorRequestPort\n(\n\"Port on side closer to memory\"\n)\nmaster\n=\nDeprecatedParam\n(\nmem_side_ports\n,\n'`master` is now called `mem_side_ports`'\n)\n...\nNote the use of\nDeprecatedParam\nthat both ensures\nmaster\nand\nslave\nstill\nfunction by redirecting to\nmem_side_ports\nand\ncpu_side_ports\nrespectively,\nas well as providing a comment explaining why this API was deprecated. This\nwill be displayed to the user as a warning if\nmaster\nor\nslave\nare ever\nused.\nAs with all changes to the gem5 source, these changes will have to go through\nour Gerrit code review system before being merged into the\ndevelop\nbranch,\nand eventually making its way to our\nstable\nbranch as part of a gem5 release.\nIn line with our API policy, these deprecated APIs must exist in a\nmarked-as-deprecated state for two gem5 major release cycles. After this they\nmay be removed though developers are under no requirement to do so.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/gem5-apis",
            "page_title": "The gem5 API",
            "parent_section": "How can the API change?",
            "section_heading": "Notes for Developers"
        }
    },
    {
        "text": "Why gem5 Resources?\ngem5 has been designed with flexibility in mind. Users may simulate a wide\nvariety of hardware, with an equally wide variety of workloads. However,\nrequiring users to find and configure workloads for gem5 (their own disk\nimages, their own OS boots, their own tests, etc.) is a significant\ninvestment, and a hurdle to many.\nThe purpose of gem5 Resources is therefore\nto provide a stable set of\ncommonly used resources, with proven and documented compatibility with gem5\n.\nIn addition to this, gem5 resources also puts emphasis on\nreproducibility\nof experiments\nby providing citable, stable resources, tied to a particular\nrelease of gem5.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/gem5_resources",
            "page_title": "gem5 Resources",
            "parent_section": "Why gem5 Resources?",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Where can I obtain the gem5 Resources?\nTo find a specific resource with the gem5 Resources, we recommend using the\ngem5 Resources Website\n. Detailed information on how searching, filtering and sorting works on this website is on this\nhelp page\n.\nThe gem5 Resources are hosted on our Google Cloud Bucket. Links to the\nresources can be found\ngem5 resources README.md file\n.\nThe resource metadata is stored in a MongoDB database hosted on MongoDB Atlas.\nTo request updates to gem5 resources, create an issue or mail gem5-dev.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/gem5_resources",
            "page_title": "gem5 Resources",
            "parent_section": "Where can I obtain the gem5 Resources?",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Using a Resource from the gem5 Resources Website in gem5\nWhen you find the Resource that you want to use in your simulation, navigate to the \u2018Usage\u2019 tab of that Resource.\nFor the purpose of this tutorial, let\u2019s assume that the Resource you are looking for is\nriscv-hello\n, found\nhere\n.In the\n\u2018Usage\u2019\ntab of this Resource, you will find the code that can be pasted in a gem5 simulation to use this Resource.\nIn this case, the code is\nobtain_resource(resource_id=\"riscv-hello\")\n.\nTo use the\nobtain_resource\nfunction, you require the following import statement:\nfrom gem5.resources.resource import obtain_resource\nThe\nobtain_resource\nfunction accepts the following parameters:\nresource_id\n: The ID of the Resource you want to use.\nresource_version\n: An optional parameter that specifies the version of the Resource you want to use. If not specified, the latest version of the Resource compatible with the version of gem5 being used will be used.\nclients\n: An optional parameter that specifies the list of clients that gem5 would search for the Resource in. If not specified, gem5 will search for the Resource in all clients specified in the\nsrc/python/gem5_default_config.py\nfile. By default, gem5 will use the public MongoDB metadata database to find resources. This can be overridden to specify your own local resource metadata.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/gem5_resources",
            "page_title": "gem5 Resources",
            "parent_section": "Using a Resource from the gem5 Resources Website in gem5",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Using a Workload from the gem5 Resources Website in gem5\nWhen you find the Workload that you want to use in your simulation, navigate to the \u2018Usage\u2019 tab of that Workload.\nFor the purpose of this tutorial, let\u2019s assume that the Workload you are looking for is\nriscv-ubuntu-20.04-boot\n, found\nhere\n. In the\n\u2018Usage\u2019\ntab of this Workload, you will find the code that can be pasted in a gem5 simulation to use this Workload.\nIn this case, the code is\nWorkload(\"riscv-ubuntu-20.04-boot\")\n.\nTo use the\nWorkload\nclass, you require the following import statement:\nfrom gem5.resources.workload import Workload\nThe\nWorkload\nclass accepts the following parameters:\nworkload_name\n: The name of the Workload you want to use.\nresource_directory\n: An optional parameter that specifies where any resources should be download and accessed from.\nresource_version\n: An optional parameter that specifies the version of the Resource that should be used. If not specified, the latest version of the Resource compatible with the version of gem5 being used will be used.\nclients\n: An optional parameter that specifies a list of clients that gem5 would search for the Resource in. If not specified, gem5 will search for the Resource in all clients specified in the\nsrc/python/gem5_default_config.py\nfile.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/gem5_resources",
            "page_title": "gem5 Resources",
            "parent_section": "Using a Workload from the gem5 Resources Website in gem5",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Using a Custom Resource in gem5\nTo use a Custom Resource in gem5, we recommend using one of the supported data sources formats in gem5. Currently, we support MongoDB Atlas, local JSON files and remote JSON files.\nYou can use your own config file by overriding the\nGEM5_DEFAULT_CONFIG\nvariable while running a file.\nNOTE: Any Custom Resource you add must be compliant with the\ngem5 Resources Schema\n.\nThere is a utility in\nutils/gem5-resources-manager\nwhich provides a GUI for updating and creating resources for both the public resources (only modifiable by gem5 admins) and local resource metadata.\nYou can find more information on the gem5 Resources Manager in the README file.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/gem5_resources",
            "page_title": "gem5 Resources",
            "parent_section": "Using a Custom Resource in gem5",
            "section_heading": "Overview"
        }
    },
    {
        "text": "How do I obtain the gem5 Resource sources?\ngem5 resources sources may be obtained from\nhttps://github.com/gem5/gem5-resources\n:\ngit clone https://github.com/gem5/gem5-resources\nThe HEAD of the\nstable\nbranch will point towards a set of resource sources\ncompatible with the latest release of gem5 (which can be obtained via\ngit clone https://github.com/gem5/gem5.git\n).\nPlease consult the\nREADME.md\nfile for information on compiling individual gem5 resources. Where license\npermits, the\nREADME.md\nfile will provide a link to download the compiled resource from our\ndist.gem5.org Google Cloud Bucket.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/gem5_resources",
            "page_title": "gem5 Resources",
            "parent_section": "How do I obtain the gem5 Resource sources?",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Section: How is gem5 Resources repository constructed?\n\nSub-section: Versioning\n\nEach resource can have multiple versions. A version is in the form of\n<major>.<minor>.<patch>\n. The versioning scheme is based on\nSemantic\nVersioning\n. Each version of a resource is linked to one\nor more gem5 versions (e.g., v20.0, v20.1, v20.2, etc.).\nBy default, gem5 uses the latest version of a resource compatible with the\nversion of gem5 being used. However, users may specify a particular version\nof a resource to use. If a user specifies a version of a resource that is not\ncompatible with the version of gem5 being used, gem5 will throw a warning.\nYou may still use the resource at your own risk.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/gem5_resources",
            "page_title": "gem5 Resources",
            "parent_section": "How is gem5 Resources repository constructed?",
            "section_heading": "Versioning"
        }
    },
    {
        "text": "Section: How is gem5 Resources repository constructed?\n\nSub-section: Citing a Resource\n\nWe strongly recommend gem5 Resources are cited in publications to aid in\nreplication of experiments, tutorials, etc.\nTo cite as a URL, please use the following formats:\n# For the git repository at a particular revision:\nhttps://github.com/gem5/gem5-resources/<revision>/src/<resource>\n\n# For the git repository at a particular tag:\nhttps://github.com/gem5/gem5-resources/tree/<branch>/src/<resource>\nAlternatively, as BibTex:\n@misc{gem5-resources,\n  title = {gem5 Resources. Resource: <resource>},\n  howpublished = {\\url{https://github.com/gem5/gem5-resources/<revision>/src/<resource>}},\n  note = {Git repository at revision '<revision>'}\n}\n\n@misc{gem5-resources,\n  title = {gem5 Resources. Resource: <resource>},\n  howpublished = {\\url{https://github.com/gem5/gem5-resources/tree/<branch>/src/<resource>}},\n  note = {Git repository at tag '<tag>'}\n}",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/gem5_resources",
            "page_title": "gem5 Resources",
            "parent_section": "How is gem5 Resources repository constructed?",
            "section_heading": "Citing a Resource"
        }
    },
    {
        "text": "How to I contribute to gem5 Resources?\nChanges to the gem5 Resources repository are made to the develop branch via our\nGerrit code review system. Therefore, to make changes, first clone the\nrepository:\ngit clone https://github.com/gem5/gem5-resources.git\nThen make changes and commit. When ready, push to Gerrit with:\ngit push origin HEAD:refs/for/stable\nThis will add resources to be used in the latest release of gem5.\nTo contribute resources to the next release of gem5,\ngit clone https://github.com/gem5/gem5-resources.git\ngit checkout --track origin/develop\nThen make changes, commit, and push with:\ngit push origin HEAD:refs/for/develop\nCommit message heads should not exceed 65 characters and start with the tag\nresources:\n. The description after the header must not exceed 72 characters.\nE.g.:\nresources: Adding a new resources X\n\nThis is where the description of this commit will occur taking into\nnote the 72 character line limit.\nWe strongly advise contributors follow our\nStyle Guide\nwhere\npossible and appropriate.\nAny change will then be reviewed via our\nGerrit code review system\n. Once fully accepted and merged into\nthe gem5 resources repository, please contact Bobby R. Bruce\n(\nbbruce@ucdavis.edu\n) to have any compiled sources\nuploaded to the gem5 resources bucket.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/gem5_resources",
            "page_title": "gem5 Resources",
            "parent_section": "How to I contribute to gem5 Resources?",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Requirements\nThe Full System GPU model is primarily designed to simulate discrete GPUs using a native software stack without modification. This means that the CPU portion of simulation is not configured for detailed simulation \u2013 only the GPU is detailed. The\nROCm software stack\nlimits usage to officially supported gfx9 devices listed in the\nROCm documentation\n. Currently gem5 provides configurations for Vega10 (gfx900), MI210/MI250X (gfx90a), and MI300X (gfx942).\nNote:\nPreviously supported \u201cgfx9\u201d devices in older versions of ROCm still work in most cases (gfx900, gfx906). As mentioned in the ROCm documentation, these may result in runtime errors for prebuilt ROCm libraries.\nThe CPU portion of code is ideally fast-forwarded using the KVM CPU model. Since the software stack is x86 you will need an x86 Linux host with KVM enabled to run Full System efficiently. The atomic CPU can also be used to run on non-x86 hosts or where KVM is not usable. See the\nRunning without KVM\nsection for details.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/gpu_models/gpufs",
            "page_title": "Full System AMD GPU model",
            "parent_section": "Requirements",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Using the model\nSeveral places in this guide assume that gem5 and gem5-resources are located in the same base directory.\nThe\ngem5 repository\ncontains the base code of the GPU model.\nThe\ngem5-resources repository\ncontains files needed to create a disk image for Full System and comes with a number of sample applications that can be used to get started with the model. We recommend users start with\nsquare\n, as it is a simple, heavily tested application that should run relatively quickly.\nBuilding gem5\nThe GPU model requires the GPU_VIPER cache coherence protocol which is implemented in Ruby and the Full System software stack is only supported in a simulated X86 environment. The VEGA_X86 build option uses the GPU_VIPER protocol and x86. Therefore, gem5 must be built using the VEGA_X86 build option:\nscons build/VEGA_X86/gem5.opt\nThe Full System GPU model is built similarly to a CPU only version of gem5. Refer to the\nbuilding gem5\ndocumentation for how to build gem5, including number of build threads, linker options, and gem5 binary targets.\nBuilding Disk Image and Kernel\nJust like a CPU only version of gem5, the Full System GPU model requires a disk image and kernel to run. The\ngem5-resources repository\nprovides a one-step disk image builder to create a disk image for the GPU model with all of the software requirements installed.\nFrom your base directory with gem5 and gem5-resources cloned, navigate to\ngem5-resources/src/x86-ubuntu-gpu-ml\n. This directory contains a file\n./build.sh\nto create the disk image in one step. Building the disk depends on the\npacker\ntool which uses\nQEMU\nas a backend. See the\nBUILDING.md\nguide for troubleshooting. Generally, the disk image can be created in one step using the following command:\n./build.sh\nThis process takes approximately 15-20 minutes and is mostly bound by download speed as a majority of the time is spent downloading Ubuntu packages.\nBuilding the disk image will also extract the Linux kernel. The extracted Linux kernel\nmust\nbe used with the disk image. In other words, you cannot input an arbitrary kernel to gem5 otherwise the GPU driver may not load successfully.\nAfter this process your environment should contain:\nDisk image:\ngem5-resources/src/x86-ubuntu-gpu-ml/disk-image/x86-ubuntu-gpu-ml\nKernel:\ngem5-resources/src/x86-ubuntu-gpu-ml/vmlinux-gpu-ml\nBuilding GPU applications\nThe GPU model is designed to run unmodified GPU binaries. If you have an application which runs on AMD GPU hardware and that hardware is supported in gem5, you can run the same binary in gem5. Note that as this is simulation, the application will need to be scaled down to a reasonable size to simulate in a realistic amount of time.\nBuilding applications for the GPU model is similar to\ncross compiling\nwhen the simulated ISA does not match the host. Either you must have the development tools installed locally or containerization like Docker can be used. Docker images to build GPU applications are provided with gem5 in\nutil/dockerfiles/gpu-fs\n. You may either build this image or use the gem5 provided image at\nghcr.io/gem5/gpu-fs\n. This docker image provides a specific version of ROCm. The ROCm version in the Dockerfile must match the ROCm version on the disk image being used to simulate gem5. The docker and disk image versions are synced upon gem5 releases. The instructions below show an example using the pre-built gem5 docker on GitHub container registry (ghcr.io).\nSquare\nis a simple application provided in gem5-resources which can be used to get started with the model. Generally, the\nsrc/gpu\ndirectory of gem5-resources contains a\nMakefile.default\nwhich is used to build a native application and\nMakefile.gpufs\nwhich contains application annotated with\nm5ops\nthat will only run within gem5.\nTo build square using the gem5 provided docker image, navigate to the square directory and use the\nMakefile.default\nMakefile:\ncd gem5-resources/src/gpu/square\ndocker run --rm -u $UID:$GID -v $PWD:$PWD -w $PWD ghcr.io/gem5/gpu-fs make -f Makefile.default\nThe square binary should then be located at\ngem5-resources/src/gpu/square/bin.default/square.default\nTesting GPU application\nThe GPU model provides multiple gfx9 configurations to simulate GPU applications. The configurations specify the ISA (e.g., gfx942, gfx90a) and generally a minimally sized device.\nThey are not intended to be indicative of real hardware measurements\n. In the gem5 repository, these are:\nMI300X:\nconfigs/example/gpufs/mi300.py\nMI210 / MI250:\nconfigs/example/gpufs/mi200.py\nThe GPU model uses config script based configuration (i.e., not\nstandard library\n) which uses command line arguments as the primary way to modify simulation parameters. However, most common configuration options are set by the top-level scripts (e.g.,\nconfigs/example/gpufs/mi300.py\n). The main required arguments are disk image, kernel, and application.\nUsing the disk image and kernel created above and the square binary built above, square can be run with the following command:\nbuild/VEGA_X86/gem5.opt configs/example/gpufs/mi300.py --disk-image gem5-resources/src/x86-ubuntu-gpu-ml/disk-image/x86-ubuntu-gpu-ml --kernel gem5-resources/src/x86-ubuntu-gpu-ml/vmlinux-gpu-ml --app gem5-resources/src/gpu/square/bin.default/square.default\nIn Full System the output of the simulator and the output of the simulated system are shown in two separate locations. By default, the gem5 output prints to the terminal where gem5 is run. The simulated terminal output is located in the gem5 output directory which is\nm5out\nby default.\nOnce gem5 completes (or while running) the output of the Full System simulation can be seen in\nm5out/system.pc.com_1.device\n. For the square example, the application will print \u201cPASSED!\u201d to the simulated terminal output upon successful completion.\nUsing Python or shell scripts\nPython scripts such as PyTorch, TensorFlow, etc. and shell scripts can be passed directly as the value of the\n--app\ncommand line. For example, the following minimal PyTorch application can be run directly when saved as\npytorch_test.py\n:\n#!/usr/bin/env python3\n\nimport torch\n\nx = torch.rand(5, 3).to('cuda')\ny = torch.rand(3, 5).to('cuda')\n\nz = x @ y\nFor example:\nbuild/VEGA_X86/gem5.opt configs/example/gpufs/mi300.py --disk-image gem5-resources/src/x86-ubuntu-gpu-ml/disk-image/x86-ubuntu-gpu-ml --kernel gem5-resources/src/x86-ubuntu-gpu-ml/vmlinux-gpu-ml --app ./pytorch_test.py\nInput files\nThe GPU model configuration files are designed to copy the file provided to the\n--app\noption into the simulator.\nFull System gem5 cannot read files from your host system!\nIf your application requires input files, they must be copied into the disk image. See instructions for\nextending the disk image\nfor ways to do this.\nIf your application requires input files, it is recommended to create a shell script and pass the shell script to the\n--app\noption. The shell script should be written with paths relative to the disk image paths as it will run within gem5. For example, if your application requires\nfoo.dat\n, create a shell script such as:\n#!/bin/bash\n\n# We have previously copied foo.dat to /data outside of simulation.\ncd /data\nmy_gpu_app -i foo.dat",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/gpu_models/gpufs",
            "page_title": "Full System AMD GPU model",
            "parent_section": "Using the model",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Advanced Usage\nRunning without KVM\nThe AtomicSimpleCPU can also be used in situations where the host is not x86 or KVM is not available. To enable the Atomic CPU, you will need to modify your config (e.g.,\nconfigs/example/gpufs/mi300.py\n) and replace\nargs.cpu_type = \"X86KvmCPU\"\nwith\nargs.cpu_type = \"AtomicSimpleCPU\"\n.\nNote that this will slow down the CPU portion of your simulation potentially by 100x. It is possible to speed this up using\ncheckpoints\n.\nCheckpoints\nThe config scripts provided allow for checkpointing after Linux boots out of the box. It is recommended to use this when using the atomic CPU. To create a checkpoint after boot, simply add a\n--checkpoint-dir\nto the command line with a directory to place the checkpoint. For example:\nbuild/VEGA_X86/gem5.opt configs/example/gpufs/mi300.py --disk-image gem5-resources/src/x86-ubuntu-gpu-ml/disk-image/x86-ubuntu-gpu-ml --kernel gem5-resources/src/x86-ubuntu-gpu-ml/vmlinux-gpu-ml --app gem5-resources/src/gpu/square/bin.default/square.default --checkpoint-dir square-cpt\nThe checkpoint can then be restored and re-simulating the application will take significantly less time. To restore a checkpoint, replace the\n--checkpoint-dir\noption with\n--restore-dir\n:\nbuild/VEGA_X86/gem5.opt configs/example/gpufs/mi300.py --disk-image gem5-resources/src/x86-ubuntu-gpu-ml/disk-image/x86-ubuntu-gpu-ml --kernel gem5-resources/src/x86-ubuntu-gpu-ml/vmlinux-gpu-ml --app gem5-resources/src/gpu/square/bin.default/square.default --restore-dir square-cpt\nCheckpoints can also be taken using the\nm5_checkpoint(..)\npseudo instruction\nor by checkpointing in the python configs after an exit event. For example, kernel exit events can be enabled using\n--exit-at-gpu-task=-1\nand the config can be modified to create a checkpoint at the\nNth\nkernel by checking the current task number in\nconfigs/example/gpufs/runfs.py\n.\nNote that checkpoints are currently not supported within a GPU kernel. Thus, checkpoints must be taken when no GPU kernels are running.\nBuild GPU custom applications\nIf you want to build an application that is not part of gem5-resources, you will want to build the GPU application targeting either\ngfx90a\n(MI210 and MI250),\ngfx942\n(MI300X), or both. For example:\nhipcc my_gpu_app.cpp -o my_gpu_app --offload-arch=gfx90a,gfx942\nYou can build without a docker image on an x86 Linux host by installing the rocm-dev package after setting up a package manager following the steps in the\nROCm Linux documentation\n.\nModifying GPU configuration\nThe configurations in\nconfigs/example/gpufs/\nare helper configurations that interface with\nconfigs/example/gpufs/runfs.py\nand set meaningful default values for a specific device. Some parameters of interest in this file are the number of compute units, the GPU topology, the system memory size, and the CPU type.\nSome of these parameters\nonly\nmodify the value in gem5 and do not change the simulated device. In particular the dgpu_mem_size parameter does not change the amount of memory seen by the device driver and is hardcoded to 16GB in C++. Changing this value will result in a gem5 fatal.\nThe supported cpu_types are X86KvmCPU and AtomicSimpleCPU as timing CPUs do not support the disjointed Ruby network required to simulate a discrete GPU.\nOther parameters related to GPU can be found in\nconfigs/example/gpufs/system/amdgpu.py\nwhich creates the compute units for the GPU. See the ComputeUnit class in\nsrc/gpu-compute/GPU.py\nfor all available options. Note that not all possible combinations of options can be tested. Options such as queue sizes and latencies are generally safe to modify.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/gpu_models/gpufs",
            "page_title": "Full System AMD GPU model",
            "parent_section": "Advanced Usage",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Using the model\nCurrently, the AMD VEGA GPU model in gem5 is supported on the stable and develop branch.\nThe\ngem5 repository\ncomes with a dockerfile located in\nutil/dockerfiles/gcn-gpu/\n. This dockerfile contains the drivers and libraries needed to run the GPU model. A pre-built version of the docker image is hosted at\nghcr.io/gem5-test/gcn-gpu:v23-1\n.\nThe\ngem5-resources repository\nalso comes with a number of sample applications that can be used to verify that the model runs correctly.  We recommend users start with\nsquare\n, as it is a simple, heavily tested application that should run relatively quickly.\nUsing the image\nThe docker image can either be built or pulled from ghcr.io.\nTo build the docker image from source:\n# Working directory: gem5/util/dockerfiles/gcn-gpu\ndocker build -t <image_name> .\nTo pull the pre-built docker image (Note the\nv23-1\ntag, to get the correct\nimage for this release):\ndocker pull ghcr.io/gem5-test/gcn-gpu:v23-1\nYou can also put\nghcr.io/gem5-test/gcn-gpu:v23-1\nas the image in the docker run command without pulling beforehand and it will be pulled automatically.\nBuilding gem5 using the image\nSee square in\ngem5 resources\nfor an example of how to build gem5 in the docker.  Note: these directions assume you are pulling the latest image automatically.\nBuilding & running a GPU application using the image\nSee\ngem5 resources\nfor examples of how to build and run GPU applications in the docker.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/gpu_models/vega",
            "page_title": "System Emulation AMD VEGA GPU model",
            "parent_section": "Using the model",
            "section_heading": "Overview"
        }
    },
    {
        "text": "ROCm\nThe AMD VEGA GPU model was designed with enough fidelity to not require an emulated runtime. Instead, the model uses the Radeon Open Compute platform (ROCm). ROCm is an open platform from AMD that implements\nHeterogeneous Systems Architecture (HSA)\nprinciples. More information about the HSA standard can be found on the HSA Foundation\u2019s website. More information about ROCm can be found on the\nROCm website\nSimulation support for ROCm\nThe model currently works with system-call emulation (SE) mode and full-system (FS) mode.\nIn SE mode, all kernel level driver functionality is modeled entirely within the SE mode layer of gem5. In particular, the emulated GPU driver supports the necessary\nioctl()\ncommands it receives from the userspace code. The source for the emulated GPU driver can be found in:\nThe GPU compute driver:\nsrc/gpu-compute/gpu_compute_driver.[hh|cc]\nThe HSA device driver:\nsrc/dev/hsa/hsa_driver.[hh|cc]\nThe HSA driver code models the basic functionality for an HSA agent, which is any device that can be targeted by the HSA runtime and accepts Architected Query Language (AQL) packets. AQL packets are a standard format for all HSA agents, and are used primarily to initiate kernel launches on the GPU. The base\nHSADriver\nclass holds a pointer to the HSA packet processor for the device, and defines the interface for any HSA device. An HSA agent does not have to be a GPU, it could be a generic accelerator, CPU, NIC, etc.\nThe\nGPUComputeDriver\nderives from\nHSADriver\nand is a device-specific implementation of an\nHSADriver\n. It provides the implementation for GPU-specific\nioctl()\ncalls.\nThe\nsrc/dev/hsa/kfd_ioctl.h\nheader must match the\nkfd_ioctl.h\nheader that comes with ROCt. The emulated driver relies on that file to interpret the\nioctl()\ncodes the thunk uses.\nIn FS mode, the real amdgpu Linux driver is used and installed as you would on a real machine. The source for the driver can instead be found in the\nROCK-Kernel-Driver\nrepository.\nROCm toolchain and software stack\nThe AMD VEGA GPU model supports ROCm versions up to 5.4 in FS mode and 4.0 in SE mode.\nThe following ROCm components are required in SE mode:\nHeterogeneous Compute Compiler (HCC)\nRadeon Open Compute runtime (ROCr)\nRadeon Open Compute thunk (ROCt)\nHIP\nThe following additional components are used to build and run machine learning programs:\nhipBLAS\nrocBLAS\nMIOpen\nrocm-cmake\nPyTorch\n(FS mode only)\nTensorflow\n- specifically the tensorflow-rocm python package (FS mode only)\nFor information about installing these components locally, the commands in the GCN3 dockerfile (\nutil/dockerfiles/gcn-gpu/\n) can be followed on an Ubuntu 16 machine.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/gpu_models/vega",
            "page_title": "System Emulation AMD VEGA GPU model",
            "parent_section": "ROCm",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Documentation and Tutorials\nNote that the VEGA ISA is a newer, superset ISA derived from GCN3. Therefore, the contents of the following papers, tutorials, and documentation apply to VEGA as well.\nGPU Model\nDescribes the gem5 GPU model with the GCN3 ISA (at the time of writing). VEGA is a newer, superset ISA derived from GCN3. Therefore the contents of the following papers)\nHPCA 2018\ngem5 GCN3 ISCA tutorial\nCovers information about the GPU architecture, GCN3 ISA and HW-SW interfaces in gem5. Also provides an introduction to ROCm.\ngem5 GCN3 ISCA webpage\ngem5 GCN3 ISCA slides\nVEGA ISA\nVEGA ISA\nROCm Documentation\nContains further documentation about the ROCm stack, as well as programming guides for using ROCm.\nROCm webpage\nAMDGPU LLVM Information\nLLVM AMDGPU",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/gpu_models/vega",
            "page_title": "System Emulation AMD VEGA GPU model",
            "parent_section": "Documentation and Tutorials",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Building M5 and libm5\nIn order to build m5 and libm5.a for your target ISA, run the following command in the util/m5/ directory.\nscons build/\n{\nTARGET_ISA\n}\n/out/m5\nThe list of target ISAs is shown below.\nx86\narm (arm-linux-gnueabihf-gcc)\nthumb (arm-linux-gnueabihf-gcc)\nsparc (sparc64-linux-gnu-gcc)\narm64 (aarch64-linux-gnu-gcc)\nriscv (riscv64-unknown-linux-gnu-gcc)\nNote if you are using a x86 system for other ISAs you need to have the cross-compiler installed. The name of the cross-compiler is shown inside the parentheses in the list above.\nSee\nutil/m5/README.md\nfor more details.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/m5ops",
            "page_title": "M5ops",
            "parent_section": "Building M5 and libm5",
            "section_heading": "Overview"
        }
    },
    {
        "text": "The m5 Utility (FS mode)\nThe m5 utility (see util/m5/) can be used in FS mode to issue special instructions to trigger simulation specific functionality. It currently offers the following options:\ninitparam: Deprecated, present only for old binary compatibility\nexit [delay]: Stop the simulation in delay nanoseconds.\nresetstats [delay [period]]: Reset simulation statistics in delay nanoseconds; repeat this every period nanoseconds.\ndumpstats [delay [period]]: Save simulation statistics to a file in delay nanoseconds; repeat this every period nanoseconds.\ndumpresetstats [delay [period]]: same as dumpstats; resetstats\ncheckpoint [delay [period]]: Create a checkpoint in delay nanoseconds; repeat this every period nanoseconds.\nreadfile: Print the file specified by the config parameter system.readfile. This is how the the rcS files are copied into the simulation environment.\ndebugbreak: Call debug_break() in the simulator (causes simulator to get SIGTRAP signal, useful if debugging with GDB).\nswitchcpu: Cause an exit event of type, \u201cswitch cpu,\u201d allowing the Python to switch to a different CPU model if desired.\nworkbegin: Cause an exit evet of type, \u201cworkbegin\u201d, that could be used to mark the begining of an ROI.\nworkend: Cause an exit event of type, \u201cworkend\u201d, that could be used to mark the termination of an ROI.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/m5ops",
            "page_title": "M5ops",
            "parent_section": "The m5 Utility (FS mode)",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Other M5 ops\nThese are other M5 ops that aren\u2019t useful in command line form.\nquiesce: De-schedule the CPUs tick() call until some asynchronous event wakes it (an interrupt)\nquiesceNS: Same as above, but automatically wakes after a number of nanoseconds if it\u2019s not woken up prior\nquiesceCycles: Same as above but with CPU cycles instead of nanoseconds\nquisceTIme: The amount of time the CPU was quiesced for\naddsymbol: Add a symbol to the simulators symbol table. For example when a kernel module is loaded",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/m5ops",
            "page_title": "M5ops",
            "parent_section": "Other M5 ops",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Using gem5 ops in Java code\nThese ops can also be used in Java code. These ops allow gem5 ops to be called from within java programs like the following:\nimport\njni.gem5Op\n;\npublic\nclass\nHelloWorld\n{\npublic\nstatic\nvoid\nmain\n(\nString\n[]\nargs\n)\n{\ngem5Op\ngem5\n=\nnew\ngem5Op\n();\nSystem\n.\nout\n.\nprintln\n(\n\"Rpns0:\"\n+\ngem5\n.\nrpns\n());\nSystem\n.\nout\n.\nprintln\n(\n\"Rpns1:\"\n+\ngem5\n.\nrpns\n());\n}\nstatic\n{\nSystem\n.\nloadLibrary\n(\n\"gem5OpJni\"\n);\n}\n}\nWhen building you need to make sure classpath includes gem5OpJni.jar:\njavac\n-\nclasspath\n$CLASSPATH\n:\n/\npath\n/\nto\n/\ngem5OpJni\n.\njar\nHelloWorld\n.\njava\nand when running you need to make sure both the java and library path are set:\njava\n-\nclasspath\n$CLASSPATH\n:\n/\npath\n/\nto\n/\ngem5OpJni\n.\njar\n-\nDjava\n.\nlibrary\n.\npath\n=\n/path/\nto\n/\nlibgem5OpJni\n.\nso\nHelloWorld",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/m5ops",
            "page_title": "M5ops",
            "parent_section": "Using gem5 ops in Java code",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Using gem5 ops with Fortran code\ngem5\u2019s special opcodes (psuedo instructions) can be used with Fortran programs. In the Fortran code, one can add calls to C functions that invoke the special opcode. While creating the final binary, compile the object files for the Fortran program and the C program (for opcodes) together. I found the documentation provided\nhere\nuseful. Read the section\n-\n- Compiling a mixed C-Fortran program\n.\nThe idea of using gem5 ops with Fortran code is essentially to compile the m5 ops C code to an object file, and then link the object file against the binary calling the m5 ops.\nThe C function calling convention in Fortran is such that, if the function name in C code is\nvoid foo_bar_(void)\n, then in Fortran, you can call the function by\ncall foo_bar\n.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/m5ops",
            "page_title": "M5ops",
            "parent_section": "Using gem5 ops with Fortran code",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Linking M5 to your C/C++ code\nIn order to link m5 to your code, first build\nlibm5.a\nas described in the section above.\nThen\nInclude\ngem5/m5ops.h\nin your source file(s)\nAdd\ngem5/include\nto your compiler\u2019s include search path\nAdd\ngem5/util/m5/build/{TARGET_ISA}/out\nto the linker search path\nLink against\nlibm5.a\nFor example, this could be achieved by adding the following to your Makefile:\nCFLAGS += -I$(GEM5_PATH)/include\nLDFLAGS += -L$(GEM5_PATH)/util/m5/build/$(TARGET_ISA)/out -lm5\nHere is a simple Makefile example:\nTARGET_ISA\n=\nx86\nGEM5_HOME\n=\n$(\nrealpath\n./\n)\n$(info\nGEM5_HOME\nis\n$(GEM5_HOME))\nCXX\n=\ng++\nCFLAGS\n=\n-I\n$(GEM5_HOME)\n/include\nLDFLAGS\n=\n-L\n$(GEM5_HOME)\n/util/m5/build/\n$(TARGET_ISA)\n/out\n-lm5\nOBJECTS\n=\nhello_world\nall\n:\nhello_world\nhello_world\n:\n$(CXX)\n-o\n$(OBJECTS)\nhello_world.cpp\n$(CFLAGS)\n$(LDFLAGS)\nclean\n:\nrm\n-f\n$(OBJECTS)",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/m5ops",
            "page_title": "M5ops",
            "parent_section": "Linking M5 to your C/C++ code",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Using the \u201c_addr\u201d version of M5ops\nThe \u201c_addr\u201d version of m5ops triggers the same simulation specific functionality as the default m5ops, but they use different trigger mechanisms. Below is a quote from the m5 utility README.md explaining the trigger mechanisms.\nThe bare function name as defined in the header file will use the magic instruction based trigger mechanism, what would have historically been the default.\n\nSome macros at the end of the header file will set up other declarations which mirror all of the other definitions, but with an \u201c_addr\u201d and \u201c_semi\u201d suffix. These other versions will trigger the same gem5 operations, but using the \u201cmagic\u201d address or semihosting trigger mechanisms. While those functions will be unconditionally declared in the header file, a definition will exist in the library only if that trigger mechanism is supported for that ABI.\nNote\n: The macros generating the \u201c_addr\u201d and \u201c_semi\u201d m5ops are called\nM5OP\n, which are defined in\nutil/m5/abi/*/m5op_addr.S\nand\nutil/m5/abi/*/m5op_semi.S\n.\nIn order to use the \u201c_addr\u201d version of m5ops, you need to include the m5_mmap.h header file, pass the \u201cmagic\u201d address (e.g., \u201c0xFFFF0000\u201d for x86, and \u201c0x10010000\u201d for arm64/riscv) to m5op_addr, then call the map_m5_mem() to open /dev/mem. You can insert m5ops by adding \u201c_addr\u201d at the end of the original m5ops functions.\nHere is a simple example using the \u201c_addr\u201d version of the m5ops:\n#include\n<gem5/m5ops.h>\n#include\n<m5_mmap.h>\n#include\n<stdio.h>\n#define GEM5\nint\nmain\n(\nvoid\n)\n{\n#ifdef GEM5\nm5op_addr\n=\n0xFFFF0000\n;\nmap_m5_mem\n();\nm5_work_begin_addr\n(\n0\n,\n0\n);\n#endif\nprintf\n(\n\"hello world!\n\\n\n\"\n);\n#ifdef GEM5\nm5_work_end_addr\n(\n0\n,\n0\n);\nunmap_m5_mem\n();\n#endif\n}\nNote\n: You\u2019ll need to add a new header location for the compiler to find the\nm5_mmap.h\n.\nIf you are following the example Makefile above, you can add the following line below where CFLAGS is defined,\nCFLAGS\n+=\n$\n(\nGEM5_PATH\n)\n/\nutil\n/\nm5\n/\nsrc\n/\nWhen you run the applications with m5ops inserted in FS mode with a KVM CPU, this error might appear.\n```illegal instruction (core dumped)```\nThis is because m5ops instructions are not valid instructions to the host. Using the \u201c_addr\u201d version of the m5ops can fix this issue, so it is necessary to use the \u201c_addr\u201d version if you want to integrate m5ops into your applications or use the m5 binary utility when running with KVM CPUs.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/m5ops",
            "page_title": "M5ops",
            "parent_section": "Using the \u201c_addr\u201d version of M5ops",
            "section_heading": "Overview"
        }
    },
    {
        "text": "authors:\nJason Lowe-Power\nlast edited:\n2025-08-21 21:40:17 +0000\nMemory system\nM5\u2019s new memory system (introduced in the first 2.0 beta release) was\ndesigned with the following goals:\nUnify timing and functional accesses in timing mode. With the old\nmemory system the timing accesses did not have data and just\naccounted for the time it would take to do an operation. Then a\nseparate functional access actually made the operation visible to\nthe system. This method was confusing, it allowed simulated\ncomponents to accidentally cheat, and prevented the memory system\nfrom returning timing-dependent values, which isn\u2019t reasonable for\nan execute-in-execute CPU model.\nSimplify the memory system code \u2013 remove the huge amount of\ntemplating and duplicate code.\nMake changes easier, specifically to allow other memory\ninterconnects besides a shared bus.\nFor details on the new coherence protocol, introduced (along with a\nsubstantial cache model rewrite) in 2.0b4, see\nCoherence\nProtocol\n.\nMemObjects\nAll objects that connect to the memory system inherit from\nMemObject\n.\nThis class adds the pure virtual functions\ngetMasterPort(const\nstd::string &name, PortID idx)\nand\ngetSlavePort(const std::string\n&name, PortID idx)\nwhich returns a port corresponding to the given name\nand index. This interface is used to structurally connect the MemObjects\ntogether.\nPorts\nThe next large part of the memory system is the idea of ports. Ports are\nused to interface memory objects to each other. They will always come in\npairs, with a MasterPort and a SlavePort, and we refer to the other port\nobject as the peer. These are used to make the design more modular. With\nports a specific interface between every type of object doesn\u2019t have to\nbe created. Every memory object has to have at least one port to be\nuseful. A master module, such as a CPU, has one or more MasterPort\ninstances. A slave module, such as a memory controller, has one or more\nSlavePorts. An interconnect component, such as a cache, bridge or bus,\nhas both MasterPort and SlavePort instances.\nThere are two groups of functions in the port object. The\nsend*\nfunctions are called on the port by the object that owns that port. For\nexample to send a packet in the memory system a CPU would call\nmyPort->sendTimingReq(pkt)\n. Each send function has a\ncorresponding recv function that is called on the ports peer. So the\nimplementation of the\nsendTimingReq()\ncall above would simply be\npeer->recvTimingReq(pkt)\non the slave port. Using this method we only\nhave one virtual function call penalty but keep generic ports that can\nconnect together any memory system objects.\nMaster ports can send requests and receive responses, whereas slave\nports receive requests and send responses. Due to the coherence\nprotocol, a slave port can also send snoop requests and receive snoop\nresponses, with the master port having the mirrored interface.\nConnections\nIn Python, Ports are first-class attributes of simulation objects, much\nlike Params. Two objects can specify that their ports should be\nconnected using the assignment operator. Unlike a normal variable or\nparameter assignment, port connections are symmetric:\nA.port1 =\nB.port2\nhas the same meaning as\nB.port2 = A.port1\n. The notion of\nmaster and slave ports exists in the Python objects as well, and a check\nis done when the ports are connected together.\nObjects such as busses that have a potentially unlimited number of ports\nuse \u201cvector ports\u201d. An assignment to a vector port appends the peer to a\nlist of connections rather than overwriting a previous connection.\nIn C++, memory ports are connected together by the python code after all\nobjects are instantiated.\nRequest\nA request object encapsulates the original request issued by a CPU or\nI/O device. The parameters of this request are persistent throughout the\ntransaction, so a request object\u2019s fields are intended to be written at\nmost once for a given request. There are a handful of constructors and\nupdate methods that allow subsets of the object\u2019s fields to be written\nat different times (or not at all). Read access to all request fields is\nprovided via accessor methods which verify that the data in the field\nbeing read is valid.\nThe fields in the request object are typically not available to devices\nin a real system, so they should normally be used only for statistics or\ndebugging and not as architectural values.\nRequest object fields include:\nVirtual address. This field may be invalid if the request was issued\ndirectly on a physical address (e.g., by a DMA I/O device).\nPhysical address.\nData size.\nTime the request was created.\nThe ID of the CPU/thread that caused this request. May be invalid if\nthe request was not issued by a CPU (e.g., a device access or a\ncache writeback).\nThe PC that caused this request. Also may be invalid if the request\nwas not issued by a CPU.\nPacket\nA Packet is used to encapsulate a transfer between two objects in the\nmemory system (e.g., the L1 and L2 cache). This is in contrast to a\nRequest where a single Request travels all the way from the requester to\nthe ultimate destination and back, possibly being conveyed by several\ndifferent Packets along the way.\nRead access to many packet fields is provided via accessor methods which\nverify that the data in the field being read is valid.\nA packet contains the following all of which are accessed by accessors\nto be certain the data is valid:\nThe address. This is the address that will be used to route the\npacket to its target (if the destination is not explicitly set) and\nto process the packet at the target. It is typically derived from\nthe request object\u2019s physical address, but may be derived from the\nvirtual address in some situations (e.g., for accessing a fully\nvirtual cache before address translation has been performed). It may\nnot be identical to the original request address: for example, on a\ncache miss, the packet address may be the address of the block to\nfetch and not the request address.\nThe size. Again, this size may not be the same as that of the\noriginal request, as in the cache miss scenario.\nA pointer to the data being manipulated.\nSet by\ndataStatic()\n,\ndataDynamic()\n, and\ndataDynamicArray()\nwhich control if the data associated with the packet is freed\nwhen the packet is, not, with\ndelete\n, and with\ndelete []\nrespectively.\nAllocated if not set by one of the above methods\nallocate()\nand the data is freed when the packet is destroyed. (Always safe\nto call).\nA pointer can be retrived by calling\ngetPtr()\nget()\nand\nset()\ncan be used to manipulate the data in the\npacket. The get() method does a guest-to-host endian conversion\nand the set method does a host-to-guest endian conversion.\nA status indicating Success, BadAddress, Not Acknowleged, and\nUnknown.\nA list of command attributes associated with the packet\nNote: There is some overlap in the data in the status field and\nthe command attributes. This is largely so that a packet can be\neasily reinitialized when nacked or easily reused with atomic or\nfunctional accesses.\nA\nSenderState\npointer which is a virtual base opaque structure\nused to hold state associated with the packet but specific to the\nsending device (e.g., an MSHR). A pointer to this state is returned\nin the packet\u2019s response so that the sender can quickly look up the\nstate needed to process it. A specific subclass would be derived\nfrom this to carry state specific to a particular sending device.\nA\nCoherenceState\npointer which is a virtual base opaque structure\nused to hold coherence-related state. A specific subclass would be\nderived from this to carry state specific to a particular coherence\nprotocol.\nA pointer to the request.\nAccess Types\nThere are three types of accesses supported by the ports.\nTiming\n- Timing accesses are the most detailed access. They\nreflect our best effort for realistic timing and include the\nmodeling of queuing delay and resource contention. Once a timing\nrequest is successfully sent at some point in the future the device\nthat sent the request will either get the response or a NACK if the\nrequest could not be completed (more below). Timing and Atomic\naccesses can not coexist in the memory system.\nAtomic\n- Atomic accesses are a faster than detailed access. They\nare used for fast forwarding and warming up caches and return an\napproximate time to complete the request without any resource\ncontention or queuing delay. When a atomic access is sent the\nresponse is provided when the function returns. Atomic and timing\naccesses can not coexist in the memory system.\nFunctional\n- Like atomic accesses functional accesses happen\ninstantaneously, but unlike atomic accesses they can coexist in the\nmemory system with atomic or timing accesses. Functional accesses\nare used for things such as loading binaries, examining/changing\nvariables in the simulated system, and allowing a remote debugger to\nbe attached to the simulator. The important note is when a\nfunctional access is received by a device, if it contains a queue of\npackets all the packets must be searched for requests or responses\nthat the functional access is effecting and they must be updated as\nappropriate. The\nPacket::intersect()\nand\nfixPacket()\nmethods can\nhelp with this.\nPacket allocation protocol\nThe protocol for allocation and deallocation of Packet objects varies\ndepending on the access type. (We\u2019re talking about low-level C++\nnew\n/\ndelete\nissues here, not anything related to the coherence\nprotocol.)\nAtomic\nand\nFunctional\n: The Packet object is owned by the\nrequester. The responder must overwrite the request packet with the\nresponse (typically using the\nPacket::makeResponse()\nmethod).\nThere is no provision for having multiple responders to a single\nrequest. Since the response is always generated before\nsendAtomic()\nor\nsendFunctional()\nreturns, the requester can\nallocate the Packet object statically or on the stack.\nTiming\n: Timing transactions are composed of two one-way messages,\na request and a response. In both cases, the Packet object must be\ndynamically allocated by the sender. Deallocation is the\nresponsibility of the receiver (or, for broadcast coherence packets,\nthe target device, typically memory). In the case where the receiver\nof a request is generating a response, it\nmay\nchoose to reuse the\nrequest packet for its response to save the overhead of calling\ndelete\nand then\nnew\n(and gain the convenience of using\nmakeResponse()\n). However, this optimization is optional, and the\nrequester must not rely on receiving the same Packet object back in\nresponse to a request. Note that when the responder is not the\ntarget device (as in a cache-to-cache transfer), then the target\ndevice will still delete the request packet, and thus the responding\ncache must allocate a new Packet object for its response. Also,\nbecause the target device may delete the request packet immediately\non delivery, any other memory device wishing to reference a\nbroadcast packet past the point where the packet is delivered must make\na copy of that packet, as the pointer to the packet that is\ndelivered cannot be relied upon to stay valid.\nTiming Flow control\nTiming requests simulate a real memory system, so unlike functional and\natomic accesses their response is not instantaneous. Because the timing\nrequests are not instantaneous, flow control is needed. When a timing\npacket is sent via\nsendTiming()\nthe packet may or may not be accepted,\nwhich is signaled by returning true or false. If false is returned the\nobject should not attempt to sent anymore packets until it receives a\nrecvRetry()\ncall. At this time it should again try to call\nsendTiming()\n; however the packet may again be rejected. Note: The\noriginal packet does not need to be resent, a higher priority packet can\nbe sent instead. Once\nsendTiming()\nreturns true, the packet may still\nnot be able to make it to its destination. For packets that require a\nresponse (i.e.\npkt->needsResponse()\nis true), any memory object can\nrefuse to acknowledge the packet by changing its result to\nNacked\nand\nsending it back to its source. However, if it is a response packet, this\ncan not be done. The true/false return is intended to be used for local\nflow control, while nacking is for global flow control. In both cases a\nresponse can not be nacked.\nResponse and Snoop ranges\nRanges in the memory system are handled by having devices that are\nsensitive to an address range provide an implementation for\ngetAddrRanges\nin their slave port objects. This method returns an\nAddrRangeList\nof addresses it responds to. When these ranges change\n(e.g. from PCI configuration taking place) the device should call\nsendRangeChange()\non its slave port so that the new ranges are\npropagated to the entire hierarchy. This is precisely what happens\nduring\ninit()\n; all memory objects call\nsendRangeChange()\n, and a\nflurry of range updates occur until everyones ranges have been\npropagated to all busses in the system.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/memory_system/",
            "page_title": "Memory system",
            "parent_section": "Overview",
            "section_heading": "memory_system"
        }
    },
    {
        "text": "authors:\nJason Lowe-Power\nlast edited:\n2025-08-21 21:40:17 +0000\nClassic Memory System coherence\nM5 2.0b4 introduced a substantially rewritten and streamlined cache\nmodel, including a new coherence protocol. (The old pre-2.0 cache model\nhad been patched up to work with the new\nMemory\nSystem\nintroduced in 2.0beta, but not\nrewritten to take advantage of the new memory system\u2019s features.)\nThe key feature of the new coherence protocol is that it is designed to\nwork with more-or-less arbitrary cache hierarchies (multiple caches each\non multiple levels). In contrast, the old protocol restricted sharing to\na single bus.\nIn the real world, a system architecture will have limits on the number\nor configuration of caches that the protocol can be designed to\naccommodate. It\u2019s not practical to design a protocol that\u2019s fully\nrealistic and yet efficient for arbitrary configurations. In order to\nenable our protocol to work on (nearly) arbitrary configurations, we\ncurrently sacrifice a little bit of realism and a little bit of\nconfigurability. Our intent is that this protocol is adequate for\nresearchers studying aspects of system behavior other than coherence\nmechanisms. Researchers studying coherence specifically will probably\nwant to replace the default coherence mechanism with implementations of\nthe specific protocols under investigation.\nThe protocol is a MOESI snooping protocol. Inclusion is\nnot\nenforced; in a CMP configuration where you have several L1s whose total\ncapacity is a significant fraction of the capacity of the common L2 they\nshare, inclusion can be very inefficient.\nRequests from upper-level caches (those closer to the CPUs) propagate\ntoward memory in the expected fashion: an L1 miss is broadcast on the\nlocal L1/L2 bus, where it is snooped by the other L1s on that bus and\n(if none respond) serviced by the L2. If the request misses in the L2,\nthen after some delay (currently set equal to the L2 hit latency), the\nL2 will issue the request on its memory-side bus, where it will possibly\nbe snooped by other L2s and then be issued to an L3 or memory.\nUnfortunately, propagating snoop requests incrementally back up the\nhierarchy in a similar fashion is a source of myriad nearly intractable\nrace conditions. Real systems don\u2019t typically do this anyway; in general\nyou want a single snoop operation at the L2 bus to tell you the state of\nthe block in the whole L1/L2 hierarchy. There are a handful of methods\nfor this:\njust snoop the L2, but enforce inclusion so that the L2 has all the\ninfo you need about the L1s as well\u2014an idea we\u2019ve already rejected\nabove\nkeep an extra set of tags for all the L1s at the L2 so those can be\nsnooped at the same time (see the Compaq Piranha)\u2014reasonable, if\nyou\u2019re hierarchy\u2019s not too deep, but now you\u2019ve got to size the tags\nin the lower-level caches based on the number, size, and\nconfiguration of the upper-level caches, which is a configuration\npain\nsnoop the L1s in parallel with the L2, something that\u2019s not hard if\nthey\u2019re all on the same die (I believe Intel started doing this with\nthe Pentium Pro; not sure if they still do with the Core2 chips or\nnot, or if AMD does this as well, but I suspect so)\u2014also\nreasonable, but adding explicit paths for these snoops would also\nmake for a very cumbersome configuration process\nWe solve this dilemma by introducing \u201cexpress snoops\u201d, which are special\nsnoop requests that get propagated up the hierarchy instantaneously and\natomically (much like the atomic-mode accesses described on the\nMemory\nSystem\npage), even when the system is running\nin timing mode. Functionally this behaves very much like options 2 or 3\nabove, but because the snoops propagate along the regular bus\ninterconnects, there\u2019s no additional configuration overhead. There is\nsome timing inaccuracy introduced, but if we assume that there are\ndedicated paths in the real hardware for these snoops (or for\nmaintaining the additional copies of the upper-level tags at the\nlower-level caches) then the differences are probably minor.\n(More to come: how does a cache know when its request is completed? and\nother fascinating questions\u2026)\nNote: there are still some bugs in this protocol as of 2.0b4,\nparticularly if you have multiple L2s each with multiple L1s behind it,\nbut I believe it works for any configuration that worked in 2.0b3.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/memory_system/classic-coherence-protocol",
            "page_title": "Classic Memory System coherence",
            "parent_section": "Overview",
            "section_heading": "classic-coherence-protocol"
        }
    },
    {
        "text": "authors:\nJason Lowe-Power\nlast edited:\n2025-08-21 21:40:17 +0000\nClassic Caches\nThe default cache is a non-blocking cache with MSHR (miss status holding\nregister) and WB (Write Buffer) for read and write misses. The Cache can\nalso be enabled with prefetch (typically in the last level of cache).\nThere are multiple possible\nreplacement policies\nand\nindexing\npolicies\nimplemented in gem5. These define, respectively, the possible\nblocks that can be used for a block replacement given an address, and\nhow to use the address information to find a block's location. By\ndefault the cache lines are replaced using\nLRU (least recently used)\n,\nand indexed with the\nSet Associative\npolicy.\nInterconnects\nCrossbars\nThe two types of traffic in the crossbar are memory-mapped packets and\nsnooping packets. The memory-mapped requests go down the memory\nhierarchy, and responses go up the memory hierarchy (same route back).\nThe snooping requests go horizontally and up the cache hierarchy,\nsnooping responses go horizontally and down the hierarchy (same route\nback). Normal snoops go horizontally and express snoops go up the cache\nhierarchy.\nBridges\nOthers\u2026\nDebugging\nThere is a feature in the classic memory system for displaying the coherence state of a particular block from within the debugger (e.g., gdb). This feature is built on the classic memory system\u2019s support for functional accesses. (Note that this feature is currently rarely used and may have bugs.)\nIf you inject a functional request with the command set to PrintReq, the packet traverses the memory system (like a regular functional request) but on any object that matches (other queued packet, cache block, etc.) it simply prints out some information about that object.\nThere\u2019s a helper method on Port called printAddr() that takes an address and builds an appropriate PrintReq packet and injects it. Since it propagates using the same mechanism as a normal functional request, it needs to be injected from a port where it will propagate through the whole memory system, such as at a CPU. There are helper printAddr() methods on MemTest, AtomicSimpleCPU, and TimingSimpleCPU objects that simply call printAddr() on their respective cache ports. (Caveat: the latter two are untested.)\nPutting it all together, you can do this:\n(gdb) set print object\n(gdb) call SimObject::find(\" system.physmem.cache0.cache0.cpu\")\n$4 = (MemTest *) 0xf1ac60\n(gdb) p (MemTest*)$4\n$5 = (MemTest *) 0xf1ac60\n(gdb) call $5->printAddr(0x107f40)\n\nsystem.physmem.cache0.cache0\n  MSHRs\n    [107f40:107f7f] Fill   state:\n      Targets:\n        cpu: [107f40:107f40] ReadReq\nsystem.physmem.cache1.cache1\n  blk VEM\nsystem.physmem\n  0xd0\n\u2026 which says that cache0.cache0 has an MSHR allocated for that address to serve a target ReadReq from the CPU, but it\u2019s not in service yet (else it would be marked as such); the block is valid, exclusive, and modified in cache1.cache1, and the byte has a value of 0xd0 in physical memory.\nObviously it\u2019s not necessarily all the info you\u2019d want, but it\u2019s pretty useful. Feel free to extend. There\u2019s also a verbosity parameter that\u2019s currently not used that could be exploited to have different levels of output.\nNote that the extra \u201cp (MemTest*)$4\u201d is needed since although \u201cset print object\u201d displays the derived type, internally gdb still considers the pointer to be of the base type, so if you try and call printAddr directly on the $4 pointer you get this:\n(gdb) call $4->printAddr(0x400000)\nCouldn't find method SimObject::printAddr",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/memory_system/classic_caches",
            "page_title": "Classic Caches",
            "parent_section": "Overview",
            "section_heading": "classic_caches"
        }
    },
    {
        "text": "Model Hierarchy\nModel that is used in this document consists of two out-of-order (O3) ARM v7\nCPUs with corresponding L1 data caches and Simple Memory. It is created by\nrunning gem5 with the following parameters:\nconfigs/example/fs.py \u2013-caches \u2013-cpu-type=arm_detailed \u2013-num-cpus=2\nGem5 uses Simulation Objects derived objects as basic blocks for building\nmemory system. They are connected via ports with established master/slave\nhierarchy. Data flow is initiated on master port while the response messages\nand snoop queries appear on the slave port.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/memory_system/gem5_memory_system",
            "page_title": "The gem5 Memory System",
            "parent_section": "Model Hierarchy",
            "section_heading": "Overview"
        }
    },
    {
        "text": "CPU\nData\nCache\nobject\nimplements a standard cache structure:\n\nIt is not in the scope of this document to describe O3 CPU model in details, so\nhere are only a few relevant notes about the model:\nRead access\nis initiated by sending message to the port towards DCache\nobject. If DCache rejects the message (for being blocked or busy) CPU will\nflush the pipeline and the access will be re-attempted later on. The access is\ncompleted upon receiving reply message (ReadRep) from DCache.\nWrite access\nis initiated by storing the request into store buffer whose\ncontext is emptied and sent to DCache on every tick. DCache may also reject the\nrequest. Write access is completed when write reply (WriteRep) message is\nreceived from DCache.\nLoad & store buffers (for read and write access) don\u2019t impose any restriction\non the number of active memory accesses. Therefore, the maximum number of\noutstanding CPU\u2019s memory access requests is not limited by CPU Simulation\nObject but by underlying memory system model.\nSplit memory access\nis implemented.\nThe message that is sent by CPU contains memory type (Normal, Device, Strongly\nOrdered and cachebility) of the accessed region. However, this is not being\nused by the rest of the model that takes more simplified approach towards\nmemory types.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/memory_system/gem5_memory_system",
            "page_title": "The gem5 Memory System",
            "parent_section": "CPU",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Section: Data Cache Object\n\nSub-section: Tags & Data Block\n\nCache\nlines (referred as\nblocks in source code) are organised into sets with configurable associativity\nand size. They have the following status flags:\nValid\n. It holds data. Address tag is valid\nRead\n. No read request will be accepted without this flag being set. For\nexample, cache line is valid and unreadable when it waits for write flag to\ncomplete write access.\nWrite\n. It may accept writes. Cache line with Write flags identifies\nUnique state \u2013 no other cache memory holds the copy.\nDirty\n. It needs Writeback when evicted.\nRead access will hit cache line if address tags match and Valid and Read flags\nare set. Write access will hit cache line if address tags match and Valid, Read\nand Write flags are set.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/memory_system/gem5_memory_system",
            "page_title": "The gem5 Memory System",
            "parent_section": "Data Cache Object",
            "section_heading": "Tags & Data Block"
        }
    },
    {
        "text": "Section: Data Cache Object\n\nSub-section: MSHR and Write Buffer Queues\n\nMiss Status and Handling Register (\nMSHR\n) queue holds the list of\nCPU\u2019s outstanding memory requests that require read access to lower memory\nlevel. They are:\nCached Read misses.\nCached Write misses.\nUncached reads.\nWriteBuffer queue holds the following memory requests:\nUncached writes.\nWriteback from evicted (& dirty) cache lines.\n\nEach memory request is assigned to corresponding\nMSHR\nobject (READ or WRITE on\ndiagram above) that represents particular block (cache line) of memory that has\nto be read or written in order to complete the command(s). As shown on gigure\nabove, cached read/writes against the same cache line have a common\nMSHR\nobject and will be\ncompleted with a single memory access.\nThe size of the block (and therefore the size of read/write access to lower\nmemory) is:\nThe size of cache line for cached access & writeback;\nAs specified in CPU instruction for uncached access.\nIn general, Data\nCache\nmodel distinguishes between just two memory types:\nNormal Cached memory. It is always treated as write back, read and write\nallocate.\nNormal uncached, Device and Strongly Ordered types are treated equally (as\nuncached memory)",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/memory_system/gem5_memory_system",
            "page_title": "The gem5 Memory System",
            "parent_section": "Data Cache Object",
            "section_heading": "MSHR and Write Buffer Queues"
        }
    },
    {
        "text": "Section: Data Cache Object\n\nSub-section: Memory Access Ordering\n\nAn unique order number is assigned to each CPU read/write request(as they\nappear on slave port). Order numbers of\nMSHR\nobjects are copied from the\nfirst assigned read/write.\nMemory read/writes from each of these two queues are executed in order\n(according to the assigned order number). When both queues are not empty the\nmodel will execute memory read from\nMSHR\nblock unless WriteBuffer is\nfull. It will, however, always preserve the order of read/writes on the same\n(or overlapping) memory cache line (block).\nIn summary:\nOrder of accesses to cached memory is not preserved unless they target the\nsame cache line. For example, the accesses #1, #5 & #10 will complete\nsimultaneously in the same tick (still in order). The access #5 will complete\nbefore #3.\nOrder of all uncached memory writes is preserved. Write#6 always completes\nbefore Write#13.\nOrder to all uncached memory reads is preserved. Read#2 always completes\nbefore Read#8.\nThe order of a read and a write uncached access is not necessarily preserved\nunless their access regions overlap. Therefore, Write#6 always completes before\nRead#8 (they target the same memory block). However, Write#13 may complete\nbefore Read#8.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/memory_system/gem5_memory_system",
            "page_title": "The gem5 Memory System",
            "parent_section": "Data Cache Object",
            "section_heading": "Memory Access Ordering"
        }
    },
    {
        "text": "Coherent Bus Object\n\nCoherent Bus object provides basic support for snoop protocol:\nAll requests on the slave port are forwarded to the appropriate master port.\nRequests for cached memory regions are also forwarded to other slave ports (as\nsnoop requests).\nMaster port replies are forwarded to the appropriate slave port.\nMaster port snoop requests are forwarded to all slave ports.\nSlave port snoop replies are forwarded to the port that was the source of the\nrequest. (Note that the source of snoop request can be either slave or master\nport.)\nThe bus declares itself blocked for a configurable period of time after any of\nthe following events:\nA packet is sent (or failed to be sent) to a slave port.\nA reply message is sent to a master port.\nSnoop response from one slave port is sent to another slave port.\nThe bus in blocked state rejects the following incoming messages:\nSlave port requests.\nMaster port replies.\nMaster port snoop requests.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/memory_system/gem5_memory_system",
            "page_title": "The gem5 Memory System",
            "parent_section": "Coherent Bus Object",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Simple Memory Object\nIt never blocks the access on slave port.\nMemory read/write takes immediate effect. (Read or write is performed when the\nrequest is received).\nReply message is sent after a configurable period of time .",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/memory_system/gem5_memory_system",
            "page_title": "The gem5 Memory System",
            "parent_section": "Simple Memory Object",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Section: Message Flow\n\nSub-section: Memory Access Ordering\n\nThe following diagram shows read access that hits Data Cache line with Valid\nand Read flags:\n\nCache miss read access will generate the following sequence of messages:\n\nNote that bus object never gets response from both DCache2 and Memory object.\nIt sends the very same ReadReq package (message) object to memory and data\ncache. When Data Cache wants to reply on snoop request it marks the message\nwith MEM_INHIBIT flag that tells Memory object not to process the message.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/memory_system/gem5_memory_system",
            "page_title": "The gem5 Memory System",
            "parent_section": "Message Flow",
            "section_heading": "Memory Access Ordering"
        }
    },
    {
        "text": "Section: Message Flow\n\nSub-section: Memory Access Ordering\n\nThe following diagram shows write access that hits DCache1 cache line with\nValid & Write flags:\n\nNext figure shows write access that hits DCache1 cache line with Valid but no\nWrite flags \u2013 which qualifies as write miss. DCache1 issues UpgradeReq to\nobtain write permission. DCache2::snoopTiming will invalidate cache line that\nhas been hit. Note that UpgradeResp message doesn\u2019t carry data.\n\nThe next diagram shows write miss in DCache. ReadExReq invalidates cache line\nin DCache2. ReadExResp carries the content of memory cache line.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/memory_system/gem5_memory_system",
            "page_title": "The gem5 Memory System",
            "parent_section": "Message Flow",
            "section_heading": "Memory Access Ordering"
        }
    },
    {
        "text": "Set Associative\nThe set associative indexing policy is the standard for table-like\nstructures, and can be further divided into Direct-Mapped (or 1-way\nset-associative), Set-Associative and Full-Associative (N-way\nset-associative, where N is the number of table entries).\nA set associative cache can be seen as a skewed associative cache whose\nskewing function maps to the same value for every way.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/memory_system/indexing_policies",
            "page_title": "Indexing Policies",
            "parent_section": "Set Associative",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Skewed Associative\nThe skewed associative indexing policy has a variable mapping based on a\nhash function, so a value x can be mapped to different sets, based on\nthe way being used. Gem5 implements skewed caches as described in\n\u201cSkewed-Associative\nCaches\u201d, from Seznec et al\n.\nNote that there are only a limited number of implemented hashing\nfunctions, so if the number of ways is higher than that number then a\nsub-optimal automatically generated hash function is used.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/memory_system/indexing_policies",
            "page_title": "Indexing Policies",
            "parent_section": "Skewed Associative",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Random\nThe simplest replacement policy; it does not need replacement data, as\nit randomly selects a victim among the candidates.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/memory_system/replacement_policies",
            "page_title": "Replacement Policies",
            "parent_section": "Random",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Least Recently Used (LRU)\nIts replacement data consists of a last touch timestamp, and the victim\nis chosen based on it: the oldest it is, the more likely its respective\nentry is to be victimized.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/memory_system/replacement_policies",
            "page_title": "Replacement Policies",
            "parent_section": "Least Recently Used (LRU)",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Tree Pseudo Least Recently Used (TreePLRU)\nA variation of the LRU that uses a binary tree to keep track of the\nrecency of use of the entries through 1-bit pointers.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/memory_system/replacement_policies",
            "page_title": "Replacement Policies",
            "parent_section": "Tree Pseudo Least Recently Used (TreePLRU)",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Bimodal Insertion Policy (BIP)\nThe\nBimodal Insertion Policy\nis similar to the LRU, however, blocks\nhave a probability of being inserted as the MRU, according to a bimodal\nthrottle parameter (btp). The highest btp is, the highest is the\nlikelihood of a new block being inserted as MRU.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/memory_system/replacement_policies",
            "page_title": "Replacement Policies",
            "parent_section": "Bimodal Insertion Policy (BIP)",
            "section_heading": "Overview"
        }
    },
    {
        "text": "LRU Insertion Policy (LIP)\nThe\nLRU Insertion Policy\nconsists of a LRU\nreplacement policy that instead of inserting blocks with the most recent\nlast touch timestamp, it inserts them as the LRU entry. On subsequent\ntouches to the block, its timestamp is updated to be the MRU, as in LRU.\nIt can also be seen as a BIP where the likelihood of inserting a new\nblock as the most recently used is 0%.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/memory_system/replacement_policies",
            "page_title": "Replacement Policies",
            "parent_section": "LRU Insertion Policy (LIP)",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Most Recently Used (MRU)\nThe Most Recently Used policy chooses replacement victims by their\nrecency, however, as opposed to LRU, the newer the entry is, the more\nlikely it is to be victimized.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/memory_system/replacement_policies",
            "page_title": "Replacement Policies",
            "parent_section": "Most Recently Used (MRU)",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Least Frequently Used (LFU)\nThe victim is chosen using the reference frequency. The least referenced\nentry is chosen to be evicted, regardless of the amount of times it has\nbeen touched, or how much time has passed since its last touch.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/memory_system/replacement_policies",
            "page_title": "Replacement Policies",
            "parent_section": "Least Frequently Used (LFU)",
            "section_heading": "Overview"
        }
    },
    {
        "text": "First-In, First-Out (FIFO)\nThe victim is chosen using the insertion timestamp. If no invalid\nentries exist, the oldest one is victimized, regardless of the amount of\ntimes it has been touched.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/memory_system/replacement_policies",
            "page_title": "Replacement Policies",
            "parent_section": "First-In, First-Out (FIFO)",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Second-Chance\nThe\nSecond-Chance\nreplacement policy is similar to FIFO, however\nentries are given a second chance before being victimized. If an entry\nwould have been the next to be victimized, but its second chance bit is\nset, this bit is cleared, and the entry is re-inserted at the end of the\nFIFO. Following a miss, an entry is inserted with its second chance bit\ncleared.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/memory_system/replacement_policies",
            "page_title": "Replacement Policies",
            "parent_section": "Second-Chance",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Not Recently Used (NRU)\nNot Recently Used (NRU) is an approximation of LRU that uses a single\nbit to determine if a block is going to be re-referenced in the near or\ndistant future. If the bit is 1, it is likely to not be referenced soon,\nso it is chosen as the replacement victim. When a block is victimized,\nall its co-replacement candidates have their re-reference bit\nincremented.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/memory_system/replacement_policies",
            "page_title": "Replacement Policies",
            "parent_section": "Not Recently Used (NRU)",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Re-Reference Interval Prediction (RRIP)\nRe-Reference Interval Prediction (RRIP)\nis an extension of NRU that\nuses a re-reference prediction value to determine if blocks are going to\nbe re-used in the near future or not. The higher the value of the RRPV,\nthe more distant the block is from its next access. From the original\npaper, this implementation of RRIP is also called Static RRIP (SRRIP),\nas it always inserts blocks with the same RRPV.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/memory_system/replacement_policies",
            "page_title": "Replacement Policies",
            "parent_section": "Re-Reference Interval Prediction (RRIP)",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Bimodal Re-Reference Interval Prediction (BRRIP)\nBimodal Re-Reference Interval Prediction\n(BRRIP)\nis an extension of\nRRIP that has a probability of not inserting blocks as the LRU, as in\nthe Bimodal Insertion Policy. This probability is controlled by the\nbimodal throtle parameter (btp).",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/memory_system/replacement_policies",
            "page_title": "Replacement Policies",
            "parent_section": "Bimodal Re-Reference Interval Prediction (BRRIP)",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Life of a memory request in Ruby\nIn this section we will provide a high level overview of how a memory\nrequest is serviced by Ruby as a whole and what components in Ruby it\ngoes through. For detailed operations within each components though,\nrefer to previous sections describing each component in isolation.\nA memory request from a core or hardware context of gem5 enters the\njurisdiction of Ruby through the\nRubyPort::recvTiming\ninterface (in src/mem/ruby/system/RubyPort.hh/cc). The number of\nRubyport instantiation in the simulated system is equal to the\nnumber of hardware thread context or cores (in case of\nnon-multithreaded\ncores). A port from the side of each core is\ntied to a corresponding RubyPort.\nThe memory request arrives as a gem5 packet and RubyPort is\nresponsible for converting it to a RubyRequest object that is\nunderstood by various components of Ruby. It also finds out if the\nrequest is for some PIO or not and maneuvers the packet to correct\nPIO. Finally once it has generated the corresponding RubyRequest\nobject and ascertained that the request is a\nnormal\nmemory request\n(not PIO access), it passes the request to the\nSequencer::makeRequest\ninterface of the attached Sequencer\nobject with the port (variable\nruby_port\nholds the pointer to\nit). Observe that Sequencer class itself is a derived class from the\nRubyPort class.\nAs mentioned in the section describing Sequencer class of Ruby,\nthere are as many objects of Sequencer in a simulated system as the\nnumber of hardware thread context (which is also equal to the number\nof RubyPort object in the system) and there is an one-to-one mapping\nbetween the Sequencer objects and the hardware thread context. Once\na memory request arrives at the\nSequencer::makeRequest\n, it\ndoes various accounting and resource allocation for the request and\nfinally pushes the request to the Ruby\u2019s coherent cache hierarchy\nfor satisfying the request while accounting for the delay in\nservicing the same. The request is pushed to the Cache hierarchy by\nenqueueing the request to the\nmandatory queue\nafter accounting for\nL1 cache access latency. The\nmandatory queue\n(variable name\nm_mandatory_q_ptr\n) effectively acts as the interface between\nthe Sequencer and the SLICC generated cache coherence files.\nL1 cache controllers (generated by SLICC according to the coherence\nprotocol specifications) dequeues request from the\nmandatory queue\nand looks up the cache, makes necessary coherence state transitions\nand/or pushes the request to the next level of cache hierarchy as\nper the requirements. Different controller and components of SLICC\ngenerated Ruby code communicates among themselves through\ninstantiations of\nMessageBuffer\nclass of Ruby\n(src/mem/ruby/buffers/MessageBuffer.cc/hh) , which can act as\nordered or unordered buffer or queues. Also the delays in servicing\ndifferent steps for satisfying a memory request gets accounted for\nscheduling enqueue-ing and dequeue-ing operations accordingly. If\nthe requested cache block may be found in L1 caches and with\nrequired coherence permissions then the request is satisfied and\nimmediately returned. Otherwise the request is pushed to the next\nlevel of cache hierarchy through\nMessageBuffer\n. A request can go\nall the way up to the Ruby\u2019s Memory Controller (also called\nDirectory in many protocols). Once the request get satisfied it is\npushed upwards in the hierarchy through\nMessageBuffer\ns.\nThe\nMessageBuffers\nalso act as entry point of coherence messages\nto the on-chip interconnect modeled. The MesageBuffers are connected\naccording to the interconnect topology specified. The coherence\nmessages thus travel through this on-chip interconnect accordingly.\nOnce the requested cache block is available at L1 cache with desired\ncoherence permissions, the L1 cache controller informs the\ncorresponding Sequencer object by calling its\nreadCallback\nor\n\u2018writeCallback\n\u2019\u2019 method depending upon the type of the request.\nNote that by the time these methods on Sequencer are called the\nlatency of servicing the request has been implicitly accounted for.\nThe Sequencer then clears up the accounting information for the\ncorresponding request and then calls the\nRubyPort::ruby_hit_callback\nmethod. This ultimately returns\nthe result of the request to the corresponding port of the core/\nhardware context of the frontend (gem5).",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/ruby",
            "page_title": "Ruby",
            "parent_section": "Life of a memory request in Ruby",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Directory Structure\nsrc/mem/\nprotocols\n: SLICC specification for coherence protocols\nslicc\n: implementation for SLICC parser and code generator\nruby\ncommon\n: frequently used data structures, e.g. Address\n(with bit-manipulation methods), histogram, data block\nfilters\n: various Bloom filters (stale code from GEMS)\nnetwork\n: Interconnect implementation, sample topology\nspecification, network power calculations, message buffers\nused for connecting controllers\nprofiler\n: Profiling for cache events, memory controller\nevents\nrecorder\n: Cache warmup and access trace recording\nslicc_interface\n: Message data structure, various\nmappings (e.g. address to directory node), utility functions\n(e.g. conversion between address & int, convert address to\ncache line address)\nstructures\n: Protocol independent memory components \u2013\nCacheMemory, DirectoryMemory\nsystem\n: Glue components \u2013 Sequencer, RubyPort,\nRubySystem",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/ruby",
            "page_title": "Ruby",
            "parent_section": "Directory Structure",
            "section_heading": "Overview"
        }
    },
    {
        "text": "CHI overview and terminology\nCHI (Coherent Hub Interface) provides a component architecture and transaction-level specification to model MESI and MOESI cache coherency. CHI defines three main components as shown in the figure below:\n\nthe request node initiates transactions and sends requests towards memory. A request node can be a\nfully coherent request node (\nRNF\n)\n, meaning the request node caches data locally and should respond to snoop requests.\nthe interconnect (ICN) which is the responder for request nodes. At protocol level the interconnect is a component encapsulating the\nfully coherent home nodes (\nHNF\n)\nof the system.\nthe\nslave nodes (\nSNF\n)\n, which interface with the memory controllers.\nAn HNF is the point of coherency (PoC) and point of serialization (PoS) for a specific address range. The HNF is responsible for issuing any required snoop requests to RNFs or memory access requests to SNFs in order to complete a transaction. The HNF can also encapsulate a shared last-level cache and include a directory for targeted snoops.\nThe\nCHI specification\nalso defines specific types of nodes for non-coherent requesters (RNI) and non-coherent address ranges (HNI and SNI), e.g., memory ranges belonging to IO components. In Ruby, IO accesses don\u2019t go though the cache coherency protocol so only CHI\u2019s fully coherent node types are implemented. In this documentation we interchangeably use the terms RN / RNF, HN / HNF, and SN/SNF. We also use the terms\nupstream\nand\ndownstream\nto refer to components in the previous (i.e. towards the cpu) and next  (i.e. towards memory) levels in the memory hierarchy, respectively.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/ruby/CHI",
            "page_title": "CHI",
            "parent_section": "CHI overview and terminology",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Protocol overview\nThe CHI protocol implementation consists mainly of two controllers:\nMemory_Controller\n(\nsrc/mem/ruby/protocol/chi/CHI-mem.sm\n) implements a CHI slave node. It receives memory read or write requests from the home nodes and interfaces with gem5\u2019s classic memory controllers.\nCache_Controller\n(\nsrc/mem/ruby/protocol/chi/CHI-cache.sm\n) generic cache controller state machine.\nIn order to allow fully flexible cache hierarchies,\nCache_Controller\ncan be configured to model any cache level (e.g. L1D, priv. L2, shared L3) within both request and home nodes. Furthermore it also supports multiple features not available in other Ruby protocols:\nconfigurable cache block allocation and deallocation policies for each request type.\nunified or separate transaction buffers for incoming and outgoing requests.\nMESI or MOESI operation.\ndirectory and cache tag and data array stalls.\nparameters to inject latency in multiple steps of the request handling flow. This allows us to more closely calibrate the performance.\nThe implementation defines the following cache states:\nI\n: line is invalid\nSC\n: line is shared and clean\nUC\n: line is exclusive/unique and clean\nSD\n: line is shared and dirty\nUD\n: line exclusive/unique and dirty\nUD_T\n:\nUD\nwith timeout. When a store conditional fails and causes the line to transition from I to UD, we transition to\nUD_T\ninstead if the number of failures is above a certain threshold (configuration defined). In\nUD_T\nthe line cannot be evicted from the requester for a given number of cycles (also configuration defined); after which the lines goes to UD. This is necessary to avoid livelocks in certain scenarios.\nThe figure below gives an overview of the state transitions when the controller is configured as a L1 cache:\n\nTransitions are annotated with the incoming request from the cpu (or generated internally, e.g.\nReplacements\n) and the resulting outgoing request sent downstream. For simplicity, the figure omits requests that do not change states (e.g., cache hits) and invalidating snoops (final state is always\nI\n). For simplicity, it also shows only the typical state transitions in a MOESI protocol. In CHI the final state will ultimately be determined by the type of data returned by the responder (e.g., requester may receive\nUD\nor\nUC\ndata in  response to a\nReadShared\n).\nThe figures below show the transition for a\nintermediate-level\ncache controller (e.g., priv. L2, shared L3, HNF, etc):\n\n\nAs in the previous case, cache hits are omitted for simplicity. In addition to the cache states, the following directory states are defined to track lines present in an upstream cache:\nRU\n:an upstream requester has line in UC or UD\nRSC\n: one or more upstream requesters have line in SC\nRSD\n: one upstream requester has line in SD; others may have it in SC\nRUSC\n:\nRSC\n+ current domain stills has exclusive access\nRUSD\n:\nRSD\n+ current domain stills has exclusive access\nWhen the line is present both in the local cache and upstream caches the following combined states are possible:\nUD_RSC\n,\nSD_RSC\n,\nUC_RSC\n,\nSC_RSC\nUD_RU\n,\nUC_RU\nUD_RSD\n,\nSD_RSD\nThe\nRUSC\nand\nRUSD\nstates (omitted in the figures above) are used to keep track of lines for which the controller still has exclusive access permissions without having it in it\u2019s local cache. This is possible in a non-inclusive cache where a local block can be deallocated without back-invalidating upstream copies.\nWhen a cache controller is a HNF (home node), the state transactions are basically the same as a intermediate level cache, except for these differences:\nA\nReadNoSnp\nis sent to obtain data from downstream, as the only downstream components are the SNs (slave nodes).\nOn a cache and directory miss, DMT (direct memory transfer) is used if enabled.\nOn a cache miss and directory hit, DCT (direct cache transfer) is used if enabled.\nFor more information on DCT and DMT transactions, see Sections 1.7 and 2.3.1 in the\nCHI specification\n. DMT and DCT are CHI features that allow the data source for a request to send data directly to the original requester. On a DMT request, the SN sends data directly to the RN (instead of sending first to the HN, which would then forwards to the RN), while with DCT, the HN requests that a RN being snooped (the snoopee) to send a copy of the line directly the original requester. With DCT enabled, the HN may also request that the snoopee to send the data to both the HN and the original requester, so the HN can also cache the data. This depends on the allocation policy defined by the configuration parameters. Notice that the allocation policy also changes the cache state transitions. For simplicity, the figure above illustrates an inclusive cache.\nThe following is a list of the main configuration parameters of the cache controller that affect the protocol behavior (please refer to the protocol SLICC specification for details and a full list of parameters)\ndownstream_destinations\n: defines the destinations for requests sent downstream and is used to build the cache hierarchy. Refer to the\ncreate_system\nfunction in\nconfigs/ruby/CHI.py\nfor an example of how to setup a system with private L1I, L1D and L2 caches for each core.\nis_HN\n: Set when the controller is used as a home node and point of coherency for an address range. Must be false for every other cache level.\nenable_DMT\nand\nenable_DCT\n: when the controller is a home node, this enables direct memory transfers and direct cache transfers for incoming read requests.\nallow_SD\n: allow the shared dirty state. This switches between MOESI and MESI operation.\nalloc_on_readshared\n,\nalloc_on_readunique\n, and\nalloc_on_readonce\n: whether or not to allocate a cache block to store data used to respond to the corresponding read request.\nalloc_on_writeback\n: whether or not to allocate a cache block to store data received from a writeback request.\ndealloc_on_unique\nand\ndealloc_on_shared\n: deallocate the local cache block if the line becomes unique or shared in an upstream cache.\ndealloc_backinv_unique\nand\ndealloc_backinv_shared\n: if a local cache block is deallocated due to a replacement, also invalidates any unique or shared copy of the line in upstream caches.\nnumber_of_TBEs\n,\nnumber_of_snoop_TBEs\n, and\nnumber_of_repl_TBEs\n: number of entries in the TBE tables for incoming requests, incoming snoops, and replacements.\nunify_repl_TBEs\n: replacements use the same TBE slot as the request that triggered it. In this case\nnumber_of_repl_TBEs\nis ignored.\nThese parameters affect the cache controller performance:\nread_hit_latency\nand\nread_miss_latency\n: pipeline latencies for a read request thar hits or misses in the local cache, respectively.\nsnoop_latency\n: pipeline latency for an incoming snoop.\nwrite_fe_latency\nand\nwrite_be_latency\n: front-end and back-end pipeline latencies for handling write requests. Front-end latency is applied between sending the acknowledgement response and the next action to be taken. Back-end is applied to the requester between receiving the acknowledgement and sending the write data.\nallocation_latency\n: latency between TBE allocation and transaction initialization.\ncache\n:\nCacheMemory\nattached to this controller includes parameters such as size, associativity, tag and data latency, and number of banks.\nSection\nProtocol implementation\ngives an overview of the protocol implementation while Section\nSupported CHI transactions\ndescribe the implemented subset of the the AMBA 5 CHI spec. The next sections refer to specific files in the protocol source code and include SLICC snippets of the protocol. Some snippets where slightly simplified compared to the actual SLICC specification.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/ruby/CHI",
            "page_title": "CHI",
            "parent_section": "Protocol overview",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Section: Protocol implementation\n\nSub-section: Transaction allocation\n\nThe code snippet below shows how an incoming request in the\nreqIn\nport is handled. The\nreqIn\nport receives incoming messages from CHI\u2019s request channel:\nin_port(reqInPort, CHIRequestMsg, reqIn) {\n  if (reqInPort.isReady(clockEdge())) {\n    peek(reqInPort, CHIRequestMsg) {\n      if (in_msg.allowRetry) {\n        trigger(Event:AllocRequest, in_msg.addr, \n              getCacheEntry(in_msg.addr), getCurrentActiveTBE(in_msg.addr));\n      } else {\n        trigger(Event:AllocRequestWithCredit, in_msg.addr,\n              getCacheEntry(in_msg.addr), getCurrentActiveTBE(in_msg.addr));\n      }\n    }\n  }\n}\nThe\nallowRetry\nfield indicates messages that can be retried. Requests that cannot be retried are only sent by a requester that previously received credit (see\nRetryAck\nand\nPCrdGrant\nin the CHI specification). The transition triggered by\nEvent:AllocRequest\nor\nEvent:AllocRequestWithCredit\nexecutes a single action which either reserves space in the TBE table for the request and moves it to the\nreqRdy\nqueue, or sends a\nRetryAck\nmessage):\naction(AllocateTBE_Request) {\n  if (storTBEs.areNSlotsAvailable(1)) {\n    // reserve a slot for this request\n    storTBEs.incrementReserved();\n    // Move request to rdy queue\n    peek(reqInPort, CHIRequestMsg) {\n      enqueue(reqRdyOutPort, CHIRequestMsg, allocation_latency) {\n        out_msg := in_msg;\n      }\n    }\n  } else {\n    // we don't have resources to track this request; enqueue a retry\n    peek(reqInPort, CHIRequestMsg) {\n      enqueue(retryTriggerOutPort, RetryTriggerMsg, 0) {\n        out_msg.addr := in_msg.addr;\n        out_msg.event := Event:SendRetryAck;\n        out_msg.retryDest := in_msg.requestor;\n        retryQueue.emplace(in_msg.addr,in_msg.requestor);\n      }\n    }\n  }\n  reqInPort.dequeue(clockEdge());\n}\nNotice we don\u2019t create and send a\nRetryAck\nmessage directly from this action. Instead we create a separate trigger event in the internal\nretryTrigger\nqueue. This is necessary to prevent resource stalls from halting this action. Section\nPerformance modeling\nbelow explains resource stalls in more details.\nIncoming request from a\nSequencer\nobject (typically connected to a CPU when the controller is used as a L1 cache) and snoop requests arrive through the\nseqIn\nand\nsnpIn\nports and are handled similarly, except for:\nthey do not support retries. If there are no TBEs available, a resource stall is generated and we try again next cycle.\nsnoops allocate TBEs from a separate TBETable to avoid deadlocks.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/ruby/CHI",
            "page_title": "CHI",
            "parent_section": "Protocol implementation",
            "section_heading": "Transaction allocation"
        }
    },
    {
        "text": "Section: Protocol implementation\n\nSub-section: Transaction initialization\n\nOnce a request has been allocated a TBE and moved to the\nreqRdy\nqueue, an event is triggered to initiate the transaction. We trigger a different event for each different request type:\nin_port(reqRdyPort, CHIRequestMsg, reqRdy) {\n  if (reqRdyPort.isReady(clockEdge())) {\n    peek(reqRdyPort, CHIRequestMsg) {\n      CacheEntry cache_entry := getCacheEntry(in_msg.addr);\n      TBE tbe := getCurrentActiveTBE(in_msg.addr);\n      trigger(reqToEvent(in_msg.type), in_msg.addr, cache_entry, tbe);\n    }\n  }\n}\nEach request requires different initialization actions depending on the initial state of the line. To illustrate this processes, let\u2019s use as example a\nReadShared\nrequest for a line in the\nSC_RSC\nstate (shared\nclean in local cache and shared clean in an upstream cache):\ntransition(SC_RSC, ReadShared, BUSY_BLKD) {\n  Initiate_Request;\n  Initiate_ReadShared_Hit;\n  Profile_Hit;\n  Pop_ReqRdyQueue;\n  ProcessNextState;\n}\nInitiate_Request\ninitializes the allocated TBE. This actions copies any state and data allocated in the local cache and directory to the TBE.\nInitiate_ReadShared_Hit\nsets-up the set of actions that need to be executed to complete this specific request (see below).\nProfile_Hit\nupdates cache statistics.\nPop_ReqRdyQueue\nremoves request message form the\nreqRdy\nqueue.\nProcessNextState\nexecutes the next action defined by\nInitiate_ReadShared_Hit\n.\nInitiate_ReadShared_Hit\nis defined as follows:\naction(Initiate_ReadShared_Hit) {\n  tbe.actions.push(Event:TagArrayRead);\n  tbe.actions.push(Event:ReadHitPipe);\n  tbe.actions.push(Event:DataArrayRead);\n  tbe.actions.push(Event:SendCompData);\n  tbe.actions.push(Event:WaitCompAck);\n  tbe.actions.pushNB(Event:TagArrayWrite);\n}\ntbe.actions\nstores the list of events that need to be triggered in order to complete an action. In this particular case,\nTagArrayRead\n,\nReadHitPipe\n, and\nDataArrayRead\nintroduces delays to model the cache\ncontroller pipeline latency and reading the cache/directory tag array and cache data array (see Section\nPerformance modeling\n).\nSendCompData\nsets-up and sends the data responses for the\nReadShared\nrequest and\nWaitCompAck\nsets-up the TBE to expect the completion acknowledgement from the requester. Finally,\nTagArrayWrite\nintroduces the delay of updating the directory state to track the new sharer.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/ruby/CHI",
            "page_title": "CHI",
            "parent_section": "Protocol implementation",
            "section_heading": "Transaction initialization"
        }
    },
    {
        "text": "Section: Protocol implementation\n\nSub-section: Transaction execution\n\nAfter initialization, the line will transition to the\nBUSY_BLKD\nstate as show in\ntransition(SC_RSC, ReadShared, BUSY_BLKD)\n.\nBUSY_BLKD\nis a transient state indicating the line has now an outstanding transaction. In this state, the transaction is driven either by incoming response messages in the\nrspIn\nand\ndatIn\nports or trigger events defined in\ntbe.actions\n.\nThe\nProcessNextState\naction is responsible for checking\ntbe.actions\nand enqueuing trigger event messages into\nactionTriggers\nat the end of all transitions to the\nBUSY_BLKD\nstate.\nProcessNextState\nfirst checks for pending response messages. If there are no pending messages, it enqueues a message to\nactionTriggers\nin order to trigger the the event at the head of\ntbe.actions\n. If there are pending responses, then\nProcessNextState\ndoes nothing as the transaction will proceed once all expected responses are received.\nPending responses are tracked by the\nexpected_req_resp\nand\nexpected_snp_resp\nfields in the TBE. For instance, the\nExpectCompAck\naction, executed from the transition triggered by\nWaitCompAck\n, is defined as follows:\naction(ExpectCompAck) {\n  tbe.expected_req_resp.addExpectedRespType(CHIResponseType:CompAck);\n  tbe.expected_req_resp.addExpectedCount(1);\n}\nThis causes the transaction to wait until a\nCompAck\nresponse is received.\nSome actions can be allowed to execute when the transaction has pending responses. This actions are enqueued using\ntbe.actions.pushNB\n(i.e., push / non-blocking). In the example above\ntbe.actions.pushNB(Event:TagArrayWrite)\nmodels a tag write being performed while the transactions waits for the\nCompAck\nresponse.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/ruby/CHI",
            "page_title": "CHI",
            "parent_section": "Protocol implementation",
            "section_heading": "Transaction execution"
        }
    },
    {
        "text": "Section: Protocol implementation\n\nSub-section: Transaction finalization\n\nThe transaction ends when it has no more pending responses and\ntbe.actions\nis empty.\nProcessNextState\nchecks for this condition and enqueues a \u201cfinalizer\u201d trigger message into\nactionTriggers\n. When handling this event, the current cache line state and sharing/ownership information determines the final stable state of the line. Data and state information are updated in the cache and directory, if necessary, and the TBE is deallocated.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/ruby/CHI",
            "page_title": "CHI",
            "parent_section": "Protocol implementation",
            "section_heading": "Transaction finalization"
        }
    },
    {
        "text": "Section: Protocol implementation\n\nSub-section: Hazard handling\n\nEach controller allows only one active transaction per cache line. If a new request or snoop arrives while the cache line is in a transient state, this creates a hazard as defined in the CHI standard. We handle hazards as follows:\nRequest hazards:\na TBE is allocated as described previously, but the new transaction initialization is delayed until the current transaction finishes and the line is back to a stable state. This is done by moving\nthe request message from\nreqRdy\nto a separate\nstall buffer\n. All stalled messages are added back to\nreqRdy\nwhen the current transaction finishes and are handled in their original order of arrival.\nSnoop hazards:\nthe CHI spec does not allow snoops to be stalled by an existing request. If a transaction is waiting on a response for a request sent downstream (e.g. we sent a\nReadShared\nand are waiting for\nthe data response) we must accept and handle the snoop. The snoop can be stalled only if the request has already been accepted by the responder and is guaranteed to complete (e.g. a\nReadShared\nwith pending data but\nalready acked with a\nRespSepData\nresponse). To distinguish between these conditions we use the\nBUSY_INTR\ntransient state.\nBUSY_INTR\nindicates the transaction can be interrupted by a snoop. When a snoop arrives for a line in this state, a snoop TBE is allocated as described previously and its state is initialized based on the currently active TBE. The snoop TBE then becomes the currently active TBE. Any cache state and sharing/ownership changes caused by snoop are copied back to the original TBE before deallocating the snoop. When a snoop arrives for a line in\nBUSY_BLKD\nstate, we stall the snoop until the current transaction either finishes or transitions to\nBUSY_INTR\n.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/ruby/CHI",
            "page_title": "CHI",
            "parent_section": "Protocol implementation",
            "section_heading": "Hazard handling"
        }
    },
    {
        "text": "Section: Protocol implementation\n\nSub-section: Performance modeling\n\nAs described previously, the cache line state is known immediately when a transaction is initialized and the cache line can be read and written without any latency. This makes it easier to implement the functional\naspects of the protocol. To model timing we use explicit actions to introduce latency to a transaction. For example, in the\nReadShared\ncode snippet:\naction(Initiate_ReadShared_Hit) {\n  tbe.actions.push(Event:TagArrayRead);\n  tbe.actions.push(Event:ReadHitPipe);\n  tbe.actions.push(Event:DataArrayRead);\n  tbe.actions.push(Event:SendCompData);\n  tbe.actions.push(Event:WaitCompAck);\n  tbe.actions.pushNB(Event:TagArrayWrite);\n}\nTagArrayRead\n,\nReadHitPipe\n,\nDataArrayRead\n, and\nTagArrayWrite\ndon\u2019t have any functional significance. They are there to introduce latencies that would exist in a real cache controller pipeline, in this case: tag read latency, hit pipeline latency, data array read latency, and tag update latency. The latency introduced by these action is defined by configuration parameters.\nIn addition to explicitly added latencies. SLICC has the concept of\nresource stalls\nto model resource contention. Given a set of actions executed during a transition, the SLICC compiler automatically generates\ncode which checks if all resources needed by those actions are available. If any resource is unavailable, a resource stall is generated and the transition is not executed. A message that causes a resource stall remains in the input queue and the protocol attempts to trigger the transition again the next cycle.\nResources are detected by the SLICC compiler in different ways:\nImplicitly. This is the case for output ports. If an action enqueues new messages, the availability of the output port is automatically checked.\nAdding the\ncheck_allocate\nstatement to an action.\nAnnotating the transition with a resource type.\nWe use (2) to check availability of TBEs. See the snippet below:\naction(AllocateTBE_Snoop) {\n  // No retry for snoop requests; just create resource stall\n  check_allocate(storSnpTBEs);\n  ...\n}\nThis signals the SLICC compiler to check if the\nstorSnpTBEs\nstructure has a TBE slot available before executing any transition that includes the\nAllocateTBE_Snoop\naction.\nThe snippet below exemplifies (3):\ntransition({BUSY_INTR,BUSY_BLKD}, DataArrayWrite) {DataArrayWrite} {\n  ...\n}\nThe\nDataArrayWrite\nannotation signals the SLICC compiler to check for availability of the\nDataArrayWrite\nresource type.\nResource request types\nused in these annotations must be explicitly defined by the protocol, as well as how to check them. In our protocol we defined the following types to check for the availability of banks in the cache tag and data arrays:\nenumeration(RequestType) {\n  TagArrayRead;\n  TagArrayWrite;\n  DataArrayRead;\n  DataArrayWrite;\n}\n\nvoid recordRequestType(RequestType request_type, Addr addr) {\n  if (request_type == RequestType:DataArrayRead) {\n    cache.recordRequestType(CacheRequestType:DataArrayRead, addr);\n  }\n  ...\n}\n\nbool checkResourceAvailable(RequestType request_type, Addr addr) {\n  if (request_type == RequestType:DataArrayRead) {\n    return cache.checkResourceAvailable(CacheResourceType:DataArray, addr);\n  }\n  ...\n}\nThe implementation of\ncheckResourceAvailable\nand\nrecordRequestType\nare required by SLICC compiler when we use annotations on transactions.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/ruby/CHI",
            "page_title": "CHI",
            "parent_section": "Protocol implementation",
            "section_heading": "Performance modeling"
        }
    },
    {
        "text": "Section: Protocol implementation\n\nSub-section: Cache block allocation and replacement modeling\n\nConsider the following transaction initialization code for a ReadShared miss:\naction(Initiate_ReadShared_Miss) {\n  tbe.actions.push(Event:ReadMissPipe);\n  tbe.actions.push(Event:TagArrayRead);\n  tbe.actions.push(Event:SendReadShared);\n  tbe.actions.push(Event:SendCompData);\n  tbe.actions.push(Event:WaitCompAck);\n  tbe.actions.push(Event:CheckCacheFill);\n  tbe.actions.push(Event:TagArrayWrite);\n}\nAll transactions that modify a cache line or received cache line data as a result of a snoop or a request sent downstream, use the\nCheckCacheFill\naction trigger event. This event triggers a transition that perform the following actions:\nChecks if we need to store the current cache line data in the local cache.\nChecks if we already have a cache block allocated for that line. If not, attempts to allocate a block. If block not available, a victim block is selected for replacement.\nModels the latency of a cache fill.\nWhen a replacement is performed, a new transaction is initialized to keep track of any WriteBack or Evict request sent downstream and/or snoops for backinvalidation (if the cache controller is configured the\nenforce inclusivity). Depending on the configuration parameters, the TBE for the replacement uses resources from a dedicated TBETable or reuses the same resources of the TBE that triggered the replacement. In both\ncases, the transaction that triggered the replacement completes without waiting for the replacement process.\nNotice\nCheckCacheFill\ndoes not actually writes data to the cache block. If only ensures a cache block is allocated if needed, triggers replacements, and models the cache fill latencies. As described previously, TBE data is copied to the cache if needed during the transaction finalization.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/ruby/CHI",
            "page_title": "CHI",
            "parent_section": "Protocol implementation",
            "section_heading": "Cache block allocation and replacement modeling"
        }
    },
    {
        "text": "Section: Supported CHI transactions\n\nSub-section: Supported requests\n\nThe following incoming requests are supported:\nReadShared\nReadNotSharedDirty\nReadUnique\nCleanUnique\nReadOnce\nWriteUniquePtl\nand\nWriteUniqueFull\nWhen receiving any request the\u00a0clusivity configuration parameters are evaluated during the transaction initialization and the\ndoCacheFill\nand\ndataToBeInvalid\nflags are set in the transaction buffer entry allocated for the request.\ndoCacheFill\nindicates we should keep any valid copy of the line in the local cache;\ndataToBeInvalid\nindicates we must invalidate the local copy when completing the transaction.\nWhen receiving\nReadShared\nor\nReadUnique\n, if the data is present at the local cache in the required state (e.g.\nUC\nor\nUD\nfor\nReadUnique\n), a\nCompData\nresponse is send to the requester. The response type depends on the value of\ndataToBeInvalid\n.\nIf\ndataToBeInvalid==true\nThe unique and/or dirty state is always propagated\nFor a\nReadNotSharedDirty\n,\nCompData_SC\nis always sent if local state is\nSD\nand the line is written-back using\nWriteCleanFull\nElse:\nIn response to a\nReadUnique\n: propagate dirty state, i.e.,\nCompData_UD\nor\nCompData_UC\n.\nIn response to a\nReadShared\nor\nReadNotSharedDirty\n: send\nCompData_SC\n. If\nfwd_unique_on_readshared\nconfiguration parameter is set, the\nReadShared\nis handled as a\nReadUnique\nif the line doesn\u2019t have other sharers.\nWhen receiving a\nReadOnce\n,\nCompData_I\nis always sent\u00a0if the data is present at the local cache. For\nWriteUniquePtl\nhandling see below.\nIf there is a cache miss, multiple actions may be performed depending on whether or not\ndoCacheFill\nand\ndataToBeInvalid==false\n; and DCT or DMT is enabled:\nReadShared\n/\nReadNotSharedDirty\n:\nIf dir state is\nRSD\nor\nRU\n:\nIf DCT disabled: send\nSnpShared\nto owner; cache the line locally (if\ndoCacheFill\n) and send response to requester.\nIf DCT enabled: send\nSnpSharedFwd\nto owner;\u00a0if\ndoCacheFill==true\n,\u00a0the\nretToSrc\nfield is set so the line can be cached locally.\nIf dir state is\nRSC\n:\nIf DCT disabled: send\nSnpOnce\nto one of the sharers;\u00a0cache the line locally (if\ndoCacheFill\n) and send\n  response to requester.\nIf DCT enabled:\u00a0send\nSnpSharedFwd\nto one of the sharers;\u00a0if\ndoCacheFill==true\n, the\nretToSrc\nfield is set so the line can be cached locally.\nOtherwise: issue a\nReadShared\n/\nReadNotSharedDirty\nor\nReadNoSnp\n(if HNF). In the HNF configuration,\nReadNoSnp\nis issued with DMT if DMT is enabled.\nFor\nReadNotSharedDirty\n,\nSnpNotSharedDirty\nand\nSnpNotSharedDirtyFwd\nis sent instead.\nReadUnique\n:\nIf dir state is\nRU,RUSD,RUSC\n:\nIf DCT disabled or clusivity is inclusive: send\nSnpUnique\nto owner; cache the line locally (if\ndoCacheFill\n) and sent response to requester.\nIf DCT enabled and clusivity is exclusive: send\nSnpUniqueFwd\nto owner.\nIf dir state is\nRSC\n/\nRSD\n:\nSend\nSnpUnique\nwith\nretToSrc=true\nto invalidate sharers and obtain dirty line (in case of\nRSD\n)\nIf not HNF: send\nCleanUnique\ndownstream to obtain unique permissions.\nOtherwise:\u00a0issue a\nReadUnique\nor\nReadNoSnp\n(if HNF). In the HNF configuration,\nReadNoSnp\nis issued with DMT if DMT is enabled.\nFor\nRUSC\namd\nRSC\n, if multiple sharers, only one sharer is selected as target of the above snoops. The other sharers are invalidated using\nSnpUnique\nwith\nretToSrc=false\n.\nReadOnce\n:\nIf dir entry exists:\nIf DCT disabled: send\nSnpOnce\nto one of the sharers; send received data response to requester.\nIf DCT enabled: send\nSnpOnceFwd\nto one of the sharers.\nOtherwise:\u00a0issue a\nReadOnce\nor\nReadNoSnp\n(if HNF). In the HNF configuration,\nReadNoSnp\nis issued with DMT if DMT is enabled.\nCleanUnique\n:\nSend\nSnpCleanInvalid\nto all sharers/owner except original requestor.\nIf not HNF: send\nCleanUnique\ndownstream to obtain unique permissions.\nIf has dirty line, requestor has clean line, and\ndoCacheFill==false\n: writeback the line with\nWriteCleanFull\n.\nWriteUniquePtl\n/\nWriteUniqueFull\n:\nIf data present in local cache on UC or UD states:\nIssue\nSnpCleanInvalid\nif there are any sharers.\nPerform the write in the local cache.\nIf no UC/UD data locally:\nIf HNF:\nIssue\nSnpCleanInvalid\nif there are any sharers.\nMerge any received snoop response data with the WriteUnique data.\nIf has a full line and\ndoCacheFill\nset, cache the line locally, otherwise writeback to memory (\nWriteNoSnp\nor\nWriteNoSnpPtl\n).\nIf no HNF:\nForwards the\nWriteUniquePtl\nand any received data to the downstream cache.\nIncoming snoops will cause any locally cached data to become invalid while handling the request.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/ruby/CHI",
            "page_title": "CHI",
            "parent_section": "Supported CHI transactions",
            "section_heading": "Supported requests"
        }
    },
    {
        "text": "Section: Supported CHI transactions\n\nSub-section: Supported snoops\n\nThe cache controller issues and accepts the following snoops:\nSnpShared\nand\nSnpSharedFwd\nSnpNotSharedDirty\nand\nSnpNotSharedDirtyFwd\nSnpUnique\nand\nSnpUniqueFwd\nSnpCleanInvalid\nSnpOnce\nand\nSnpOnceFwd\nThe snoop response is generated according to the current state of the line as defined in the specification. Data is returned with the snoop response depending on the data state and the value of\nretToSrc\nset by the snooper. If\nretToSrc\nis set, the snoop response always includes data.\nSnpShared\n/\nSnpNotSharedDirty\n:\nSnoopee always returns data is the line is dirty, unique or\nretToSrc\n.\nretToSrc\nis set if the snooper needs to cache the line.\nFinal snoopee state always shared clean.\nSnpUnique\n:\nSnoopee always returns data is the line is dirty, unique or\nretToSrc\n.\nretToSrc\nis set if the snooper needs to cache the line.\nFinal snoopee state always invalid.\nSnpCleanInvalid\n:\nSame as\nSnpUnique\n, except data is not returned if line is unique and clean.\nSnpSharedFwd\n:\nretToSrc\nis set if the snooper needs to cache the line.\nLine forwarded as dirty if dirty\nFinal snoopee state always shared clean\nSnpNotSharedDirtyFwd\n:\nretToSrc\nis set if the snooper needs to cache the line.\nAlways returns data if line was dirty at the snoopee; line always forwarded as clean.\nFinal snoopee state always shared clean.\nSnpUniqueFwd\n:\nSame as SnpUnique, except data is never returned to the snooper (as defined by the spec)\nSnpOnce\n:\nAlways generated with\nretToSrc=true\nand\u00a0snoopee always returns data.\nAccepted in any state\u00a0(except invalid). Final snoopee state does not change.\nSnpOnceFwd\n:\nSame as SnpOnce, except data is never returned to the snooper.\nIf the snoopee has sharers in any state, the same request is sent upstream to all sharers. For\u00a0SnpSharedFwd/SnpNotSharedDirtyFwd and SnpUniqueFwd, a SnpShared/SnpNotSharedFwd or SnpUnique is sent, respectively. For a received SnpOnce, a SnpOnce is sent upstream only if the line is not present locally. In this particular implementation, there\u00a0is always a directory entry for upstream caches that have the line.\nSnoops are never sent to caches that do not have the line\n.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/ruby/CHI",
            "page_title": "CHI",
            "parent_section": "Supported CHI transactions",
            "section_heading": "Supported snoops"
        }
    },
    {
        "text": "Section: Supported CHI transactions\n\nSub-section: Writeback and evictions\n\nA writeback is triggered internally by the controller when a cache line needs to be evicted due to capacity reasons (\ncache maintenance operations are currently not supported\n). See Section\nCache block allocation and replacement modeling\nfor more information on replacements. These internal events are generated depending on the configurations parameters of the controller:\nGlobalEviction\n: evict a line from the current and all upstream caches. This applies if\ndealloc_backinv_unique\nor\ndealloc_backinv_shared\nparameters are set.\nLocalEviction\n: evict a line without backinvaliding upstream caches.\nFirst we deallocate the local cache block (so the request that cause the eviction can allocate a new block and finish).\u00a0For GlobalEviction, a\nSnpCleanInvalid\nis sent to all upstream caches. Once all snoops responses are received (possibly with dirty data), a LocalEviction is performed. The LocalEviction is done by issuing the appropriate request as follows:\nWriteBackFull\n,\u00a0if the the line is dirty\nWriteEvictFull\n,\u00a0if the line is unique and clean\nWriteCleanFull\n,\u00a0if the the line is dirty, but there are clean sharers\nEvict\n, if the line is shared and clean\nFor a HNF configuration the behavior changes slightly:\nWriteNoSnp\nto the SNF is used instead of\nWriteBackFull\nand no requests are issued if the line is clean.\nThe\nWriteBack*\nand\nEvict\nrequests are handled at the downstream cache as follows:\nWriteBackFull\n/\nWriteEvictFull\n/\nWriteCleanFull\n:\nIf\nalloc_on_writeback\n, a cache block may need to be allocated. If there are no free blocks, a LocalEviction is triggered for a cache line in the target cache set. The victim line is selected based on the replacement policy implemented by object pointed by the\ncache\nparameter (which can be configured separately).\nSend a\nCompDBIDResp\nto the requester.\nOnce data is received, update local cache and remove requestor from directory (if\nWriteBackFull\n/\nWriteEvictFull\n).\nEvict\n:\nRemove requestor from directory and reply with\nComp\\_I\n.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/ruby/CHI",
            "page_title": "CHI",
            "parent_section": "Supported CHI transactions",
            "section_heading": "Writeback and evictions"
        }
    },
    {
        "text": "Section: Supported CHI transactions\n\nSub-section: Hazards\n\nA request for a line that currently has an outstanding transaction is always stalled until the transaction completes. Snoops received while there is an outstanding request are handled following the requirements\nin the specification:\nFor an outstanding\nCleanUnique\n:\nSnoop response is sent immediately and the current line state is changed accordingly.\nNotice we don\u2019t model the\nUCE\nand\nUDP\nstates from the CHI spec. If the line is invalidated while the requester waits for a\nCleanUnique\nresponse, it immediately follows up with a\nReadUnique\n.\nFor outstanding\nWriteBackFull\n/\nWriteEvictFull\n/\nWriteCleanFull\nthat have not yet been acked with a\nCompDBIDResp\n; or Evict before\nComp_I\nis received:\nSnoop response is sent immediately and the current line state is changed accordingly.\nThe state of the line that will be written back will the state after the snoop.\nIf a snoop is received while the current transaction is waiting for snoop responses from upstream caches, the incoming snoop is stalled until all pending responses from upstream are received and any follow-up request is sent. This can happen in these scenarios:\nDuring a global replacement\nAn accepted\nReadUnique\nthat required snooping upstream caches\nMultiple snoops may be received while there is an outstanding transaction. In this particular implementation, a\nSnpShared\nor\nSnpSharedFwd\nmay be followed by a\nSnpUnique\nor\nSnpCleanInvalid\n. However, it\u2019s not possible to have concurrent snoops coming from the downstream cache.\nBoth incoming requests and snoops require the allocation of a TBE. To prevent deadlocks when transaction buffers are full, a separate buffer is used to allocate snoop TBEs. Snoops do not allow retry, so if the snoop TBE table is full messages in the snpIn port are stalled, potentially causing severe congestion in the snoop channel in the interconnect.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/ruby/CHI",
            "page_title": "CHI",
            "parent_section": "Supported CHI transactions",
            "section_heading": "Hazards"
        }
    },
    {
        "text": "Section: Supported CHI transactions\n\nSub-section: Other implementations notes\n\nIf an HNF uses DMT, it will send\nReadNoSnpSep\ninstead of\nReadNoSnp\nif the\nenable_DMT_early_dealloc\nconfiguration parameter is set. This allow the HNF to deallocate the TBE earlier.\nOrder bit field is not implemented, thus\nReadReceipt\nresponses are never used except for\nReadNoSnpSep\n.\u00a0Request ordering, when required, is enforced by Ruby by serializing requests at the requester. At the cache controller, requests to the same line are handled in the order of arrival. Requests to different lines can be handled in any order, however they are typically handled in order of arrival given that there are resources available.\nExclusive accesses and atomic requests\u00a0are not implemented. Ruby has its own global monitor in the Sequencer to manage exclusive load and stores. Atomic operations also handled by Ruby and they only require a\nReadUnique\nat the protocol level.\nCompAck\nresponse is always sent when stated as optional in the spec. Requesters always wait for\nCompAck\n(if required or optional) before finalizing the transaction and deallocating resources.\nSeparate\nComp\nand\nDBIDresp\nused only for\nWriteUnique\nrequests.\nDBIDresp\nis sent after receiving all snoop responses;\nComp\nis sent after\nDBIDresp\nand accounting for the front-end write latency (\nwrite_fe_latency\n).\nMemory attribute fields are not implemented.\nDoNotGoToSD\nfield is not implemented.\nCBusy\nis not implemented.\nWriteDataCancel\nresponses are never used.\nError handling is not implemented.\nCache stashing is not implemented.\nAtomic transactions are not implemented.\nDMV transactions are not implemented.\nAny request not listed in the protocol table below is not supported in this implementation.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/ruby/CHI",
            "page_title": "CHI",
            "parent_section": "Supported CHI transactions",
            "section_heading": "Other implementations notes"
        }
    },
    {
        "text": "Section: Supported CHI transactions\n\nSub-section: Protocol table\n\nClick here",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/ruby/CHI",
            "page_title": "CHI",
            "parent_section": "Supported CHI transactions",
            "section_heading": "Protocol table"
        }
    },
    {
        "text": "authors:\nJason Lowe-Power\nlast edited:\n2025-08-21 21:40:17 +0000\nGarnet Standalone\nThis is a dummy cache coherence protocol that is used to operate Garnet\nin a standalone manner. This protocol works in conjunction with the\nGarnet Synthetic Traffic\ninjector.\nRelated Files\nsrc/mem/protocols\nGarnet_standalone-cache.sm\n: cache controller specification\nGarnet_standalone-dir.sm\n: directory controller\nspecification\nGarnet_standalone-msg.sm\n: message type specification\nGarnet_standalone.slicc\n: container file\nCache Hierarchy\nThis protocol assumes a 1-level cache hierarchy. The role of the cache\nis to simply send messages from the cpu to the appropriate directory\n(based on the address), in the appropriate virtual network (based on the\nmessage type). It does not track any state. Infact, no CacheMemory is\ncreated unlike other protocols. The directory receives the messages from\nthe caches, but does not send any back. The goal of this protocol is to\nenable simulation/testing of just the interconnection network.\nStable States and Invariants\nStates\nInvariants\nI\nDefault state of all cache blocks\nCache controller\nRequests, Responses, Triggers:\nLoad, Instruction fetch, Store from the core.\nThe network tester (in src/cpu/testers/networktest/networktest.cc)\ngenerates packets of the type\nReadReq\n,\nINST_FETCH\n, and\nWriteReq\n, which are converted into\nRubyRequestType:LD\n,\nRubyRequestType:IFETCH\n, and\nRubyRequestType:ST\n, respectively, by\nthe RubyPort (in src/mem/ruby/system/RubyPort.hh/cc). These messages\nreach the cache controller via the Sequencer. The destination for these\nmessages is determined by the traffic type, and embedded in the address.\nMore details can be found\nhere\n.\nMain Operation:\nThe goal of the cache is only to act as a source node in the\nunderlying interconnection network. It does not track any\nstates.\nOn a\nLD\nfrom the core:\nit returns a hit, and\nmaps the address to a directory, and issues a message for it\nof type\nMSG\n, and size\nControl\n(8 bytes) in the\nrequest vnet (0).\nNote: vnet 0 could also be made to broadcast, instead of\nsending a directed message to a particular directory, by\nuncommenting the appropriate line in the\na_issueRequest\naction in Network_test-cache.sm\nOn a\nIFETCH\nfrom the core:\nit returns a hit, and\nmaps the address to a directory, and issues a message for it\nof type\nMSG\n, and size\nControl\n(8 bytes) in the\nforward vnet (1).\nOn a\nST\nfrom the core:\nit returns a hit, and\nmaps the address to a directory, and issues a message for it\nof type\nMSG\n, and size\nData\n(72 bytes) in the\nresponse vnet (2).\nNote: request, forward and response are just used to\ndifferentiate the vnets, but do not have any physical\nsignificance in this protocol.\nDirectory controller\nRequests, Responses, Triggers:\nMSG\nfrom the cores\nMain Operation:\nThe goal of the directory is only to act as a destination node\nin the underlying interconnection network. It does not track any\nstates.\nThe directory simply pops its incoming queue upon receiving the\nmessage.\nOther features\nThis protocol assumes only 3 vnets.\nIt should only be used when running\nGarnet Synthetic\n    Traffic\n.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/ruby/Garnet_standalone",
            "page_title": "Garnet Standalone",
            "parent_section": "Overview",
            "section_heading": "Garnet_standalone"
        }
    },
    {
        "text": "authors:\nJason Lowe-Power\nlast edited:\n2025-08-21 21:40:17 +0000\nMESI Two Level\nProtocol Overview\nThis protocol models\ntwo-level cache hierarchy\n. The L1 cache is\nprivate to a core, while the L2 cache is shared among the cores. L1\nCache is split into Instruction and Data cache.\nInclusion\nis maintained between the L1 and L2 cache.\nAt high level the protocol has four stable states,\nM\n,\nE\n,\nS\nand\nI\n. A block in\nM\nstate means the blocks is writable\n(i.e. has exclusive permission) and has been dirtied (i.e. its the\nonly valid copy on-chip).\nE\nstate represent a cache block with\nexclusive permission (i.e. writable) but is not written yet.\nS\nstate means the cache block is only readable and possible multiple\ncopies of it exists in multiple private cache and as well as in the\nshared cache.\nI\nmeans that the cache block is invalid.\nThe on-chip cache coherence is maintained through\nDirectory\nCoherence\nscheme, where the directory information is co-located\nwith the corresponding cache blocks in the shared L2 cache.\nThe protocol has four types of controllers \u2013\nL1 cache controller,\nL2 cache controller, Directory controller\nand\nDMA controller\n.\nL1 cache controller is responsible for managing L1 Instruction and\nL1 Data Cache. Number of instantiations of L1 cache controller is\nequal to the number of cores in the simulated system. L2 cache\ncontroller is responsible for managing the shared L2 cache and for\nmaintaining coherence of on-chip data through directory coherence\nscheme. The Directory controller acts as interface to the Memory\nController/Off-chip main memory and is also responsible for coherence\nacross multiple chips/and external coherence request from DMA\ncontroller. DMA controller is responsible for satisfying coherent\nDMA requests.\nOne of the primary optimizations in this protocol is that if a L1\nCache request a data block even for read permission, the L2 cache\ncontroller if finds that no other core has the block, it returns the\ncache block with exclusive permission. This is an optimization done\nin anticipation that a cache blocks read would be written by the\nsame core soon and thus save an extra request with this\noptimization. This is exactly why\nE\nstate exists (i.e. when a\ncache block is writable but not yet written).\nThe protocol supports\nsilent eviction\nof\nclean\ncache blocks from\nthe private L1 caches. This means that cache blocks which have not\nbeen written to and has readable permission only can drop the cache\nblock from the private L1 cache without informing the L2 cache. This\noptimization helps reducing write-back traffic to the L2 cache\ncontroller.\nRelated Files\nsrc/mem/protocols\nMESI_CMP_directory-L1cache.sm\n: L1 cache controller\nspecification\nMESI_CMP_directory-L2cache.sm\n: L2 cache controller\nspecification\nMESI_CMP_directory-dir.sm\n: directory controller\nspecification\nMESI_CMP_directory-dma.sm\n: dma controller specification\nMESI_CMP_directory-msg.sm\n: coherence message type\nspecifications. This defines different field of different type\nof messages that would be used by the given protocol\nMESI_CMP_directory.slicc\n: container file\nController Description\n**L1 cache\ncontroller**\nStates\nInvariants and Semantic/Purpose of the state\nM\nThe cache block is held in exclusive state by\nonly one L1 cache\n. There are no sharers of this block. The data is potentially is the only valid copy in the system. The copy of the cache block is\nwritable\nand as well as\nreadable\n.\nE\nThe cache block is held with exclusive permission by exactly\nonly one L1 cache\n. The difference with the\nM\nstate is that the cache block is writable (and readable) but not yet written.\nS\nThe cache block is held in shared state by 1 or more L1 caches and/or by the L2 cache. The block is only\nreadable\n. No cache can have the cache block with exclusive permission.\nI / NP\nThe cache block is invalid.\nIS\nTransient state. This means that\nGETS (Read)\nrequest has been issued for the cache block and awaiting for response. The cache block is neither readable nor writable.\nIM\nTransient state. This means that\nGETX (Write)\nrequest has been issued for the cache block and awaiting for response. The cache block is neither readable nor writable.\nSM\nTransient state. This means the cache block was originally in S state and then\nUPGRADE (Write)\nrequest was issued to get exclusive permission for the blocks and awaiting response. The cache block is\nreadable\n.\nIS_I\nTransient state. This means that while in IS state the cache controller received Invalidation from the L2 Cache\u2019s directory. This happens due to race condition due to write to the same cache block by other core, while the given core was trying to get the same cache blocks for reading. The cache block is neither readable nor writable..\nM_I\nTransient state. This state indicates that the cache is trying to replace a cache block in\nM\nstate from its cache and the write-back (PUTX) to the L2 cache\u2019s directory has been issued but awaiting write-back acknowledgement.\nSINK_WB_ACK\nTransient state. This state is reached when waiting for write-back acknowledgement from the L2 cache\u2019s directory, the L1 cache received intervention (forwarded request from other cores). This indicates a race between the issued write-back to the directory and another request from the another cache has happened. This also indicates that the write-back has lost the race (i.e. before it reached the L2 cache\u2019s directory, another core\u2019s request has reached the L2). This state is essential to avoid possibility of complicated race condition that can happen if write-backs are silently dropped at the directory.\nL2 cache controller\nRecall that the on-chip directory is co-located with the corresponding\ncache blocks in the L2 Cache. Thus following states in the L2 cache\nblock encodes the information about the status and permissions of the\ncache blocks in the L2 cache as well as the coherence status of the\ncache block that may be present in one or more private L1 caches. Beyond\nthe coherence states there are also two more important fields per cache\nblock that aids to make proper coherence actions. These fields are\nSharers\nfield, which can be thought of as a bit-vector indicating\nwhich of the private L1 caches potentially have the given cache block.\nThe other important field is the\nOwner\nfield, which is the identity\nof the private L1 cache in case the cache block is held with exclusive\npermission in a L1\ncache.\nStates\nInvariants and Semantic/Purpose of the state\nNP\nThe cache blocks is not present in the on-chip cache hierarchy.\nSS\nThe cache block is present in potentially multiple private caches in only readable mode (i.e.in \u201cS\u201d state in private caches). Corresponding \u201cSharers\u201d vector with the block should give the identity of the private caches which possibly have the cache block in its cache. The cache block in the L2 cache is valid and\nreadable\n.\nM\nThe cache block is present ONLY in the L2 cache and has exclusive permission. L1 Cache\u2019s read/write requests (GETS/GETX) can be satisfied directly from the L2 cache.\nMT\nThe cache block is in ONE of the private L1 caches with exclusive permission. The data in the L2 cache is potentially stale. The identity of the L1 cache which has the block can be found in the \u201cOwner\u201d field associated with the cache block. Any request for read/write (GETS/GETX) from other cores/private L1 caches need to be forwarded to the owner of the cache block. L2 can not service requests itself.\nM_I\nIts a transient state. This state indicates that the cache is trying to replace the cache block from its cache and the write-back (PUTX/PUTS) to the Directory controller (which act as interface to Main memory) has been issued but awaiting write-back acknowledgement. The data is neither readable nor writable.\nMT_I\nIts a transient state. This state indicates that the cache is trying to replace a cache block in\nMT\nstate from its cache. Invalidation to the current owner (private L1 cache) of the cache block has been issued and awaiting write-back from the Owner L1 cache. Note that the this Invalidation (called back-invalidation) is instrumental in making sure that the inclusion is maintained between L1 and L2 caches. The data is neither readable nor writable.\nMCT_I\nIts a transient state.This state is same as\nMT_I\n, except that it is known that the data in the L2 cache is in\nclean\nstate. The data is neither readable nor writable.\nI_I\nIts a transient state. The L2 cache is trying to replace a cache block in the\nSS\nstate and the cache block in the L2 is in\nclean\nstate. Invalidations has been sent to all potential sharers (L1 caches) of the cache block. The L2 cache\u2019s directory is waiting for all the required Acknowledgements to arrive from the L1 caches. Note that the this Invalidation (called back-invalidation) is instrumental in making sure that the inclusion is maintained between L1 and L2 caches. The data is neither readable nor writable.\nS_I\nIts a transient state.Same as\nI_I\n, except the data in L2 cache for the cache block is\ndirty\n. This means unlike in the case of\nI_I\n, the data needs to be sent to the Main memory. The cache block is neither readable nor writable..\nISS\nIts a transient state. L2 has received a\nGETS (read)\nrequest from one of the private L1 caches, for a cache block that it not present in the on-chip caches. A read request has been sent to the Main Memory (Directory controller) and waiting for the response from the memory. This state is reached only when the request is for data cache block (not instruction cache block). The purpose of this state is that if it is found that only one L1 cache has requested the cache block then the block is returned to the requester with exclusive permission (although it was requested for reading permission). The cache block is neither readable nor writable.\nIS\nIts a transient state. The state is similar to\nISS\n, except the fact that if the requested cache block is Instruction cache block or more than one core request the same cache block while waiting for the response from the memory, this state is reached instead of\nISS\n. Once the requested cache block arrives from the Main Memory, the block is sent to the requester(s) with read-only permission. The cache block is neither readable nor writable at this state.\nIM\nIts a transient state. This state is reached when a L1 GETX (write) request is received by the L2 cache for a cache blocks that is not present in the on-chip cache hierarchy. The request for the cache block in exclusive mode has been issued to the main memory but response is yet to arrive.The cache block is neither readable nor writable at this state.\nSS_MB\nIts a transient state. In general any state whose name ends with \u201cB\u201d (like this one) also means that it is a\nblocking\ncoherence state. This means the directory awaiting for some response from the private L1 cache ans until it receives the desired response any other request is not entertained (i.e. request are effectively serialized). This particular state is reached when a L1 cache requests a cache block with exclusive permission (i.e. GETX or UPGRADE) and the coherence state of the cache blocks was in\nSS\nstate. This means that the requested cache blocks potentially has readable copies in the private L1 caches. Thus before giving the exclusive permission to the requester, all the readable copies in the L1 caches need to be invalidated. This state indicate that the required invalidations has been sent to the potential sharers (L1 caches) and the requester has been informed about the required number of Invalidation Acknowledgement it needs before it can have the exclusive permission for the cache block. Once the requester L1 cache gets the required number of Invalidation Acknowledgement it informs the director about this by\nUNBLOCK\nmessage which allows the directory to move out of this blocking coherence state and thereafter it can resume entertaining other request for the given cache block. The cache block is neither readable nor writable at this state.\nMT_MB\nIts a transient state and also a\nblocking\nstate. This state is reached when L2 cache\u2019s directory has sent out a cache block with exclusive permission to a requester L1 cache but yet to receive\nUNBLOCK\nfrom the requester L1 cache acknowledging the receipt of exclusive permission. The cache block is neither readable nor writable at this state.\nMT_IIB\nIts a transient state and also a\nblocking\nstate. This state is reached when a read request (GETS) request is received for a cache blocks which is currently held with exclusive permission in another private L1 cache (i.e. directory state is\nMT\n). On such requests the L2 cache\u2019s directory forwards the request to the current owner L1 cache and transitions to this state. Two events need to happen before this cache block can be unblocked (and thus start entertaining further request for this cache block). The current owner cache block need to send a write-back to the L2 cache to update the L2\u2019s copy with latest value. The requester L1 cache also needs to send\nUNBLOCK\nto the L2 cache indicating that it has got the requested cache block with desired coherence permissions. The cache block is neither readable nor writable at this state in the L2 cache.\nMT_IB\nIts a transient state and also a\nblocking\nstate. This state is reached when at\nMT_IIB\nstate the L2 cache controller receives the\nUNBLOCK\nfrom the requester L1 cache but yet to receive the write-back from the previous owner L1 cache of the block. The cache block is neither readable nor writable at this state in the L2 cache.\nMT_SB\nIts a transient state and also a\nblocking\nstate. This state is reached when at\nMT_IIB\nstate the L2 cache controller receives write-back from the previous owner L1 cache for the blocks, while yet to receive the\nUNBLOCK\nfrom the current requester for the cache block. The cache block is neither readable nor writable at this state in the L2 cache.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/ruby/MESI_Two_Level",
            "page_title": "MESI Two Level",
            "parent_section": "Overview",
            "section_heading": "MESI_Two_Level"
        }
    },
    {
        "text": "authors:\nJason Lowe-Power\nlast edited:\n2025-08-21 21:40:17 +0000\nMI Example\nProtocol Overview\nThis is a simple cache coherence protocol that is used to illustrate\nprotocol specification using SLICC.\nThis protocol assumes a 1-level cache hierarchy. The cache is\nprivate to each node. The caches are kept coherent by a directory\ncontroller. Since the hierarchy is only 1-level, there is no\ninclusion/exclusion requirement.\nThis protocol does not differentiate between loads and stores.\nThis protocol cannot implement the semantics of LL/SC instructions,\nbecause external GETS requests that hit a block within a LL/SC\nsequence steal exclusive permissions, thus causing the SC\ninstruction to fail.\nRelated Files\nsrc/mem/protocols\nMI_example-cache.sm\n: cache controller specification\nMI_example-dir.sm\n: directory controller specification\nMI_example-dma.sm\n: dma controller specification\nMI_example-msg.sm\n: message type specification\nMI_example.slicc\n: container file\nStable States and Invariants\nStates\nInvariants\nM\nThe cache block has been accessed (read/written) by this node. No other node holds a copy of the cache block\nI\nThe cache block at this node is invalid\nThe notation used in the controller FSM diagrams is described\nhere\n.\nCache controller\nRequests, Responses, Triggers:\nLoad, Instruction fetch, Store from the core\nReplacement from self\nData from the directory controller\nForwarded request (intervention) from the directory controller\nWriteback acknowledgement from the directory controller\nInvalidations from directory controller (on dma activity)\nMain Operation:\nOn a\nload/Instruction fetch/Store\nrequest from the core:\nit checks whether the corresponding block is present in the\nM state. If so, it returns a hit\notherwise, if in I state, it initiates a GETX request from\nthe directory controller\nOn a\nreplacement\ntrigger from self:\nit evicts the block, issues a writeback request to the\ndirectory controller\nit waits for acknowledgement from the directory controller\n(to prevent races)\nOn a\nforwarded request\nfrom the directory controller:\nThis means that the block was in M state at this node when\nthe request was generated by some other node\nIt sends the block directly to the requesting node\n(cache-to-cache transfer)\nIt evicts the block from this node\nInvalidations\nare similar to replacements\nDirectory controller\nRequests, Responses, Triggers:\nGETX from the cores, Forwarded GETX to the cores\nData from memory, Data to the cores\nWriteback requests from the cores, Writeback acknowledgements to\nthe cores\nDMA read, write requests from the DMA controllers\nMain Operation:\nThe directory maintains track of which core has a block in the M\nstate. It designates this core as owner of the block.\nOn a\nGETX\nrequest from a core:\nIf the block is not present, a memory fetch request is\ninitiated\nIf the block is already present, then it means the request\nis generated from some other core\nIn this case, a forwarded request is sent to the\noriginal owner\nOwnership of the block is transferred to the requestor\nOn a\nwriteback\nrequest from a core:\nIf the core is owner, the data is written to memory and\nacknowledgement is sent back to the core\nIf the core is not owner, a NACK is sent back\nThis can happen in a race condition\nThe core evicted the block while a forwarded request\nsome other core was on the way and the directory has\nalready changed ownership for the core\nThe evicting core holds the data till the forwarded\nrequest arrives\nOn\nDMA\naccesses (read/write)\nInvalidation is sent to the owner node (if any). Otherwise\ndata is fetched from memory.\nThis ensures that the most recent data is available.\nOther features\nMI protocols don\u2019t support LL/SC semantics. A load from a remote\n    core will invalidate the cache block.\nThis protocol has no timeout mechanisms.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/ruby/MI_example",
            "page_title": "MI Example",
            "parent_section": "Overview",
            "section_heading": "MI_example"
        }
    },
    {
        "text": "authors:\nJason Lowe-Power\nlast edited:\n2025-08-21 21:40:17 +0000\nMOESI CMP Directory\nProtocol Overview\nTODO: cache hierarchy\nIn contrast with the MESI protocol, the MOESI protocol introduces an\nadditional\nOwned\nstate.\nThe MOESI protocol also includes many coalescing optimizations not\navailable in the MESI protocol.\nRelated Files\nsrc/mem/protocols\nMOESI_CMP_directory-L1cache.sm\n: L1 cache controller\nspecification\nMOESI_CMP_directory-L2cache.sm\n: L2 cache controller\nspecification\nMOESI_CMP_directory-dir.sm\n: directory controller\nspecification\nMOESI_CMP_directory-dma.sm\n: dma controller specification\nMOESI_CMP_directory-msg.sm\n: message type specification\nMOESI_CMP_directory.slicc\n: container file\nL1 Cache Controller\nStable States and Invariants\nStates\nInvariants\nMM\nThe cache block is held exclusively by this node and is potentially modified (similar to conventional \u201cM\u201d state).\nMM_W\nThe cache block is held exclusively by this node and is potentially modified (similar to conventional \u201cM\u201d state). Replacements and DMA accesses are not allowed in this state. The block automatically transitions to MM state after a timeout.\nO\nThe cache block is owned by this node. It has not been modified by this node. No other node holds this block in exclusive mode, but sharers potentially exist.\nM\nThe cache block is held in exclusive mode, but not written to (similar to conventional \u201cE\u201d state). No other node holds a copy of this block. Stores are not allowed in this state.\nM_W\nThe cache block is held in exclusive mode, but not written to (similar to conventional \u201cE\u201d state). No other node holds a copy of this block. Only loads and stores are allowed. Silent upgrade happens to MM_W state on store. Replacements and DMA accesses are not allowed in this state. The block automatically transitions to M state after a timeout.\nS\nThe cache block is held in shared state by 1 or more nodes. Stores are not allowed in this state.\nI\nThe cache block is invalid.\nFSM Abstraction\nThe notation used in the controller FSM diagrams is described\nhere\n.\nOptimizations\nStates\nDescription\nSM\nA GETX has been issued to get exclusive permissions for an impending store to the cache block, but an old copy of the block is still present. Stores and Replacements are not allowed in this state.\nOM\nA GETX has been issued to get exclusive permissions for an impending store to the cache block, the data has been received, but all expected acknowledgments have not yet arrived. Stores and Replacements are not allowed in this state.\nThe notation used in the controller FSM diagrams is described\nhere\n.\nL2 Cache Controller\nStable States and Invariants\nIntra-chip Inclusion\nInter-chip Exclusion\nStates\nDescription\nNot in any L1 or L2 at this chip\nMay be present at other chips\nNP/I\nThe cache block at this chip is invalid.\nNot in L2, but in 1 or more L1s at this chip\nMay be present at other chips\nILS\nThe cache block is not present at L2 on this chip. It is shared locally by L1 nodes in this chip.\nILO\nThe cache block is not present at L2 on this chip. Some L1 node in this chip is an owner of this cache block.\nILOS\nThe cache block is not present at L2 on this chip. Some L1 node in this chip is an owner of this cache block. There are also L1 sharers of this cache block in this chip.\nNot present at any other chip\nILX\nThe cache block is not present at L2 on this chip. It is held in exclusive mode by some L1 node in this chip.\nILOX\nThe cache block is not present at L2 on this chip. It is held exclusively by this chip and some L1 node in this chip is an owner of the block.\nILOSX\nThe cache block is not present at L2 on this chip. It is held exclusively by this chip. Some L1 node in this chip is an owner of the block. There are also L1 sharers of this cache block in this chip.\nIn L2, but not in any L1 at this chip\nMay be present at other chips\nS\nThe cache block is not present at L1 on this chip. It is held in shared mode at L2 on this chip and is also potentially shared across chips.\nO\nThe cache block is not present at L1 on this chip. It is held in owned mode at L2 on this chip. It is also potentially shared across chips.\nNot present at any other chip\nM\nThe cache block is not present at L1 on this chip. It is present at L2 on this chip and is potentially modified.\nBoth in L2, and 1 or more L1s at this chip\nMay be present at other chips\nSLS\nThe cache block is present at L2 in shared mode on this chip. There exists local L1 sharers of the block on this chip. It is also potentially shared across chips.\nOLS\nThe cache block is present at L2 in owned mode on this chip. There exists local L1 sharers of the block on this chip. It is also potentially shared across chips.\nNot present at any other chip\nOLSX\nThe cache block is present at L2 in owned mode on this chip. There exists local L1 sharers of the block on this chip. It is held exclusively by this chip.\nFSM Abstraction\nThe controller is described in 2 parts. The first picture shows\ntransitions between all \u201cintra-chip inclusion\u201d categories and within\ncategories 1, 3, 4. Transitions within category 2 (Not in L2, but in 1\nor more L1s at this chip) are shown in the second picture.\nThe notation used in the controller FSM diagrams is described\nhere\n. Transitions\ninvolving other chips are annotated in\nbrown\n.\nThe second picture below expands the central hexagonal portion of the\nabove picture to show transitions within category 2 (Not in L2, but in 1\nor more L1s at this chip).\nThe notation used in the controller FSM diagrams is described\nhere\n. Transitions\ninvolving other chips are annotated in\nbrown\n.\nDirectory Controller\n**Stable States and\nInvariants**\nStates\nInvariants\nM\nThe cache block is held in exclusive state by only 1 node (which is also the owner). There are no sharers of this block. The data is potentially different from that in memory.\nO\nThe cache block is owned by exactly 1 node. There may be sharers of this block. The data is potentially different from that in memory.\nS\nThe cache block is held in shared state by 1 or more nodes. No node has ownership of the block. The data is consistent with that in memory (Check).\nI\nThe cache block is invalid.\nFSM Abstraction\nThe notation used in the controller FSM diagrams is described\nhere\n.\nOther features\nTimeouts\n:",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/ruby/MOESI_CMP_directory",
            "page_title": "MOESI CMP Directory",
            "parent_section": "Overview",
            "section_heading": "MOESI_CMP_directory"
        }
    },
    {
        "text": "authors:\nJason Lowe-Power\nlast edited:\n2025-08-21 21:40:17 +0000\nMOESI CMP token\nProtocol Overview\nThis protocol also models a 2-level cache hierarchy.\nIt maintains coherence permission by explicitly exchanging and\ncounting tokens.\nA fix number of token are assigned to each cache block in the\nbeginning, the number of token remains unchanged.\nTo write a block, the processor must have all the token for that\nblock. For reading at least one token is required.\nThe protocol also has a persistent message support to avoid\nstarvation.\nRelated Files\nsrc/mem/protocols\nMOESI_CMP_token-L1cache.sm\n: L1 cache controller\nspecification\nMOESI_CMP_token-L2cache.sm\n: L2 cache controller\nspecification\nMOESI_CMP_token-dir.sm\n: directory controller specification\nMOESI_CMP_token-dma.sm\n: dma controller specification\nMOESI_CMP_token-msg.sm\n: message type specification\nMOESI_CMP_token.slicc\n: container file\nController Description\nL1 Cache\nStates\nInvariants\nMM\nThe cache block is held exclusively by this node and is potentially modified (similar to conventional \u201cM\u201d state).\nMM_W\nThe cache block is held exclusively by this node and is potentially modified (similar to conventional \u201cM\u201d state). Replacements and DMA accesses are not allowed in this state. The block automatically transitions to MM state after a timeout.\nO\nThe cache block is owned by this node. It has not been modified by this node. No other node holds this block in exclusive mode, but sharers potentially exist.\nM\nThe cache block is held in exclusive mode, but not written to (similar to conventional \u201cE\u201d state). No other node holds a copy of this block. Stores are not allowed in this state.\nM_W\nThe cache block is held in exclusive mode, but not written to (similar to conventional \u201cE\u201d state). No other node holds a copy of this block. Only loads and stores are allowed. Silent upgrade happens to MM_W state on store. Replacements and DMA accesses are not allowed in this state. The block automatically transitions to M state after a timeout.\nS\nThe cache block is held in shared state by 1 or more nodes. Stores are not allowed in this state.\nI\nThe cache block is invalid.\nL2 cache\nStates\nInvariants\nNP\nThe cache block is held exclusively by this node and is potentially locally modified (similar to conventional \u201cM\u201d state).\nO\nThe cache block is owned by this node. It has not been modified by this node. No other node holds this block in exclusive mode, but sharers potentially exist.\nM\nThe cache block is held in exclusive mode, but not written to (similar to conventional \u201cE\u201d state). No other node holds a copy of this block. Stores are not allowed in this state.\nS\nThe cache line holds the most recent, correct copy of the data. Other processors in the system may hold copies of the data in the shared state, as well. The cache line can be read, but not written in this state.\nI\nThe cache line is invalid and does not hold a valid copy of the data.\nDirectory controller\nStates\nInvariants\nO\nOwner .\nNO\nNot Owner.\nL\nLocked.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/ruby/MOESI_CMP_token",
            "page_title": "MOESI CMP token",
            "parent_section": "Overview",
            "section_heading": "MOESI_CMP_token"
        }
    },
    {
        "text": "authors:\nJason Lowe-Power\nlast edited:\n2025-08-21 21:40:17 +0000\nMOESI Hammer\nThis is an implementation of AMD\u2019s Hammer protocol, which is used in\nAMD\u2019s Hammer chip (also know as the Opteron or Athlon 64). The protocol\nimplements both the original a HyperTransport protocol, as well as the\nmore recent ProbeFilter protocol. The protocol also includes a full-bit\ndirectory mode.\nRelated Files\nsrc/mem/protocols\nMOESI_hammer-cache.sm\n: cache controller specification\nMOESI_hammer-dir.sm\n: directory controller specification\nMOESI_hammer-dma.sm\n: dma controller specification\nMOESI_hammer-msg.sm\n: message type specification\nMOESI_hammer.slicc\n: container file\nCache Hierarchy\nThis protocol implements a 2-level private cache hierarchy. It assigns\nseparate Instruction and Data L1 caches, and a unified L2 cache to each\ncore. These caches are private to each core and are controlled with one\nshared cache controller. This protocol enforce exclusion between L1 and\nL2\ncaches.\nStable States and Invariants\nStates\nInvariants\nMM\nThe cache block is held exclusively by this node and is potentially locally modified (similar to conventional \u201cM\u201d state).\nO\nThe cache block is owned by this node. It has not been modified by this node. No other node holds this block in exclusive mode, but sharers potentially exist.\nM\nThe cache block is held in exclusive mode, but not written to (similar to conventional \u201cE\u201d state). No other node holds a copy of this block. Stores are not allowed in this state.\nS\nThe cache line holds the most recent, correct copy of the data. Other processors in the system may hold copies of the data in the shared state, as well. The cache line can be read, but not written in this state.\nI\nThe cache line is invalid and does not hold a valid copy of the data.\nCache controller\nThe notation used in the controller FSM diagrams is described\nhere\n.\nMOESI_hammer supports cache flushing. To flush a cache line, the cache\ncontroller first issues a GETF request to the directory to block the\nline until the flushing is completed. It then issues a PUTF and writes\nback the cache line.\nDirectory controller\nMOESI_hammer memory module, unlike a typical directory protocol, does\nnot contain any directory state and instead broadcasts requests to all\nthe processors in the system. In parallel, it fetches the data from the\nDRAM and forward the response to the requesters.\nprobe filter: TODO\nStable States and Invariants\nStates\nInvariants\nNX\nNot Owner, probe filter entry exists, block in O at Owner.\nNO\nNot Owner, probe filter entry exists, block in E/M at Owner.\nS\nData clean, probe filter entry exists pointing to the current owner.\nO\nData clean, probe filter entry exists.\nE\nExclusive Owner, no probe filter entry.\nController\nThe notation used in the controller FSM diagrams is described\nhere\n.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/ruby/MOESI_hammer",
            "page_title": "MOESI Hammer",
            "parent_section": "Overview",
            "section_heading": "MOESI_hammer"
        }
    },
    {
        "text": "Section: Common Notations and Data Structures\n\nSub-section: Coherence Messages\n\nThese are described in the <\nprotocol-name\n>-msg.sm file for each\nprotocol.\nMessage\nDescription\nACK/NACK\npositive/negative acknowledgement for requests that wait for the direction of resolution before deciding on the next action. Examples are writeback requests, exclusive requests.\nGETS\nrequest for shared permissions to satisfy a CPU\u2019s load or IFetch.\nGETX\nrequest for exclusive access.\nINV\ninvalidation request. This can be triggered by the coherence protocol itself, or by the next cache level/directory to enforce inclusion or to trigger a writeback for a DMA access so that the latest copy of data is obtained.\nPUTX\nrequest for writeback of cache block. Some protocols (e.g. MOESI_CMP_directory) may use this only for writeback requests of exclusive data.\nPUTS\nrequest for writeback of cache block in shared state.\nPUTO\nrequest for writeback of cache block in owned state.\nPUTO_Sharers\nrequest for writeback of cache block in owned state but other sharers of the block exist.\nUNBLOCK\nmessage to unblock next cache level/directory for blocking protocols.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/ruby/cache-coherence-protocols",
            "page_title": "Cache Coherence Protocols",
            "parent_section": "Common Notations and Data Structures",
            "section_heading": "Coherence Messages"
        }
    },
    {
        "text": "Section: Common Notations and Data Structures\n\nSub-section: AccessPermissions\n\nThese are associated with each cache block and determine what operations\nare permitted on that block. It is closely correlated with coherence\nprotocol\nstates.\nPermissions\nDescription\nInvalid\nThe cache block is invalid. The block must first be obtained (from elsewhere in the memory hierarchy) before loads/stores can be performed. No action on invalidates (except maybe sending an ACK). No action on replacements. The associated coherence protocol states are I or NP and are stable states in every protocol.\nBusy\nTODO\nRead_Only\nOnly operations permitted are loads, writebacks, invalidates. Stores cannot be performed before transitioning to some other state.\nRead_Write\nLoads, stores, writebacks, invalidations are allowed. Usually indicates that the block is dirty.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/ruby/cache-coherence-protocols",
            "page_title": "Cache Coherence Protocols",
            "parent_section": "Common Notations and Data Structures",
            "section_heading": "AccessPermissions"
        }
    },
    {
        "text": "Section: Common Notations and Data Structures\n\nSub-section: Data Structures\n\nMessage Buffers\n:TODO\nTBE Table\n: TODO\nTimer Table\n: This maintains a map of address-based timers. For\neach target address, a timeout value can be associated and added to\nthe Timer table. This data structure is used, for example, by the L1\ncache controller implementation of the MOESI_CMP_directory\nprotocol to trigger separate timeouts for cache blocks. Internally,\nthe Timer Table uses the event queue to schedule the timeouts. The\nTimerTable supports a polling-based interface,\nisReady()\nto\ncheck if a timeout has occurred. Timeouts on addresses can be set\nusing the\nset()\nmethod and removed using the\nunset()\nmethod.\nRelated Files\n:\nsrc/mem/ruby/system/TimerTable.hh: Declares the\n        TimerTable class\nsrc/mem/ruby/system/TimerTable.cc: Implementation of the\n        methods of the TimerTable class, that deals with setting\n        addresses & timeouts, scheduling events using the event\n        queue.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/ruby/cache-coherence-protocols",
            "page_title": "Cache Coherence Protocols",
            "parent_section": "Common Notations and Data Structures",
            "section_heading": "Data Structures"
        }
    },
    {
        "text": "Section: Common Notations and Data Structures\n\nSub-section: Coherence controller FSM Diagrams\n\nThe Finite State Machines show only the stable states\nTransitions are annotated using the notation \u201c\nEvent list\n\u201d or\n\u201c\nEvent list : Action list\n\u201d or \u201c\nEvent list : Action list :\nEvent list\n\u201d. For example, Store : GETX indicates that on a Store\nevent, a GETX message was sent whereas GETX : Mem Read indicates\nthat on receiving a GETX message, a memory read request was sent.\nOnly the main triggers and actions are listed.\nOptional actions (e.g. writebacks depending on whether or not the\nblock is dirty) are enclosed within\n[ ]\nIn the diagrams, the transition labels are associated with the arc\nthat cuts across the transition label or the closest arc.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/ruby/cache-coherence-protocols",
            "page_title": "Cache Coherence Protocols",
            "parent_section": "Common Notations and Data Structures",
            "section_heading": "Coherence controller FSM Diagrams"
        }
    },
    {
        "text": "Invocation\nThe garnet networks can be enabled by adding\n\u2013network=garnet2.0\n.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/ruby/garnet-2",
            "page_title": "No Title Found",
            "parent_section": "Invocation",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Configuration\nGarnet2.0 uses the generic network parameters in Network.py:\nnumber_of_virtual_networks\n: This is the maximum number of\nvirtual networks. The actual number of active virtual networks\nis determined by the protocol.\ncontrol_msg_size\n: The size of control messages in bytes.\nDefault is 8.\nm_data_msg_size\nin Network.cc is set to the\nblock size in bytes + control_msg_size.\nAdditional parameters are specified in garnet2.0/GarnetNetwork.py:\nni_flit_size\n: flit size in bytes. Flits are the\ngranularity at which information is sent from one router to the\nother. Default is 16 (=> 128 bits). [This default value of 16\nresults in control messages fitting within 1 flit, and data\nmessages fitting within 5 flits]. Garnet requires the\nni_flit_size to be the same as the bandwidth_factor (in\nnetwork/BasicLink.py) as it does not model variable bandwidth\nwithin the network. This can also be set from the command line\nwith\n\u2013link-width-bits\n.\nvcs_per_vnet\n: number of virtual channels (VC) per virtual\nnetwork. Default is 4. This can also be set from the command\nline with\n\u2013vcs-per-vnet\n.\nbuffers_per_data_vc\n: number of flit-buffers per VC in the\ndata message class. Since data messages occupy 5 flits, this\nvalue can lie between 1-5. Default is 4.\nbuffers_per_ctrl_vc\n: number of flit-buffers per VC in the\ncontrol message class. Since control messages occupy 1 flit, and\na VC can only hold one message at a time, this value has to be\nDefault is 1.\nrouting_algorithm\n: 0: Weight-based table (default), 1: XY,\n2: Custom. More details below.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/ruby/garnet-2",
            "page_title": "No Title Found",
            "parent_section": "Configuration",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Topology\nGarnet2.0 leverages the\nTopology\ninfrastructure\nprovided by gem5\u2019s ruby memory system model. Any heterogeneous topology\ncan be modeled. Each router in the topology file can be given an\nindependent latency, which overrides the default. In addition, each link\nhas 2 optional parameters: src_outport and dst_inport, which are\nstrings with names of the output and input ports of the source and\ndestination routers for each link. These can be used inside garnet2.0 to\nimplement custom routing algorithms, as described next. For instance, in\na Mesh, the west to east links have src_outport set to \u201cwest\u201d and\ndst_inport\u201d set to \u201ceast\u201d.\nNetwork Components\n:\nGarnetNetwork\n: This is the top level object that\ninstantiates all network interfaces, routers, and links.\nTopology.cc calls the methods to add \u201cexternal links\u201d between\nNIs and routers, and \u201cinternal links\u201d between routers.\nNetworkInterface\n: Each NI connects to one coherence\ncontroller via MsgBuffer interfaces on one side. It has a link\nto a router on the other. Every protocol message is put into a\none-flit control or multi (default=5)-flit data (depending on\nits vnet), and injected into the router. Multiple NIs can\nconnect to the same router (for e.g., in the Mesh topology,\ncache and dir controllers connect via individual NIs to the same\nrouter).\nRouter\n: The router manages arbitration for output links, and\nflow control between routers.\nNetworkLink\n: Network links carry flits. They can be of one\nof 3 types: EXT_OUT_ (router to NI), EXT_IN_ (NI to router),\nand INT_ (internal router to router)\nCreditLink\n: Credit links carry VC/buffer credits between\nrouters for flow control.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/ruby/garnet-2",
            "page_title": "No Title Found",
            "parent_section": "Topology",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Routing\nGarnet2.0 leverages the\nRouting\ninfrastructure\nprovided by gem5\u2019s ruby memory system model. The default routing\nalgorithm is a deterministic table-based routing algorithm with shortest\npaths. Link weights can be used to prioritize certain links over others.\nSee src/mem/ruby/network/Topology.cc for details about how the routing\ntable is populated.\nCustom Routing\n: To model custom routing algorithms, say adaptive, we\nprovide a framework to name each link with a src_outport and\ndst_inport direction, and use these inside garnet to implement routing\nalgorithms. For instance, in a Mesh, West-first can be implemented by\nsending a flit along the \u201cwest\u201d outport link till the flit no longer has\nany X- hops remaining, and then randomly (or based on next router VC\navailability) choosing one of the remaining links. See how\noutportComputeXY() is implemented in\nsrc/mem/ruby/network/garnet2.0/RoutingUnit.cc. Similarly,\noutportComputeCustom() can be implemented, and invoked by adding\n\u2013routing-algorithm=2 in the command line.\nMulticast messages\n: The network modeled does not have hardware\nmulti-cast support within the network. A multi-cast message gets broken\ninto multiple uni-cast messages at the Network Interface.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/ruby/garnet-2",
            "page_title": "No Title Found",
            "parent_section": "Routing",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Flow Control\nVirtual Channel Flow Control is used in the design. Each VC can hold one\npacket. There are two kinds of VCs in the design - control and data. The\nbuffer depth in each can be independently controlled from\nGarnetNetwork.py. The default values are 1-flit deep control VCs, and\n4-flit deep data VCs. Default size of control packets is 1-flit, and\ndata packets is 5-flit.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/ruby/garnet-2",
            "page_title": "No Title Found",
            "parent_section": "Flow Control",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Router Microarchitecture\nThe garnet2.0 router performs the following actions:\nBuffer Write (BW)\n: The incoming flit gets buffered in its VC.\nRoute Compute (RC)\nThe buffered flit computes its output port,\nand this information is stored in its VC.\nSwitch Allocation (SA)\n: All buffered flits try to reserve the\nswitch ports for the next cycle. [The allocation occurs in a\nseparable\nmanner: First, each input chooses one input VC, using\ninput arbiters, which places a switch request. Then, each output\nport breaks conflicts via output arbiters]. All arbiters in ordered\nvirtual networks are\nqueueing\nto maintain point-to-point ordering.\nAll other arbiters are\nround-robin\n.\nVC Selection (VS)\n: The winner of SA selects a free VC (if\nHEAD/HEAD_TAIL flit) from its output port.\nSwitch Traversal (ST)\n: Flits that won SA traverse the crossbar\nswitch.\nLink Traversal (LT)\n: Flits from the crossbar traverse links to\nreach the next routers.\nIn the default design, BW, RC, SA, VS, and ST all happen in 1-cycle. LT\nhappens in the next cycle.\nMulti-cycle Router\n: Multi-cycle routers can be modeled by specifying\na per-router latency in the topology file, or changing the default\nrouter latency in src/mem/ruby/network/BasicRouter.py. This is\nimplemented by making a buffered flit wait in the router for (latency-1)\ncycles before becoming eligible for SA.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/ruby/garnet-2",
            "page_title": "No Title Found",
            "parent_section": "Router Microarchitecture",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Buffer Management\nEach router input port has number_of_virtual_networks Vnets, each\nwith vcs_per_vnet VCs. VCs in control Vnets have a depth of\nbuffers_per_ctrl_vc (default = 1) and VCs in data Vnets have a depth\nof buffers_per_data_vc (default = 4).\nCredits are used to relay\ninformation about free VCs, and number of buffers within each VC.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/ruby/garnet-2",
            "page_title": "No Title Found",
            "parent_section": "Buffer Management",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Lifecycle of a Network Traversal\nNetworkInterface.cc::wakeup()\nEvery NI connected to one coherence protocol controller on one\nend, and one router on the other.\nreceives messages from coherence protocol buffer in appropriate\nvnet and converts them into network packets and sends them into\nthe network.\ngarnet2.0 adds the ability to capture a network trace at\nthis point [under development].\nreceives flits from the network, extracts the protocol message\nand sends it to the coherence protocol buffer in appropriate\nvnet.\nmanages flow-control (i.e., credits) with its attached router.\nThe consuming flit/credit output link of the NI is put in the\nglobal event queue with a timestamp set to next cycle. The\neventqueue calls the wakeup function in the consumer.\nNetworkLink.cc::wakeup()\nreceives flits from NI/router and sends it to NI/router after\nm_latency cycles delay\nDefault latency value for every link can be set from command\nline (see configs/network/Network.py)\nPer link latency can be overwritten in the topology file\nThe consumer of the link (NI/router) is put in the global event\nqueue with a timestamp set after m_latency cycles. The\neventqueue calls the wakeup function in the consumer.\nRouter.cc::wakeup()\nLoop through all InputUnits and call their wakeup()\nLoop through all OutputUnits and call their wakeup()\nCall SwitchAllocator\u2019s wakeup()\nCall CrossbarSwitch\u2019s wakeup()\nThe router\u2019s wakeup function is called whenever any of its\nmodules (InputUnit, OutputUnit, SwitchAllocator, CrossbarSwitch)\nhave a ready flit/credit to act upon this cycle.\nInputUnit.cc::wakeup()\nRead input flit from upstream router if it is ready for this\ncycle\nFor HEAD/HEAD_TAIL flits, perform route computation, and update\nroute in the VC.\nBuffer the flit for (m_latency - 1) cycles and mark it valid\nfor SwitchAllocation starting that cycle.\nDefault latency for every router can be set from command\nline (see configs/network/Network.py)\nPer router latency (i.e., num pipeline stages) can be set in\nthe topology file.\nOutputUnit.cc::wakeup()\nRead input credit from downstream router if it is ready for this\ncycle\nIncrement the credit in the appropriate output VC state.\nMark output VC as free if the credit carries is_free_signal as\ntrue\nSwitchAllocator.cc::wakeup()\nNote: SwitchAllocator performs VC arbitration and selection\nwithin it.\nSA-I (or SA-i): Loop through all input VCs at every input port,\nand select one in a round robin manner.\nFor HEAD/HEAD_TAIL flits only select an input VC whose\noutput port has at least one free output VC.\nFor BODY/TAIL flits, only select an input VC that has\ncredits in its output VC.\nPlace a request for the output port from this VC.\nSA-II (or SA-o): Loop through all output ports, and select one\ninput VC (that placed a request during SA-I) as the winner for\nthis output port in a round robin manner.\nFor HEAD/HEAD_TAIL flits, perform outvc allocation (i.e.,\nselect a free VC from the output port.\nFor BODY/TAIL flits, decrement a credit in the output vc.\nRead the flit out from the input VC, and send it to the\nCrossbarSwitch\nSend a increment_credit signal to the upstream router for this\ninput VC.\nfor HEAD_TAIL/TAIL flits, mark is_free_signal as true in\nthe credit.\nThe input unit sends the credit out on the credit link to\nthe upstream router.\nReschedule the Router to wakeup next cycle for any flits ready\nfor SA next cycle.\nCrossbarSwitch.cc::wakeup()\nLoop through all input ports, and send the winning flit out of\nits output port onto the output link.\nThe consuming flit output link of the router is put in the\nglobal event queue with a timestamp set to next cycle. The\neventqueue calls the wakeup function in the consumer.\nNetworkLink.cc::wakeup()\nreceives flits from NI/router and sends it to NI/router after\nm_latency cycles delay\nDefault latency value for every link can be set from command\nline (see configs/network/Network.py)\nPer link latency can be overwritten in the topology file\nThe consumer of the link (NI/router) is put in the global event\nqueue with a timestamp set after m_latency cycles. The\neventqueue calls the wakeup function in the consumer.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/ruby/garnet-2",
            "page_title": "No Title Found",
            "parent_section": "Lifecycle of a Network Traversal",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Running Garnet2.0 with Synthetic Traffic\nGarnet2.0 can be run in a standalone manner and fed with synthetic\ntraffic. The details are described here:\nGarnet Synthetic\nTraffic",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/ruby/garnet-2",
            "page_title": "No Title Found",
            "parent_section": "Running Garnet2.0 with Synthetic Traffic",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Related files\nconfigs/example/garnet_synth_traffic.py: file to invoke the network tester\nsrc/cpu/testers/garnet_synthetic_traffic: files implementing the tester.\nGarnetSyntheticTraffic.py\nGarnetSyntheticTraffic.hh\nGarnetSyntheticTraffic.cc",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/ruby/garnet_synthetic_traffic",
            "page_title": "Garnet Synthetic Traffic",
            "parent_section": "Related files",
            "section_heading": "Overview"
        }
    },
    {
        "text": "How to run\nFirst build gem5 with the\nGarnet_standalone\ncoherence protocol. The Garnet_standalone protocol is ISA-agnostic, and hence we build it with the NULL ISA.\nFor gem5 <= 23.0:\nscons build/NULL/gem5.debug PROTOCOL=Garnet_standalone\nFor gem5 >= 23.1\nscons defconfig build/NULL build_opts/NULL\nscons setconfig build/NULL RUBY_PROTOCOL_GARNET_STANDALONE=y\nscons build/NULL/gem5.debug\nExample command:\n./build/NULL/gem5.debug configs/example/garnet_synth_traffic.py  \\\n        --num-cpus=16 \\\n        --num-dirs=16 \\\n        --network=garnet \\\n        --topology=Mesh_XY \\\n        --mesh-rows=4  \\\n        --sim-cycles=1000 \\\n        --synthetic=uniform_random \\\n        --injectionrate=0.01",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/ruby/garnet_synthetic_traffic",
            "page_title": "Garnet Synthetic Traffic",
            "parent_section": "How to run",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Parameterized Options\nSystem Configuration\nDescription\n\u2013num-cpus\nNumber of cpus. This is the number of source (injection) nodes in the network.\n\u2013num-dirs\nNumber of directories. This is the number of destination (ejection) nodes in the network.\n\u2013network\nNetwork model: simple or garnet. Use garnet for running synthetic traffic.\n\u2013topology\nTopology for connecting the cpus and dirs to the network routers/switches. More detail about different topologies can be found (here)[Interconnection_Network#Topology].\n\u2013mesh-rows\nThe number of rows in the mesh. Only valid when \u2018\u2019\u2013topology\u2019\u2019 is \u2018\u2018Mesh_\n\u2019\u2019 or \u2018\u2018MeshDirCorners_\n\u2019\u2019.\nNetwork Configuration\nDescription\n\u2013router-latency\nDefault number of pipeline stages in the garnet router. Has to be >= 1.  Can be over-ridden on a per router basis in the topology file.\n\u2013link-latency\nDefault latency of each link in the network. Has to be >= 1.  Can be over-ridden on a per link basis in the topology file.\n\u2013vcs-per-vnet\nNumber of VCs per Virtual Network.\n\u2013link-width-bits\nWidth in bits for all links inside the garnet network. Default = 128.\nTraffic Injection\nDescription\n\u2013sim-cycles\nTotal number of cycles for which the simulation should run.\n\u2013synthetic\nThe type of synthetic traffic to be injected. The following synthetic traffic patterns are currently supported: \u2018uniform_random\u2019, \u2018tornado\u2019, \u2018bit_complement\u2019, \u2018bit_reverse\u2019, \u2018bit_rotation\u2019, \u2018neighbor\u2019, \u2018shuffle\u2019,  and \u2018transpose\u2019.\n\u2013injectionrate\nTraffic Injection Rate in packets/node/cycle. It can take any decimal value between 0 and 1. The number of digits of precision after the decimal point can be controlled by \u2018\u2019\u2013precision\u2019\u2019 which is set to 3 as default in \u2018\u2018garnet_synth_traffic.py\u2019\u2019.\n\u2013single-sender-id\nOnly inject from this sender. To send from all nodes, set to -1.\n\u2013single-dest-id\nOnly send to this destination. To send to all destinations as specified by the synthetic traffic pattern, set to -1.\n\u2013num-packets-max\nMaximum number of packets to be injected by each cpu node. Default value is -1 (keep injecting till sim-cycles).\n\u2013inj-vnet\nOnly inject in this vnet (0, 1 or 2). 0 and 1 are 1-flit, 2 is 5-flit. Set to -1 to inject randomly in all vnets.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/ruby/garnet_synthetic_traffic",
            "page_title": "Garnet Synthetic Traffic",
            "parent_section": "Parameterized Options",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Implementation of Garnet synthetic traffic\nThe synthetic traffic injector is implemented in GarnetSyntheticTraffic.cc. The sequence of steps involved in generating and sending a packet are as follows.\nEvery cycle, each cpu performs a bernouli trial with probability equal to \u2013injectionrate to determine whether to generate a packet or not.\nIf \u2013num-packets-max is non negative, each cpu stops generating new packets after generating \u2013num-packets-max number of packets. The injector terminates after \u2013sim-cycles.\nIf the cpu has to generate a new packet, it computes the destination for the new packet based on the synthetic traffic type (\u2013synthetic).\nThis destination is embedded into the bits after block offset in the packet address.\nThe generated packet is randomly tagged as a ReadReq, or an INST_FETCH, or a WriteReq, and sent to the Ruby Port (src/mem/ruby/system/RubyPort.hh/cc).\nThe Ruby Port converts the packet into a RubyRequestType:LD, RubyRequestType:IFETCH, and RubyRequestType:ST, respectively, and sends it to the Sequencer, which in turn sends it to the Garnet_standalone cache controller.\nThe cache controller extracts the destination directory from the packet address.\nThe cache controller injects the LD, IFETCH and ST into virtual networks 0, 1 and 2 respectively.\nLD and IFETCH are injected as control packets (8 bytes), while ST is injected as a data packet (72 bytes).\nThe packet traverses the network and reaches the directory.\nThe directory controller simply drops it.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/ruby/garnet_synthetic_traffic",
            "page_title": "Garnet Synthetic Traffic",
            "parent_section": "Implementation of Garnet synthetic traffic",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Topology Construction\nHeteroGarnet allows users to configure complex topologies using a python configuration file as the topology.\nThe overall topology configuration could include the complete interconnect definition of the system including\nany heterogeneous components. The general flow of defining a topology involves the following steps:\nDetermine the total number of routers in the system and instantiate them.\nUse the\nRouter\nclass to instantiate individual routers.\nConfigure properties of each router, such as clock domain, supported flit width, depending on the requirements.\nrouters = Router(id, latency, clock_domain,\n         flit_width, supported_vnets,\n         vcs_per_vnet)\nConnect the routers which connect to the end points (e.g, Cores, Caches, Directories) using external physical interconnects.\nUse\nExternalLink\nclass to instantiate the links connecting the end points.\nConfigure properties of each external link, such as clock domain, link width, depending on the requirements.\nEnable clock-domain crossings(CDC) and Serializer-Deserializer(SerDes) units at either depending on the interconnect topology.\nexternal_link = ExternalLink(id, latency, clock_domain,\n                      flit_width, supported_vnets,\n                      serdes_enable, cdc_enable)\nConnect the individual routers within the network depending upon the topology.\nUse\nInternalLink\nclass to instantiate the links connecting the end points.\nConfigure properties of each internal link, such as clock domain, link width, depending on the requirements.\nEnable clock-domain crossings and Serializer-Deserializer units at either depending on the interconnect topology.\ninternal_link = InternalLink(id, latency, clock_domain,\n                      flit_width, supported_vnets,\n                      serdes_enable, cdc_enable)\nGarnet 3.0 also provides several pre-configuration scripts(./configs/Network/Network.py) which automatically do some of the other steps, such as instantiating network interfaces, domain crossings, and SerDes units. The several types of units used to configure the topologies are discussed below.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/ruby/heterogarnet",
            "page_title": "No Title Found",
            "parent_section": "Topology Construction",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Physical Links\nThe physical link model in Garnet represents the interconnect wire itself. A link is a single entity which has its own latency, width and the types of flit it can transmit. The links also support a credit based back-pressuring mechanism. Similar to the upgraded Garnet 3.0 router, each Garnet 3.0 link can be configured to an operating frequency and width using appropriate parameters. This allows links and routers operating at different frequencies to be connected to each other.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/ruby/heterogarnet",
            "page_title": "No Title Found",
            "parent_section": "Physical Links",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Network Interface\nThe network interface controller (NIC) is an object which sits between the network end points (e.g., Caches, DMA nodes) and the interconnection system. The NIC receives messages form the controllers and converts them into fixed-length flits, short for flow control units. These flits are sized appropriately according to the outgoing physical links. The network interface also governs the flow control and buffer management for the outgoing and incoming flits. Garnet 3.0 allows multiple ports to be attached to a single end points. Thus, the NIC decides where a certain message/flit must be scheduled.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/ruby/heterogarnet",
            "page_title": "No Title Found",
            "parent_section": "Network Interface",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Clock Domain Crossing Units\nTo support multiple clock domains, Garnet 3.0 introduces Clock Domain Crossing (CDC) unit, as shown in the Figure below (left), which consists of first-In-First-Out (FIFO) buffers and can be instantiated anywhere within the network model. The CDC unit enables architectures with different clock domains across the system. The delay of each CDC unit configurable. The latency can also be calculated dynamically depending on the clock domains connected to it. This enables accurate modeling of DVFS techniques as CDC latencies are generally a function of the operating frequency of producer and consumer.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/ruby/heterogarnet",
            "page_title": "No Title Found",
            "parent_section": "Clock Domain Crossing Units",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Serializer-Deserializer Units\nAnother critical feature necessary in modeling SoCs and heterogeneous architectures is supporting various interconnect widths across the system. Consider a link between two routers within a GPU and a link between a memory controller and on-chip memory. These two links might be of different widths. To enable such configuration, Garnet 3.0 introduces the Serializer-Deserializer unit as shown in the figure below, which converts flits into appropriate widths at bit-width boundaries. These SerDes units can be instantiated anywhere in the Garnet 3.0 topology similar to the CDC unit described in the previous sub-section.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/ruby/heterogarnet",
            "page_title": "No Title Found",
            "parent_section": "Serializer-Deserializer Units",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Section: Routing\n\nSub-section: Routing Policies.\n\nThere are several generic routing  policies that have been proposed for deadlock free routing of flits through the interconnect network.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/ruby/heterogarnet",
            "page_title": "No Title Found",
            "parent_section": "Routing",
            "section_heading": "Routing Policies."
        }
    },
    {
        "text": "Section: Routing\n\nSub-section: Table based routing\n\nGarnet also features table based routing policy which users can select to set custom routing policies using a weight-age based system. Lower weighted links are preferred over links which are configured to have higher weights.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/ruby/heterogarnet",
            "page_title": "No Title Found",
            "parent_section": "Routing",
            "section_heading": "Table based routing"
        }
    },
    {
        "text": "Section: Flow Control and Buffer Management\n\nSub-section: Virtual Channels\n\nVirtual Channels (VCs) in a network act as separate queues which can share physical wires (physical links) between two routers or arbiters. Virtual channels are mainly used to alleviate head-of-line blocking. However, they are also used as a means for deadlock-avoidance.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/ruby/heterogarnet",
            "page_title": "No Title Found",
            "parent_section": "Flow Control and Buffer Management",
            "section_heading": "Virtual Channels"
        }
    },
    {
        "text": "Section: Flow Control and Buffer Management\n\nSub-section: Buffer Backpressure\n\nMost implementations of interconnection networks do not tolerate dropping of packets or flits during traversal. Thus, there is a need to strictly manage the flits using backpressuring mechanisms.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/ruby/heterogarnet",
            "page_title": "No Title Found",
            "parent_section": "Flow Control and Buffer Management",
            "section_heading": "Buffer Backpressure"
        }
    },
    {
        "text": "Section: Flow Control and Buffer Management\n\nSub-section: Credit-based backpressuring\n\nCredit-based backpressuring mechanism is often used for low-latency implementation of flit-stalling. Credits track the number of buffers available at the next intermediate destination by decrementing the overall buffers every time a flit is sent. A credit is then sent back by the destination when it is vacated.\nRouters in interconnect systems perform arbitration, allocation of buffers, and flow control within the network. The objective of the router microarchitecture is to minimize the contention within the router while offering minimal per-hop latency for the flits. The complexity of the router microarchitecture also affects the overall energy and area consumption of the interconnect system.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/ruby/heterogarnet",
            "page_title": "No Title Found",
            "parent_section": "Flow Control and Buffer Management",
            "section_heading": "Credit-based backpressuring"
        }
    },
    {
        "text": "Section: Life of a Message in Garnet 3.0\n\nSub-section: Injection of Message\n\nThe source cache controller creates a message and assigns one or more  cache controllers as the destination. This message is then injected into message queues. A cache controller often has several outgoing and incoming message buffers for different kinds of messages.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/ruby/heterogarnet",
            "page_title": "No Title Found",
            "parent_section": "Life of a Message in Garnet 3.0",
            "section_heading": "Injection of Message"
        }
    },
    {
        "text": "Section: Life of a Message in Garnet 3.0\n\nSub-section: Conversion to Flits.\n\nA network interface controller unit (NIC) is attached to each cache controller. This NIC wakes up and consumes the messages from the message queues. Each message is then converted to unicast messages before being broken down into fixed-length flits according to the size supported by the outgoing physical links. These flits are then scheduled for transmission depending on the availability of buffers at the next hop through one of the output links. The outgoing link is chosen depending on the destination, routing policy, and the type of message.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/ruby/heterogarnet",
            "page_title": "No Title Found",
            "parent_section": "Life of a Message in Garnet 3.0",
            "section_heading": "Conversion to Flits."
        }
    },
    {
        "text": "Section: Life of a Message in Garnet 3.0\n\nSub-section: Transmission to Local Router.\n\nEach network interface is connected to one or more \u201clocal\u201d routers which is could be connected through an \u201cExternal\u201d link. Once a flit is scheduled, it is transmitted over these external links which deliver the flit to the router after a period of defined latency.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/ruby/heterogarnet",
            "page_title": "No Title Found",
            "parent_section": "Life of a Message in Garnet 3.0",
            "section_heading": "Transmission to Local Router."
        }
    },
    {
        "text": "Section: Life of a Message in Garnet 3.0\n\nSub-section: Router Arbitration.\n\nThe flit wakes up the router which is a multi-stage unit. The router houses the input buffers, VC allocation, switch arbitration, and crossbar units. On arrival the flit is first placed in a input buffer queue. There are several input buffer queues in a router which contend for an output link and a VC for the next hop. This is done using the VC allocation and switch arbitration stages. Once a flit is selected for transmission, the crossbar stage directs the flit to the output link. A credit is then sent back to the NIC as the input buffer space is vacated for the next flit to arrive.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/ruby/heterogarnet",
            "page_title": "No Title Found",
            "parent_section": "Life of a Message in Garnet 3.0",
            "section_heading": "Router Arbitration."
        }
    },
    {
        "text": "Section: Life of a Message in Garnet 3.0\n\nSub-section: Serialization-Deserialization.\n\nThe serialization-deserialization (SerDes) is an optional unit that can be enabled depending on the design requirements. The SerDes units consumes the flits and appropriately converts it into outgoing flit size. In addition to manipulating the data packets, the SerDes also handles the credit system, by serializing or deserializing the credit units.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/ruby/heterogarnet",
            "page_title": "No Title Found",
            "parent_section": "Life of a Message in Garnet 3.0",
            "section_heading": "Serialization-Deserialization."
        }
    },
    {
        "text": "Area, Power and Energy Model\nFrameworks like Orion2.0 and DSENT provide models for the area and power for the various building blocks of a NoC router and links. HeteroGarnet integrates DSENT as an external tool to report area, power and energy (which depends on activity) at the end of the simulation.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/ruby/heterogarnet",
            "page_title": "No Title Found",
            "parent_section": "Area, Power and Energy Model",
            "section_heading": "Overview"
        }
    },
    {
        "text": "How to invoke the network\nSimple Network\n:\n./build/<ISA>/gem5.debug \\\n                      configs/example/ruby_random_test.py \\\n                      --num-cpus=16  \\\n                      --num-dirs=16  \\\n                      --network=simple\n                      --topology=Mesh_XY  \\\n                      --mesh-rows=4\nThe default network is simple, and the default topology is crossbar.\nGarnet network\n:\n./build/<ISA>/gem5.debug \\\n                      configs/example/ruby_random_test.py  \\\n                      --num-cpus=16 \\\n                      --num-dirs=16  \\\n                      --network=garnet2.0 \\\n                      --topology=Mesh_XY \\\n                      --mesh-rows=4",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/ruby/interconnection-network",
            "page_title": "Interconnection Network",
            "parent_section": "How to invoke the network",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Topology\nThe connection between the various controllers are specified via python\nfiles. All external links (between the controllers and routers) are\nbi-directional. All internal links (between routers) are uni-directional\n\u2013 this allows a per-direction weight on each link to bias routing\ndecisions.\nRelated Files\n:\nsrc/mem/ruby/network/topologies/Crossbar.py\nsrc/mem/ruby/network/topologies/CrossbarGarnet.py\nsrc/mem/ruby/network/topologies/Mesh_XY.py\nsrc/mem/ruby/network/topologies/Mesh_westfirst.py\nsrc/mem/ruby/network/topologies/MeshDirCorners_XY.py\nsrc/mem/ruby/network/topologies/Pt2Pt.py\nsrc/mem/ruby/network/Network.py\nsrc/mem/ruby/network/BasicLink.py\nsrc/mem/ruby/network/BasicRouter.py\nTopology Descriptions\n:\nCrossbar\n: Each controller (L1/L2/Directory) is connected to\na simple switch. Each switch is connected to a central switch\n(modeling the crossbar). This can be invoked from command line\nby\n\u2013topology=Crossbar\n.\nCrossbarGarnet\n: Each controller (L1/L2/Directory) is\nconnected to every other controller via one garnet router (which\ninternally models the crossbar and allocator). This can be\ninvoked from command line by\n\u2013topology=CrossbarGarnet\n.\nMesh_*\n: This topology requires the number of directories\nto be equal to the number of cpus. The number of\nrouters/switches is equal to the number of cpus in the system.\nEach router/switch is connected to one L1, one L2 (if present),\nand one Directory. The number of rows in the mesh\nhas to be\nspecified\nby\n\u2013mesh-rows\n. This parameter enables the\ncreation of non-symmetrical meshes too.\nMesh_XY\n: Mesh with XY routing. All x-directional links\nare biased with a weight of 1, while all y-directional links\nare biased with a weight of 2. This forces all messages to\nuse X-links first, before using Y-links. It can be invoked\nfrom command line by\n\u2013topology=Mesh_XY\nMesh_westfirst\n: Mesh with west-first routing. All\nwest-directional links are biased with a weight of 1, al\nother links are biased with a weight of 2. This forces all\nmessages to use west-directional links first, before using\nother links. It can be invoked from command line by\n\u2013topology=Mesh_westfirst\nMeshDirCorners_XY\n: This topology requires the number of\ndirectories to be equal to 4. number of routers/switches is\nequal to the number of cpus in the system. Each router/switch is\nconnected to one L1, one L2 (if present). Each corner\nrouter/switch is connected to one Directory. It can be invoked\nfrom command line by\n\u2013topology=MeshDirCorners_XY\n. The\nnumber of rows in the mesh\nhas to be specified\nby\n\u2013mesh-rows\n. The XY routing algorithm is used.\nPt2Pt\n: Each controller (L1/L2/Directory) is connected to\nevery other controller via a direct link. This can be invoked\nfrom command line by\nPt2Pt\n: All to all point-to-point connection\n\nIn each topology, each link and each router can independently be\npassed a parameter that overrides the defaults (in BasicLink.py and\nBasicRouter.py)\n:\nLink Parameters:\nlatency\n: latency of traversal within the link.\nweight\n: weight associated with this link. This parameter is\nused by the routing table while deciding routes, as explained\nnext in\nRouting\n.\nbandwidth_factor\n: Only used by simple network to specify\nwidth of the link in bytes. This translates to a bandwidth\nmultiplier (simple/SimpleLink.cc) and the individual link\nbandwidth becomes bandwidth multiplier x endpoint_bandwidth\n(specified in SimpleNetwork.py). In garnet, the bandwidth is\nspecified by ni_flit_size in GarnetNetwork.py)\nInternal Link Parameters:\nsrc_outport\n: String with name for output port from source\nrouter.\ndst_inport\n: String with name for input port at destination\nrouter.\nThese two parameters can be used by routers to implement custom routing\nalgorithms in garnet2.0\nRouter Parameters:\nlatency\n: latency of each router. Only supported by\ngarnet2.0.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/ruby/interconnection-network",
            "page_title": "Interconnection Network",
            "parent_section": "Topology",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Routing\nTable-based Routing (Default):\nBased on the topology, shortest\npath graph traversals are used to populate\nrouting tables\nat each\nrouter/switch. This is done in src/mem/ruby/network/Topology.cc The\ndefault routing algorithm is table-based and tries to choose the route\nwith minimum number of link traversals. Links can be given weights in\nthe topology files to model different routing algorithms. For example,\nin Mesh_XY.py and MeshDirCorners_XY.py Y-direction links are given\nweights of 2, while X-direction links are given weights of 1, resulting\nin XY traversals. In Mesh_westfirst.py, the west-links are given\nweights of 1, and all other links are given weights of 2. In garnet2.0,\nthe routing algorithm randomly chooses between links with equal weights.\nIn simple network, it statically chooses between links with equal\nweights.\nCustom Routing algorithms:\nIn garnet2.0, we provide additional\nsupport to implement custom (including adaptive) routing algorithms (See\noutportComputeXY() in src/mem/ruby/network/garnet2.0/RoutingUnit.cc).\nThe src_outport and dst_inport fields of the links can be used to give\ncustom names to each link (e.g., directions if a mesh), and these can be\nused inside garnet to implement any routing algorithm. A custom routing\nalgorithm can be selected from the command line by setting\n\u2013routing-algorithm=2. See configs/network/Network.py and\nsrc/mem/ruby/network/garnet2.0/GarnetNetwork.py",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/ruby/interconnection-network",
            "page_title": "Interconnection Network",
            "parent_section": "Routing",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Section: Flow-Control and Router Microarchitecture\n\nSub-section: Simple Network\n\nThe default network model in Ruby is the simple network.\nRelated Files\n:\nsrc/mem/ruby/network/Network.py\nsrc/mem/ruby/network/simple\nsrc/mem/ruby/network/simple/SimpleNetwork.py",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/ruby/interconnection-network",
            "page_title": "Interconnection Network",
            "parent_section": "Flow-Control and Router Microarchitecture",
            "section_heading": "Simple Network"
        }
    },
    {
        "text": "Configuration\nSimple network uses the generic network parameters in Network.py:\nnumber_of_virtual_networks\n: This is the maximum number of\n    virtual networks. The actual number of active virtual networks\n    is determined by the protocol.\ncontrol_msg_size\n: The size of control messages in bytes.\n    Default is 8.\nm_data_msg_size\nin Network.cc is set to the\n    block size in bytes + control_msg_size.\nAdditional parameters are specified in simple/SimpleNetwork.py:\nbuffer_size\n: Size of buffers at each switch input and\noutput ports. A value of 0 implies infinite buffering.\nendpoint_bandwidth\n: Bandwidth at the end points of the\nnetwork in 1000th of byte.\nadaptive_routing\n: This enables adaptive routing based on\noccupancy of output buffers.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/ruby/interconnection-network",
            "page_title": "Interconnection Network",
            "parent_section": "Configuration",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Section: Switch Model\n\nSub-section: Garnet2.0\n\nDetails of the new (2016) Garnet2.0 network are\nhere\n.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/ruby/interconnection-network",
            "page_title": "Interconnection Network",
            "parent_section": "Switch Model",
            "section_heading": "Garnet2.0"
        }
    },
    {
        "text": "Running the Network with Synthetic Traffic\nThe interconnection networks can be run in a standalone manner and fed\nwith synthetic traffic. We recommend doing this with garnet2.0.\nRunning Garnet Standalone with Synthetic Traffic",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/ruby/interconnection-network",
            "page_title": "Interconnection Network",
            "parent_section": "Running the Network with Synthetic Traffic",
            "section_heading": "Overview"
        }
    },
    {
        "text": "authors:\nJason Lowe-Power\nlast edited:\n2025-08-21 21:40:17 +0000\nSLICC\nSLICC is a domain specific language for specifying cache coherence\nprotocols. The SLICC compiler generates C++ code for different\ncontrollers, which can work in tandem with other parts of Ruby. The\ncompiler also generates an HTML specification of the protocol. HTML\ngeneration is turned off by default. To enable HTML output, pass the\noption \u201cSLICC_HTML=True\u201d to scons when compiling.\nInput To the Compiler\nThe SLICC compiler takes, as input, files that specify the controllers\ninvolved in the protocol. The .slicc file specifies the different files\nused by the particular protocol under consideration. For example, if\ntrying to specify the MI protocol using SLICC, then we may use MI.slicc\nas the file that specifies all the files necessary for the protocol. The\nfiles necessary for specifying a protocol include the definitions of the\nstate machines for different controllers, and of the network messages\nthat are passed on between these controllers.\nThe files have a syntax similar to that of C++. The compiler, written\nusing\nPLY (Python Lex-Yacc)\n, parses these\nfiles to create an Abstract Syntax Tree (AST). The AST is then traversed\nto build some of the internal data structures. Finally the compiler\noutputs the C++ code by traversing the tree again. The AST represents\nthe hierarchy of different structures present with in a state machine.\nWe describe these structures next.\nProtocol State Machines\nIn this section we take a closer look at what goes in to a file\ncontaining specification of a state machine.\nSpecifying Data Members\nEach state machine is described using SLICC\u2019s\nmachine\ndatatype. Each\nmachine has several different types of members. Machines for cache and\ndirectory controllers include cache memory and directory memory data\nmembers respectively. We will use the MI protocol available in\nsrc/mem/protocol as our running example. So here is how you might want\nto start writing a state machine\nmachine(MachineType:L1Cache,\u00a0\"MI\u00a0Example\u00a0L1\u00a0Cache\")\n\u00a0\u00a0:\u00a0Sequencer\u00a0*\u00a0sequencer,\n\u00a0\u00a0\u00a0\u00a0CacheMemory\u00a0*\u00a0cacheMemory,\n\u00a0\u00a0\u00a0\u00a0int\u00a0cache_response_latency\u00a0=\u00a012,\n\u00a0\u00a0\u00a0\u00a0int\u00a0issue_latency\u00a0=\u00a02\u00a0{\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0//\u00a0Add\u00a0rest\u00a0of\u00a0the\u00a0stuff\n\u00a0\u00a0\u00a0\u00a0}\nIn order to let the controller receive messages from different\nentities in the system, the machine has a number of\nMessage\nBuffers\n. These act as input and output ports for the machine. Here\nis an example specifying the output ports.\nMessageBuffer\u00a0requestFromCache,\u00a0network=\"To\",\u00a0virtual_network=\"2\",\u00a0ordered=\"true\";\n\u00a0MessageBuffer\u00a0responseFromCache,\u00a0network=\"To\",\u00a0virtual_network=\"4\",\u00a0ordered=\"true\";\nNote that Message Buffers have some attributes that need to be specified\ncorrectly. Another example, this time for specifying the input\nports.\nMessageBuffer\u00a0forwardToCache,\u00a0network=\"From\",\u00a0virtual_network=\"3\",\u00a0ordered=\"true\";\n\u00a0MessageBuffer\u00a0responseToCache,\u00a0network=\"From\",\u00a0virtual_network=\"4\",\u00a0ordered=\"true\";\nNext the machine includes a declaration of the\nstates\nthat\nmachine can possibly reach. In cache coherence protocol, states can\nbe of two types \u2013 stable and transient. A cache block is said to be\nin a stable state if in the absence of any activity (in coming\nrequest for the block from another controller, for example), the\ncache block would remain in that state for ever. Transient states\nare required for transitioning between stable states. They are\nneeded when ever the transition between two stable states can not be\ndone in an atomic fashion. Next is an example that shows how states\nare declared. SLICC has a keyword\nstate_declaration\nthat has to\nbe used for declaring\nstates.\nstate_declaration(State,\u00a0desc=\"Cache\u00a0states\")\u00a0{\n\u00a0\u00a0\u00a0I,\u00a0AccessPermission:Invalid,\u00a0desc=\"Not\u00a0Present/Invalid\";\n\u00a0\u00a0\u00a0II,\u00a0AccessPermission:Busy,\u00a0desc=\"Not\u00a0Present/Invalid,\u00a0issued\u00a0PUT\";\n\u00a0\u00a0\u00a0M,\u00a0AccessPermission:Read_Write,\u00a0desc=\"Modified\";\n\u00a0\u00a0\u00a0MI,\u00a0AccessPermission:Busy,\u00a0desc=\"Modified,\u00a0issued\u00a0PUT\";\n\u00a0\u00a0\u00a0MII,\u00a0AccessPermission:Busy,\u00a0desc=\"Modified,\u00a0issued\u00a0PUTX,\u00a0received\u00a0nack\";\n\u00a0\u00a0\u00a0IS,\u00a0AccessPermission:Busy,\u00a0desc=\"Issued\u00a0request\u00a0for\u00a0LOAD/IFETCH\";\n\u00a0\u00a0\u00a0IM,\u00a0AccessPermission:Busy,\u00a0desc=\"Issued\u00a0request\u00a0for\u00a0STORE/ATOMIC\";\n}\nThe states I and M are the only stable states in this example. Again\nnote that certain attributes have to be specified with the states.\nThe state machine needs to specify the\nevents\nit can handle and\nthus transition from one state to another. SLICC provides the\nkeyword\nenumeration\nwhich can be used for specifying the set of\npossible events. An example to shed more light on this -\nenumeration(Event,\u00a0desc=\"Cache\u00a0events\")\u00a0{\n\u00a0\u00a0\u00a0//\u00a0From\u00a0processor\n\u00a0\u00a0\u00a0Load,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0desc=\"Load\u00a0request\u00a0from\u00a0processor\";\n\u00a0\u00a0\u00a0Ifetch,\u00a0\u00a0\u00a0\u00a0\u00a0desc=\"Ifetch\u00a0request\u00a0from\u00a0processor\";\n\u00a0\u00a0\u00a0Store,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0desc=\"Store\u00a0request\u00a0from\u00a0processor\";\n\u00a0\u00a0\u00a0Data,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0desc=\"Data\u00a0from\u00a0network\";\n\u00a0\u00a0\u00a0Fwd_GETX,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0desc=\"Forward\u00a0from\u00a0network\";\n\u00a0\u00a0\u00a0Inv,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0desc=\"Invalidate\u00a0request\u00a0from\u00a0dir\";\n\u00a0\u00a0\u00a0Replacement,\u00a0\u00a0desc=\"Replace\u00a0a\u00a0block\";\n\u00a0\u00a0\u00a0Writeback_Ack,\u00a0\u00a0\u00a0desc=\"Ack\u00a0from\u00a0the\u00a0directory\u00a0for\u00a0a\u00a0writeback\";\n\u00a0\u00a0\u00a0Writeback_Nack,\u00a0\u00a0\u00a0desc=\"Nack\u00a0from\u00a0the\u00a0directory\u00a0for\u00a0a\u00a0writeback\";\n}\nWhile developing a protocol machine, we may need to define\nstructures that represent different entities in a memory system.\nSLICC provides the keyword\nstructure\nfor this purpose. An\nexample\nfollows\nstructure(Entry,\u00a0desc=\"...\",\u00a0interface=\"AbstractCacheEntry\")\u00a0{\n\u00a0\u00a0\u00a0State\u00a0CacheState,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0desc=\"cache\u00a0state\";\n\u00a0\u00a0\u00a0bool\u00a0Dirty,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0desc=\"Is\u00a0the\u00a0data\u00a0dirty\u00a0(different\u00a0than\u00a0memory)?\";\n\u00a0\u00a0\u00a0DataBlock\u00a0DataBlk,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0desc=\"Data\u00a0in\u00a0the\u00a0block\";\n}\nThe cool thing about using SLICC\u2019s structure is that it automatically\ngenerates for you the get and set functions on different fields. It also\nwrites a nice print function and overloads the << operator. But in\ncase you would prefer do everything on your own, you can make use of the\nkeyword\nexternal\nin the declaration of the structure. This would\nprevent SLICC from generating C++ code for this structure.\nstructure(TBETable,\u00a0external=\"yes\")\u00a0{\n\u00a0\u00a0\u00a0TBE\u00a0lookup(Address);\n\u00a0\u00a0\u00a0void\u00a0allocate(Address);\n\u00a0\u00a0\u00a0void\u00a0deallocate(Address);\n\u00a0\u00a0\u00a0bool\u00a0isPresent(Address);\n}\nIn fact many predefined types exist in src/mem/protocol/RubySlicc_*.sm\nfiles. You can make use of them, or if you need new types, you can\ndefine new ones as well. You can also use the keyword\ninterface\nto\nmake use of inheritance features available in C++. Note that currently\nSLICC supports public inheritance only.\nWe can also declare and define functions as we do in C++. There are\ncertain functions that the compiler expects would always be defined\nby the controller. These include\ngetState()\nsetState()\nInput for the Machine\nSince protocol is state machine, we need to specify how to machine\ntransitions from one state to another on receiving inputs. As mentioned\nbefore, each machine has several input and output ports. For each input\nport, the\nin_port\nkeyword is used for specifying the behavior of\nthe machine, when a message is received on that input port. An example\nfollows that shows the syntax for declaring an input\nport.\nin_port(mandatoryQueue_in,\u00a0RubyRequest,\u00a0mandatoryQueue,\u00a0desc=\"...\")\u00a0{\n\u00a0\u00a0if\u00a0(mandatoryQueue_in.isReady())\u00a0{\n\u00a0\u00a0\u00a0\u00a0peek(mandatoryQueue_in,\u00a0RubyRequest,\u00a0block_on=\"LineAddress\")\u00a0{\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Entry\u00a0cache_entry\u00a0:=\u00a0getCacheEntry(in_msg.LineAddress);\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0if\u00a0(is_invalid(cache_entry)\u00a0&&\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0cacheMemory.cacheAvail(in_msg.LineAddress)\u00a0==\u00a0false\u00a0)\u00a0{\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0//\u00a0make\u00a0room\u00a0for\u00a0the\u00a0block\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0trigger(Event:Replacement,\u00a0cacheMemory.cacheProbe(in_msg.LineAddress),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0getCacheEntry(cacheMemory.cacheProbe(in_msg.LineAddress)),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0TBEs[cacheMemory.cacheProbe(in_msg.LineAddress)]);\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0}\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0else\u00a0{\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0trigger(mandatory_request_type_to_event(in_msg.Type),\u00a0in_msg.LineAddress,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0cache_entry,\u00a0TBEs[in_msg.LineAddress]);\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0}\n\u00a0\u00a0\u00a0\u00a0}\n\u00a0\u00a0}\n}\nAs you can see, in_port takes in multiple arguments. The first\nargument, mandatoryQueue_in, is the identifier for the in_port\nthat is used in the file. The next argument, RubyRequest, is the\ntype of the messages that this input port receives. Each input port\nuses a queue to store the messages, the name of the queue is the\nthird argument.\nThe keyword\npeek\nis used to extract messages from the queue of\nthe input port. The use of this keyword implicitly declares a\nvariable\nin_msg\nwhich is of the same type as specified in the\ninput port\u2019s declaration. This variable points to the message at the\nhead of the queue. It can be used for accessing the fields of the\nmessage as shown in the code above.\nOnce the incoming message has been analyzed, it is time for using\nthis message for taking some appropriate action and changing the\nstate of the machine. This done using the keyword\ntrigger\n. The\ntrigger function is actually used only in SLICC code and is not\npresent in the generated code. Instead this call is converted in to\na call to the\ndoTransition()\nfunction which appears in the\ngenerated code. The doTransition() function is automatically\ngenerated by SLICC for each of the state machines. The number of\narguments to trigger depend on the machine itself. In general, the\ninput arguments for trigger are the type of the message that needs\nto processed, the address for which this message is meant for, the\ncache and the transaction buffer entries for that address.\ntrigger\nalso increments a counter that is checked before a\ntransition is made. In one ruby cycle, there is a limit on the\nnumber of transitions that can be carried out. This is done to\nresemble more closely to a hardware based state machine.\n@TODO:\nWhat happens if there are no more transitions left? Does the wakeup\nabort?\nActions\nIn this section we will go over how the actions that a state machine can\ncarry out are defined. These actions will be called in to action when\nthe state machine receives some input message which is then used to make\na transition. Let\u2019s go over an example on how the key word\naction\ncan be made use of.\naction(a_issueRequest,\u00a0\"a\",\u00a0desc=\"Issue\u00a0a\u00a0request\")\u00a0{\n\u00a0\u00a0\u00a0enqueue(requestNetwork_out,\u00a0RequestMsg,\u00a0latency=issue_latency)\u00a0{\n\u00a0\u00a0\u00a0out_msg.Address\u00a0:=\u00a0address;\n\u00a0\u00a0\u00a0\u00a0\u00a0out_msg.Type\u00a0:=\u00a0CoherenceRequestType:GETX;\n\u00a0\u00a0\u00a0\u00a0\u00a0out_msg.Requestor\u00a0:=\u00a0machineID;\n\u00a0\u00a0\u00a0\u00a0\u00a0out_msg.Destination.add(map_Address_to_Directory(address));\n\u00a0\u00a0\u00a0\u00a0\u00a0out_msg.MessageSize\u00a0:=\u00a0MessageSizeType:Control;\n\u00a0\u00a0\u00a0}\n}\nThe first input argument is the name of the action, the next\nargument is the abbreviation used for generating the documentation\nand last one is the description of the action which used in the HTML\ndocumentation and as a comment in the C++ code.\nEach action is converted in to a C++ function of that name. The\ngenerated C++ code implicitly includes up to three input parameters\nin the function header, again depending on the machine. These\narguments are the memory address on which the action is being taken,\nthe cache and transaction buffer entries pertaining to this address.\nNext useful thing to look at is the\nenqueue\nkeyword. This\nkeyword is used for queuing a message, generated as a result of the\naction, to an output port. The keyword takes three input arguments,\nnamely, the name of the output port, the type of the message to be\nqueued and the latency after which this message can be dequeued.\nNote that in case randomization is enabled, the specified latency is\nignored. The use of the keyword implicitly declares a variable\nout_msg which is populated by the follow on statements.\nTransitions\nA transition function is a mapping from the cross product of set of\nstates and set of events to the set of states. SLICC provides the\nkeyword\ntransition\nfor specifying the transition function for state\nmachines. An example follows \u2013\ntransition(IM,\u00a0Data,\u00a0M)\u00a0{\n\u00a0\u00a0\u00a0u_writeDataToCache;\n\u00a0\u00a0\u00a0sx_store_hit;\n\u00a0\u00a0\u00a0w_deallocateTBE;\n\u00a0\u00a0\u00a0n_popResponseQueue;\n}\nIn this example, the initial state is\nIM\n. If an event of type\nData\noccurs in that state, then final state would be\nM\n. Before making the\ntransition, the state machine can perform certain actions on the\nstructures that it maintains. In the given example,\nu_writeDataToCache\nis an action. All these operations are performed\nin an atomic fashion, i.e. no other event can occur before the set of\nactions specified with the transition has been completed.\nFor ease of use, sets of events and states can be provided as input\nto transition. The cross product of these sets will map to the same\nfinal state. Note that the final state cannot be a set. If for a\nparticular event, the final state is same as the initial state, then\nthe final state can be omitted.\ntransition({IS,\u00a0IM,\u00a0MI,\u00a0II},\u00a0{Load,\u00a0Ifetch,\u00a0Store,\u00a0Replacement})\u00a0{\n\u00a0\u00a0\u00a0z_stall;\n}\nSpecial Functions\nStalling/Recycling/Waiting input ports\nOne of the more complicated internal features of SLICC and the resulting\nstate machines is how the deal with the situation when events cannot be\nprocess due to the cache block being in a transient state. There are\nseveral possible ways to deal with this situation and each solution has\ndifferent tradeoffs. This sub-section attempts to explain the\ndifferences. Please email the gem5-user list for further follow-up.\nStalling the input port\nThe simplest way to handle events that can\u2019t be processed is to simply\nstall the input port. The correct way to do this is to include the\n\u201cz_stall\u201d action within the transition statement:\ntransition({IS,\u00a0IM,\u00a0MI,\u00a0II},\u00a0{Load,\u00a0Ifetch,\u00a0Store,\u00a0Replacement})\u00a0{\n\u00a0\u00a0\u00a0z_stall;\n}\nInternally SLICC will return a ProtocolStall for this transition and no\nsubsequent messages from the associated input port will be processed\nuntil the stalled message is processed. However, the other input ports\nwill be analyzed for ready messages and processed in parallel. While\nthis is a relatively simple solution, one may notice that stalling\nunrelated messages on the same input port will cause excessive and\nunnecessary stalls.\nOne thing to note is\nDo Not\nleave the transition statement blank\nlike so:\ntransition({IS,\u00a0IM,\u00a0MI,\u00a0II},\u00a0{Load,\u00a0Ifetch,\u00a0Store,\u00a0Replacement})\u00a0{\n\u00a0\u00a0\u00a0//\u00a0stall\u00a0the\u00a0input\u00a0port\u00a0by\u00a0simply\u00a0not\u00a0popping\u00a0the\u00a0message\n}\nThis will cause SLICC to return success for this transition and SLICC\nwill continue to repeatedly analyze the same input port. The result is\neventual deadlock.\nRecycling the input port\nThe better performance but more unrealistic solution is to recycle the\nstalled message on the input port. The way to do this is to use the\n\u201czz_recycleMandatoryQueue\u201d\naction:\naction(zz_recycleMandatoryQueue,\u00a0\"\\z\",\u00a0desc=\"Send\u00a0the\u00a0head\u00a0of\u00a0the\u00a0mandatory\u00a0queue\u00a0to\u00a0the\u00a0back\u00a0of\u00a0the\u00a0queue.\")\u00a0{\n\u00a0\u00a0\u00a0mandatoryQueue_in.recycle();\n}\ntransition({IS,\u00a0IM,\u00a0MI,\u00a0II},\u00a0{Load,\u00a0Ifetch,\u00a0Store,\u00a0Replacement})\u00a0{\n\u00a0\u00a0\u00a0zz_recycleMandatoryQueue;\n}\nThe result of this action is that the transition returns a Protocol\nStall and the offending message moved to the back of the FIFO input\nport. Therefore, other unrelated messages on the same input port can be\nprocessed. The problem with this solution is that recycled messages may\nbe analyzed and reanalyzed every cycle until an address changes state.\nStall and wait the input port\nAn even better, but more complicated solution is to \u201cstall and wait\u201d the\noffending input message. The way to do this is to use the\n\u201cz_stallAndWaitMandatoryQueue\u201d\naction:\naction(z_stallAndWaitMandatoryQueue,\u00a0\"\\z\",\u00a0desc=\"recycle\u00a0L1\u00a0request\u00a0queue\")\u00a0{\n\u00a0\u00a0\u00a0stall_and_wait(mandatoryQueue_in,\u00a0address);\n}\ntransition({IS,\u00a0IM,\u00a0IS_I,\u00a0M_I,\u00a0SM,\u00a0SINK_WB_ACK},\u00a0{Load,\u00a0Ifetch,\u00a0Store,\u00a0L1_Replacement})\u00a0{\n\u00a0\u00a0\u00a0z_stallAndWaitMandatoryQueue;\n}\nThe result of this action is that the transition returns success, which\nis ok because stall_and_wait moves the offending message off the input\nport and to a side table associated with the input port. The message\nwill not be analyzed again until it is woken up. In the meantime, other\nunrelated messages will be processed.\nThe complicated part of stall and wait is that stalled messages must be\nexplicitly woken up by other messages/transitions. In particular,\ntransitions that move an address to a base state should wake up\npotentially stalled messages waiting for that address:\naction(kd_wakeUpDependents,\u00a0\"kd\",\u00a0desc=\"wake-up\u00a0dependents\")\u00a0{\n\u00a0\u00a0\u00a0wakeUpBuffers(address);\n}\ntransition(M_I,\u00a0WB_Ack,\u00a0I)\u00a0{\n\u00a0\u00a0\u00a0s_deallocateTBE;\n\u00a0\u00a0\u00a0o_popIncomingResponseQueue;\n\u00a0\u00a0\u00a0kd_wakeUpDependents;\n}\nReplacements are particularly complicated since stalled addresses are\nnot associated with the same address they are actually waiting to\nchange. In those situations all waiting messages must be woken\nup:\naction(ka_wakeUpAllDependents,\u00a0\"ka\",\u00a0desc=\"wake-up\u00a0all\u00a0dependents\")\u00a0{\n\u00a0\u00a0\u00a0wakeUpAllBuffers();\n}\ntransition(I,\u00a0L2_Replacement)\u00a0{\n\u00a0\u00a0\u00a0rr_deallocateL2CacheBlock;\n\u00a0\u00a0\u00a0ka_wakeUpAllDependents;\n}\nOther Compiler Features\nSLICC supports conditional statements in form of\nif\nand\nelse\n. Note that SLICC does not support\nelse if\n.\nEach function has return type which can be void as well. Returned\nvalues cannot be ignored.\nSLICC has limited support for pointer variables. is_valid() and\nis_invalid() operations are supported for testing whether a given\npointer \u2018is not NULL\u2019 and \u2018is NULL\u2019 respectively. The keyword\nOOD\n, which stands for Out of Domain, plays the role of keyword\nNULL used in C++.\nSLICC does not support\n!\n(the not operator).\nStatic type casting is supported in SLICC. The keyword\nstatic_cast\nhas been provided for this purpose. For example, in\nthe following piece of code, a variable of type AbstractCacheEntry\nis being casted in to a variable of type Entry.\nEntry\u00a0L1Dcache_entry\u00a0:=\u00a0static_cast(Entry,\u00a0\"pointer\",\u00a0L1DcacheMemory[addr]);\nSLICC Internals\nC++ to Slicc Interface - @note: What do each of these files\ndo/define???\nsrc/mem/protocol/RubySlicc_interaces.sm\nRubySlicc_Exports.sm\nRubySlicc_Defines.sm\nRubySlicc_Profiler.sm\nRubySlicc_Types.sm\nRubySlicc_MemControl.sm\nRubySlicc_ComponentMapping.sm\nVariable Assignments\nUse the\n:=\noperator to assign members in class (e.g. a member\ndefined in RubySlicc_Types.sm):\nan automatic\nm_\nis added to the name mentioned in the SLICC\n  file.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/ruby/slicc",
            "page_title": "SLICC",
            "parent_section": "Overview",
            "section_heading": "slicc"
        }
    },
    {
        "text": "Scalar\nThe most basic stat is the Scalar. This embodies the basic counting stat. It is a templatized stat and takes two parameters, a type and a bin. The default type is a Counter, and the default bin is NoBin (i.e. there is no binning on this stat). It\u2019s usage is straightforward: to assign a value to it, just say foo = 10;, or to increment it, just use ++ or += like for any other type.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/statistics/",
            "page_title": "Stats Package",
            "parent_section": "Scalar",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Average\nThis is a \u201cspecial use\u201d stat, geared toward calculating the average of something over the number of cycles in the simulation. This stat is best explained by example. If you wanted to know the average occupancy of the load-store queue over the course of the simulation, you\u2019d need to accumulate the number of instructions in the LSQ each cycle and at the end divide it by the number of cycles. For this stat, there may be many cycles where there is no change in the LSQ occupancy. Thus, you could use this stat, where you only need to explicitly update the stat when there is a change in the LSQ occupancy. The stat itself will take care of itself for cycles where there is no change. This stat can be binned and it also templatized the same way Stat is.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/statistics/",
            "page_title": "Stats Package",
            "parent_section": "Average",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Vector\nA Vector is just what it sounds like, a vector of type T in the template parameters. It can also be binned. The most natural use of Vector is for something like tracking some stat over number of SMT threads. A Vector of size n can be declared just by saying Vector<> foo; and later initializing the size to n. At that point, foo can be accessed as if it were a regular vector or array, like foo[7]++.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/statistics/",
            "page_title": "Stats Package",
            "parent_section": "Vector",
            "section_heading": "Overview"
        }
    },
    {
        "text": "AverageVector\nAn AverageVector is just a Vector of Averages.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/statistics/",
            "page_title": "Stats Package",
            "parent_section": "AverageVector",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Vector2d\nA Vector2d is a 2 dimensional vector. It can be named in both the x and y directions, though the primary name is given across the x-dimension. To name in the y-dimension, use a special ysubname function only available to Vector2d\u2019s.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/statistics/",
            "page_title": "Stats Package",
            "parent_section": "Vector2d",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Distribution\nThis is essentially a Vector, but with minor differences. Whereas in a Vector, the index maps to the item of interest for that bucket, in a Distribution you could map different ranges of interest to a bucket. Basically, if you had the bkt parameter of init for a Distribution = 1, you might as well use a Vector.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/statistics/",
            "page_title": "Stats Package",
            "parent_section": "Distribution",
            "section_heading": "Overview"
        }
    },
    {
        "text": "StandardDeviation\nThis stat calculates standard deviation over number of cycles in the simulation. It\u2019s similar to Average in that it has behavior built into it, but it needs to be updated every cycle.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/statistics/",
            "page_title": "Stats Package",
            "parent_section": "StandardDeviation",
            "section_heading": "Overview"
        }
    },
    {
        "text": "AverageDeviation\nThis stat also calculates the standard deviation but it does not need to be updated every cycle, much like Average. It will handle cycles where there is no change itself.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/statistics/",
            "page_title": "Stats Package",
            "parent_section": "AverageDeviation",
            "section_heading": "Overview"
        }
    },
    {
        "text": "VectorDistribution\nThis is just a vector of distributions.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/statistics/",
            "page_title": "Stats Package",
            "parent_section": "VectorDistribution",
            "section_heading": "Overview"
        }
    },
    {
        "text": "VectorStandardDeviation\nThis is just a vector of standard deviations.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/statistics/",
            "page_title": "Stats Package",
            "parent_section": "VectorStandardDeviation",
            "section_heading": "Overview"
        }
    },
    {
        "text": "VectorAverageDeviation\nThis is just a vector of AverageDeviations.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/statistics/",
            "page_title": "Stats Package",
            "parent_section": "VectorAverageDeviation",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Histogram\nThis stat puts each sampled value into one bin out of a configurable number of bins. All bins form a contiguous interval and are of equal length. The length of the bins is dynamically extended, if there is a sample value which does not fit into one the existing bins.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/statistics/",
            "page_title": "Stats Package",
            "parent_section": "Histogram",
            "section_heading": "Overview"
        }
    },
    {
        "text": "SparseHistogram\nThis stat is similar to a histogram, except that it can only sample natural numbers. SparseHistogram is e.g. suitable for counting the number of accesses to memory addresses.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/statistics/",
            "page_title": "Stats Package",
            "parent_section": "SparseHistogram",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Formula\nThis is a Formula stat. This is for anything that requires calculations at the end of the simulation, for example something that is a rate. So, an example of defining a Formula would be:\nFormula foo = bar + 10 / num;\nThere are a few subtleties to Formula. If bar and num are both stats(including Formula type), then there is no problem. If bar or num are regular variables, then they must be qualified with constant(bar). This is essentially cast. If you want to use the value of bar or num at the moment of definition, then use constant(). If you want to use the value of bar or num at the moment the formula is calculated (i.e. the end), define num as a Scalar. If num is a Vector, use sum(num) to calculate its sum for the formula. The operation \u201cscalar(num)\u201d, which casts a regular variable to a Scalar, does no longer exist.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/statistics/",
            "page_title": "Stats Package",
            "parent_section": "Formula",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Contents\nGeneral Statistics Functions\nStats::Group - Statistics Container\nStats Flags\nStatistic Classes\nAppendix: Migrating to the new style of tracking statistics",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/statistics/api",
            "page_title": "Statistics APIs",
            "parent_section": "Contents",
            "section_heading": "Overview"
        }
    },
    {
        "text": "General Statistics Functions\nFunction signatures\nDescriptions\nvoid Stats::dump()\nDump all stats to registered outputs, e.g. stats.txt.\nvoid Stats::reset()\nReset stats.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/statistics/api",
            "page_title": "Statistics APIs",
            "parent_section": "General Statistics Functions",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Section: Stats::Group - Statistics Container\n\nSub-section: Stats::Group macros\n\n#define ADD_STAT(n, ...) n(this, # n, __VA_ARGS__)\nConvenience macro to add a stat to a statistics group.\nThis macro is used to add a stat to a Stats::Group in the\ninitilization list in the Group\u2019s constructor. The macro\nautomatically assigns the stat to the current group and gives it\nthe same name as in the class. For example:\nstruct MyStats : public Stats::Group\n{\n    Stats::Scalar scalar0;\n    Stats::Scalar scalar1;\n\n    MyStats(Stats::Group *parent)\n        : Stats::Group(parent),\n          ADD_STAT(scalar0, \"Description of scalar0\"),       // equivalent to scalar0(this, \"scalar0\", \"Description of scalar0\"), where scalar0 has the follwing constructor\n                                                             // Stats::Scalar(Group *parent = nullptr, const char *name = nullptr, const char *desc = nullptr)\n          scalar1(this, \"scalar1\", \"Description of scalar1\")\n     {\n     }\n};",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/statistics/api",
            "page_title": "Statistics APIs",
            "parent_section": "Stats::Group - Statistics Container",
            "section_heading": "Stats::Group macros"
        }
    },
    {
        "text": "Section: Stats::Group - Statistics Container\n\nSub-section: Stats::Group functions\n\nGroup(Group *parent, const char *name = nullptr)\nConstruct a new statistics group.\nThe constructor takes two parameters, a parent and a name. The\nparent group should typically be specified. However, there are\nspecial cases where the parent group may be null. One such\nspecial case is SimObjects where the Python code performs late\nbinding of the group parent.\nIf the name parameter is NULL, the group gets merged into the\nparent group instead of creating a sub-group. Stats belonging\nto a merged group behave as if they have been added directly to\nthe parent group.\nvirtual void regStats()\nCallback to set stat parameters.\nThis callback is typically used for complex stats (e.g.,\ndistributions) that need parameters in addition to a name and a\ndescription. In the case stats objects cannot be initilalized\nin the constructor (such as the stats that keep track of the\nbus masters, which only can be discovered after the entire\nsystem is instantiated). Stat names and descriptions should\ntypically be set from the constructor using the\nADD_STAT\nmacro.\nvirtual void resetStats()\nCallback to reset stats.\nvirtual void preDumpStats()\nCallback before stats are dumped. This can be overridden by\nobjects that need to perform calculations in addition to the\ncapabiltiies implemented in the stat framework.\nvoid addStat(Stats::Info *info)\nRegister a stat with this group. This method is normally called\nautomatically when a stat is instantiated.\nconst std::map<std::string, Group *> &getStatGroups() const\nGet all child groups associated with this object.\nconst std::vector<Info *> &getStats() const\nGet all stats associated with this object.\nvoid addStatGroup(const char *name, Group *block)\nAdd a stat block as a child of this block.\nThis method may only be called from a Group constructor or from\nregStats. It\u2019s typically only called explicitly from Python\nwhen setting up the SimObject hierarchy.\nconst Info * resolveStat(std::string name) const\nResolve a stat by its name within this group.\nThis method goes through the stats in this group and sub-groups\nand returns a pointer to the the stat that matches the provided\nname. The input name has to be relative to the name of this\ngroup.\nFor example, if this group is the\nSimObject\nsystem.bigCluster.cpus\nand we want the stat\nsystem.bigCluster.cpus.ipc\n, the input param should be the\nstring \u201cipc\u201d.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/statistics/api",
            "page_title": "Statistics APIs",
            "parent_section": "Stats::Group - Statistics Container",
            "section_heading": "Stats::Group functions"
        }
    },
    {
        "text": "Stats Flags\nFlags\nDescriptions\nStats::none\nNothing extra to print.\nStats::total\nPrint the total.\nStats::pdf\nPrint the percent of the total that this entry represents.\nStats::cdf\nPrint the cumulative percentage of total upto this entry.\nStats::dist\nPrint the distribution.\nStats::nozero\nDon\u2019t print if this is zero.\nStats::nonan\nDon\u2019t print if this is NAN\nStats::oneline\nPrint all values on a single line. Useful only for histograms.\nNote: even though the flags\nStats::init\nand\nStats::display\nare available, the flags\nare not allowed to be set by users.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/statistics/api",
            "page_title": "Statistics APIs",
            "parent_section": "Stats Flags",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Section: Statistics Classes\n\nSub-section: Common statistic functions\n\nFunction signatures\nDescriptions\nStatClass name(const std::string &name)\nsets the statistic name, marks the stats to be printed\nStatClass desc(const std::string &_desc)\nsets the description for the statistic\nStatClass precision(int _precision)\nsets the precision of the statistic\nStatClass flags(Flags _flags)\nsets the flags\nStatClass prereq(const Stat &prereq)\nsets the prerequisite stat",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/statistics/api",
            "page_title": "Statistics APIs",
            "parent_section": "Statistics Classes",
            "section_heading": "Common statistic functions"
        }
    },
    {
        "text": "Section: Statistics Classes\n\nSub-section: Stats::Scalar\n\nStoring a signed integer statistic.\nFunction signatures\nDescriptions\nvoid operator++()\nincrements the stat by 1 // prefix ++, e.g.\n++scalar\nvoid operator--()\ndecrements the stat by 1 // prefix \u2013\nvoid operator++(int)\nincrements the stat by 1 // postfix ++, e.g.\nscalar++\nvoid operator--(int)\ndecrements the stat by 1 // postfix \u2013\ntemplate <typename U> void operator=(const U &v)\nsets the scalar to the given value\ntemplate <typename U> void operator+=(const U &v)\nincrements the stat by the given value\ntemplate <typename U> void operator-=(const U &v)\ndecrements the stat by the given value\nsize_type size()\nreturns 1\nCounter value()\nreturns the current value of the stat as an integer\nCounter value() const\nreturns the current value of the stat as an integer\nResult result()\nreturns the current value of the stat as a\ndouble\nResult total()\nreturns the current value of the stat as a\ndouble\nbool zero()\nreturns\ntrue\nif the stat equals to zero, returns\nfalse\notherwise\nvoid reset()\nresets the stat to 0",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/statistics/api",
            "page_title": "Statistics APIs",
            "parent_section": "Statistics Classes",
            "section_heading": "Stats::Scalar"
        }
    },
    {
        "text": "Section: Statistics Classes\n\nSub-section: Stats::Average\n\nStoring an average of an integer quantity, supposely A, over the number of simulated ticks.\nThe quantity A keeps the same value across all ticks after its latest update and before the next update.\nNote:\nthe number of simulated ticks is reset when the user calls\nStats::reset()\n.\nFunction signatures\nDescriptions\nvoid set(Counter val)\nsets the quantity A to the given value\nvoid inc(Counter val)\nincrements the quantity A by the given value\nvoid dec(Counter val)\ndecrements the quantity A by the given value\nCounter value()\nreturns the current value of A as an integer\nResult result()\nreturns the current average as a\ndouble\nbool zero()\nreturns\ntrue\nif the average equals to zero, returns\nfalse\notherwise\nvoid reset(Info \\*info)\nkeeps the current value of A, does not count the value of A before the current tick",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/statistics/api",
            "page_title": "Statistics APIs",
            "parent_section": "Statistics Classes",
            "section_heading": "Stats::Average"
        }
    },
    {
        "text": "Section: Statistics Classes\n\nSub-section: Stats::Value\n\nStoring a signed integer statistic that is either an integer or an integer that is a result from calling a function or an object\u2019s method.\nFunction signatures\nDescriptions\nCounter value()\nreturns the value as an integer\nResult result() const\nreturns the value as a double\nResult total() const\nreturns the value as a double\nsize_type size() const\nreturns 1\nbool zero() const\nreturns\ntrue\nif the value is zero, returns\nfalse\notherwise",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/statistics/api",
            "page_title": "Statistics APIs",
            "parent_section": "Statistics Classes",
            "section_heading": "Stats::Value"
        }
    },
    {
        "text": "Section: Statistics Classes\n\nSub-section: Stats::Vector\n\nStoring an array of scalar statistics where each element of the vector has function signatures similar to those of\nStats::Scalar\n.\nFunction signatures\nDescriptions\nDerived & init(size_type size)\ninitializes the vector to the given size (throws an error if attempting to resize an initilized vector)\nDerived & subname(off_type index, const std::string &name)\nadds a name to the statistic at the given index\nDerived & subdesc(off_type index, const std::string &desc)\nadds a description to the statistic at the given index\nvoid value(VCounter &vec) const\ncopies the vector of statistics to the given vector of integers\nvoid result(VResult &vec) const\ncopies the vector of statistics to the given vector of doubles\nResult total() const\nreturns the sum of all statistics in the vector as a double\nsize_type size() const\nreturns the size of the vector\nbool zero() const\nreturns\ntrue\nif each statistic in the vector is 0, returns\nfalse\notherwise\noperator[](off_type index)\ngets the reference to the statistic at the given index, e.g.\nvecStats[1]+=9",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/statistics/api",
            "page_title": "Statistics APIs",
            "parent_section": "Statistics Classes",
            "section_heading": "Stats::Vector"
        }
    },
    {
        "text": "Section: Statistics Classes\n\nSub-section: Stats::AverageVector\n\nStoring an array of average statistics where each element of the vector has function signatures similar to those of\nStats::Average\n.\nFunction signatures\nDescriptions\nDerived & init(size_type size)\ninitializes the vector to the given size (throws an error if attempting to resize an initilized vector)\nDerived & subname(off_type index, const std::string &name)\nadds a name to the statistic at the given index\nDerived & subdesc(off_type index, const std::string &desc)\nadds a description to the statistic at the given index\nvoid value(VCounter &vec) const\ncopies the vector of statistics to the given vector of integers\nvoid result(VResult &vec) const\ncopies the vector of statistics to the given vector of doubles\nResult total() const\nreturns the sum of all statistics in the vector as a double\nsize_type size() const\nreturns the size of the vector\nbool zero() const\nreturns\ntrue\nif each statistic in the vector is 0, returns\nfalse\notherwise\noperator[](off_type index)\ngets the reference to the statistic at the given index, e.g.\navgStats[1].set(9)",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/statistics/api",
            "page_title": "Statistics APIs",
            "parent_section": "Statistics Classes",
            "section_heading": "Stats::AverageVector"
        }
    },
    {
        "text": "Section: Statistics Classes\n\nSub-section: Stats::Vector2d\n\nStoring a 2-dimensional array of scalar statistics, where each element of the array has function signatures similar to those of\nStats::Scalar\n.\nThis data structure assumes all elements whose the same second dimension index has the same name.\nFunction signatures\nDescriptions\nDerived & init(size_type _x, size_type _y)\ninitializes the vector to the given size (throws an error if attempting to resize an initilized vector)\nDerived & ysubname(off_type index, const std::string &subname)\nsets\nsubname\nas the name of the statistics of elements whose the second dimension of\nindex\nDerived & ysubnames(const char **names)\nsimilar to\nysubname()\nabove, but sets name for all indices of the second dimension\nstd::string ysubname(off_type i) const\nreturns the name of the statistics of elements whose the second dimension of\ni\nsize_type size() const\nreturns the number of elements in the array\nbool zero()\nreturns\ntrue\nif the element at row 0 column 0 equals to 0, returns\nfalse\notherwise\nResult total()\nreturns the sum of all elements as a double\nvoid reset()\nsets each element in the array to 0\noperator[](off_type index)\ngets the reference to the statistic at the given index, e.g.\nvecStats[1][2]+=9",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/statistics/api",
            "page_title": "Statistics APIs",
            "parent_section": "Statistics Classes",
            "section_heading": "Stats::Vector2d"
        }
    },
    {
        "text": "Section: Statistics Classes\n\nSub-section: Stats::Distribution\n\nStoring a distribution of a quantity.\nThe statistics of the distribution include,\nthe smallest/largest value being sampled\nthe number of values that are smaller/larger than the specified minimum and maximum\nthe sum of all samples\nthe mean, the geometric mean and the standard deviation of the samples\nhistogram within the range of [\nmin\n,\nmax\n] splitted into\n(max-min)/bucket_size\nequally sized buckets,  where the\nmin\n/\nmax\n/\nbucket_size\nare inputs to the init() function.\nFunction signatures\nDescriptions\nDistribution & init(Counter min, Counter max, Counter bkt)\ninitializes the distribution where\nmin\nis the minimum value being tracked by the distribution\u2019s histogram,\nmax\nis the minimum value being tracked by the distribution\u2019s histogram, and\nbkt\nis the number of values in each bucket\nvoid sample(Counter val, int number)\nadds\nval\nto the distribution\nnumber\ntimes\nsize_type size() const\nreturns the number of bucket in the distribution\nbool zero() const\nreturns\ntrue\nif the number of samples is zero, returns\nfalse\notherwise\nvoid reset(Info *info)\ndiscards all samples\nadd(DistBase &)\nmerges the samples from another\nStats\nclass with\nDistBase\n(e.g.\nStats::Histogram\n)",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/statistics/api",
            "page_title": "Statistics APIs",
            "parent_section": "Statistics Classes",
            "section_heading": "Stats::Distribution"
        }
    },
    {
        "text": "Section: Statistics Classes\n\nSub-section: Stats::Histogram\n\nStoring a histogram of a quantity given the number of buckets.\nAll buckets are equally sized.\nDifferent from the histogram of\nStats::Distribution\nwhich keeps track of the samples in a specific range,\nStats::Histogram\nkeeps track of all samples in its histogram.\nAlso, while\nStats::Distribution\nis parameterized by the number of values in a bucket,\nStats::Histogram\n\u2019s sole parameter is the number of buckets.\nWhen a new sample is outside of the current range of all all buckets, the buckets will be resized.\nRoughly, two consecutive buckets will be merged until the new sample is inside one of the buckets.\nOther than the histogram itself, the statistics of the distribution include,\nthe smallest/largest value being sampled\nthe sum of all samples\nthe mean, the geometric mean and the standard deviation of the samples\nFunction signatures\nDescriptions\nHistogram & init(size_type size)\ninitializes the histogram, sets the number of buckets to\nsize\nvoid sample(Counter val, int number)\nadds\nval\nto the histogram\nnumber\ntimes\nvoid add(HistStor *)\nmerges another histogram to this histogram\nsize_type size() const\nreturns the number of buckets\nbool zero() const\nreturns\ntrue\nif the number of samples is zero, returns\nfalse\notherwise\nvoid reset(Info *info)\ndiscards all samples",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/statistics/api",
            "page_title": "Statistics APIs",
            "parent_section": "Statistics Classes",
            "section_heading": "Stats::Histogram"
        }
    },
    {
        "text": "Section: Statistics Classes\n\nSub-section: Stats::SparseHistogram\n\nStoring a histogram of a quantity given a set of integral values.\nFunction signatures\nDescriptions\ntemplate <typename U> void sample(const U &v, int n = 1)\nadds\nv\nto the histogram\nn\ntimes\nsize_type size() const\nreturns the number of entries\nbool zero() const\nreturns\ntrue\nif the number of samples is zero, returns\nfalse\notherwise\nvoid reset()\ndiscards all samples",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/statistics/api",
            "page_title": "Statistics APIs",
            "parent_section": "Statistics Classes",
            "section_heading": "Stats::SparseHistogram"
        }
    },
    {
        "text": "Section: Statistics Classes\n\nSub-section: Stats::StandardDeviation\n\nKeeps track of the standard deviation of a sample.\nFunction signatures\nDescriptions\nvoid sample(Counter val, int number)\nadds\nval\nto the distribution\nnumber\ntimes\nsize_type size() const\nreturns 1\nbool zeros() const\ndiscards all samples\nadd(DistBase &)\nmerges the samples from another\nStats\nclass with\nDistBase\n(e.g.\nStats::Distribution",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/statistics/api",
            "page_title": "Statistics APIs",
            "parent_section": "Statistics Classes",
            "section_heading": "Stats::StandardDeviation"
        }
    },
    {
        "text": "Section: Statistics Classes\n\nSub-section: Stats::AverageDeviation\n\nKeeps track of the average deviation of a sample.\nFunction signatures\nDescriptions\nvoid sample(Counter val, int number)\nadds\nval\nto the distribution\nnumber\ntimes\nsize_type size() const\nreturns 1\nbool zeros() const\ndiscards all samples\nadd(DistBase &)\nmerges the samples from another\nStats\nclass with\nDistBase\n(e.g.\nStats::Distribution",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/statistics/api",
            "page_title": "Statistics APIs",
            "parent_section": "Statistics Classes",
            "section_heading": "Stats::AverageDeviation"
        }
    },
    {
        "text": "Section: Statistics Classes\n\nSub-section: Stats::VectorDistribution\n\nStoring a vector of distributions where each element of the vector has function signatures similar to those of\nStats::Distribution\n.\nFunction signatures\nDescriptions\nVectorDistribution & init(size_type size, Counter min, Counter max, Counter bkt)\ninitializes a vector of\nsize\ndistributions where\nmin\nis the minimum value being tracked by each distribution\u2019s histogram,\nmax\nis the minimum value being tracked by each distribution\u2019s histogram, and\nbkt\nis each distribution\u2019s the number of values in each bucket\nDerived & subname(off_type index, const std::string &name)\nadds a name to the statistic at the given index\nDerived & subdesc(off_type index, const std::string &desc)\nadds a description to the statistic at the given index\nsize_type size() const\nreturns the number of elements in the vector\nbool zero() const\nreturns\ntrue\nif each of distributions has 0 samples, return\nfalse\notherwise\noperator[](off_type index)\ngets the reference to the distribution at the given index, e.g.\ndists[1].sample(2,3)",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/statistics/api",
            "page_title": "Statistics APIs",
            "parent_section": "Statistics Classes",
            "section_heading": "Stats::VectorDistribution"
        }
    },
    {
        "text": "Section: Statistics Classes\n\nSub-section: Stats::VectorStandardDeviation\n\nStoring a vector of standard deviations where each element of the vector has function signatures similar to those of\nStats::StandardDeviation\n.\nFunction signatures\nDescriptions\nVectorStandardDeviation & init(size_type size)\ninitializes a vector of\nsize\nstandard deviations\nDerived & subname(off_type index, const std::string &name)\nadds a name to the statistic at the given index\nDerived & subdesc(off_type index, const std::string &desc)\nadds a description to the statistic at the given index\nsize_type size() const\nreturns the number of elements in the vector\nbool zero() const\nreturns\ntrue\nif each of distributions has 0 samples, return\nfalse\notherwise\noperator[](off_type index)\ngets the reference to the standard deviation at the given index, e.g.\ndists[1].sample(2,3)",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/statistics/api",
            "page_title": "Statistics APIs",
            "parent_section": "Statistics Classes",
            "section_heading": "Stats::VectorStandardDeviation"
        }
    },
    {
        "text": "Section: Statistics Classes\n\nSub-section: Stats::VectorAverageDeviation\n\nStoring a vector of average deviations where each element of the vector has function signatures similar to those of\nStats::AverageDeviation\n.\nFunction signatures\nDescriptions\nVectorAverageDeviation & init(size_type size)\ninitializes a vector of\nsize\naverage deviations\nDerived & subname(off_type index, const std::string &name)\nadds a name to the statistic at the given index\nDerived & subdesc(off_type index, const std::string &desc)\nadds a description to the statistic at the given index\nsize_type size() const\nreturns the number of elements in the vector\nbool zero() const\nreturns\ntrue\nif each of distributions has 0 samples, return\nfalse\notherwise\noperator[](off_type index)\ngets the reference to the average deviation at the given index, e.g.\ndists[1].sample(2,3)",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/statistics/api",
            "page_title": "Statistics APIs",
            "parent_section": "Statistics Classes",
            "section_heading": "Stats::VectorAverageDeviation"
        }
    },
    {
        "text": "Section: Statistics Classes\n\nSub-section: Stats::Formula\n\nStoring a statistic that is a result of a series of arithmetic operations on\nStats\nobjects.\nNote that, in the following function,\nTemp\ncould be any of\nStats\nclass holding statistics (including vector statistics), a formula, or a number (e.g.\nint\n,\ndouble\n,\n1.2\n).\nFunction signatures\nDescriptions\nconst Formula &operator=(const Temp &r)\nassigns an uninitialized\nStats::Formula\nto the given root\nconst Formula &operator=(const T &v)\nassigns the formula to a statistic or another formula or a number\nconst Formula &operator+=(Temp r)\nadds to the current formula a statistic or another formula or a number\nconst Formula &operator/=(Temp r)\ndivides the current formula by a statistic or another formula or a number\nvoid result(VResult &vec) const\nassigns the evaluation of the formula to the given vector; if the formula does\nnot\nhave a vector component (none of the variables in the formula is a vector), then the vector size is 1\nResult total() const\nreturns the evaluation of the\nStats::Formula\nas a double; if the formula does have a vector component (one of the variables in the formula is a vector), then the vector is turned in to a scalar by setting it to the sum all elements in the vector\nsize_type size() const\nreturns 1 if the root element is not a vector, returns the size of the vector otherwise\nbool zero()\nreturns\ntrue\nif all elements in\nresult()\nare 0\u2019s, returns\nfalse\notherwise\nAn example of using\nStats::Formula\n,\nStats::Scalar totalReadLatency;\nStats::Scalar numReads;\nStats::Formula averageReadLatency = totalReadLatency/numReads;",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/statistics/api",
            "page_title": "Statistics APIs",
            "parent_section": "Statistics Classes",
            "section_heading": "Stats::Formula"
        }
    },
    {
        "text": "Section: Appendix. Migrating to the new style of tracking statistics\n\nSub-section: A new style of tracking statistics\n\ngem5 statistics have a flat structure that are not aware of the hierarchical structure of\nSimObject\n, which usually contains stat objects.\nThis causes the problem of different stats having the same name, and more importantly, it was not trivial to manipulating the structure of gem5 statistics.\nAlso, gem5 did not offer a way to group a collection of stat objects into different groups, which is important to maintain a large number of stat objects.\nA recent commit\nintroduces\nStats::Group\n, a structure intended to keep all statistics belong to an object.\nThe new structure offers an explicit way to reflect the hierarchical nature of\nSimObject\nStats::Group\nalso makes it more explicit and easier to maintain a large set of\nStats\nobjects that should be grouped into different collections as one can make several\nStats::Group\n\u2019s in a\nSimObject\nand merges them to the\nSimObject\n, which is also a\nStats::Group\nthat is aware of its children\nStats::Group\n\u2019s.\nGenerally, this is a step towards a more structured\nStats\nformat, which should facilitate the process of manipulating the overall structure of statistics in gem5, such as filtering out statistics and producing\nStats\nto more standardized formats such as JSON and XML, which, in turns, have an enormous amount of supported libraries in a variety of programming languages.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/statistics/api",
            "page_title": "Statistics APIs",
            "parent_section": "Appendix. Migrating to the new style of tracking statistics",
            "section_heading": "A new style of tracking statistics"
        }
    },
    {
        "text": "Section: Appendix. Migrating to the new style of tracking statistics\n\nSub-section: Migrating to the new style of tracking statistics\n\nNotes\n: Migrating to the new style is highly encouraged; however, the legacy style of statistics (i.e. the one with a flat structure) is still supported.\nThis guide provides a broad look of how to migrate to the new style of gem5 statistics tracking, as well as points out some concrete examples showing how it is being done.\nADD_STAT\nADD_STAT\nis a macro defined as,\n#define ADD_STAT(n, ...) n(this, # n, __VA_ARGS__)\nThis macro is intended to be used in\nStats::Group\nconstructors to initilize a\nStats\nobject.\nIn other words,\nADD_STAT\nis an alias for caling\nStats\nobject constructors.\nFor example,\nADD_STAT(stat_name, stat_desc)\nis the same as,\nstat_name.parent = the `Stats::Group` where stat_name is defined\n  stat_name.name = \"stat_name\"\n  stat_name.desc = \"stat_desc\"\nThis is applicable for most of\nStats\ndata types with an exception that for\nStats::Formula\n, the macro\nADD_STAT\ncan handle an optional parameter specifying the formula.\nFor example,\nADD_STAT(ips, \"Instructions per Second\", n_instructions/sim_seconds)\n.\nAn example use case of\nADD_STAT\n(and we refer to this example as \u201c\nExample 1\n\u201d throughout this section).\nThis example is also served as a template of constructing a\nStats::Group\nstruct.\nprotected:\n        // Defining the a stat group\n        struct StatGroup : public Stats::Group\n        {\n            StatGroup(Stats::Group *parent); // constructor\n            Stats::Histogram histogram;\n            Stats::Scalar scalar;\n            Stats::Formula formula;\n        } stats;\n\n    // Defining the declared constructor\n    StatGroup::StatGroup(Stats::Group *parent)\n      : Stats::Group(parent),                           // initilizing the base class\n        ADD_STAT(histogram, \"A useful histogram\"),\n        scalar(this, \"scalar\", \"A number\"),             // this is the same as ADD_STAT(scalar, \"A number\")\n        ADD_STAT(formula, \"A formula\", scalar1/scalar2)\n    {\n        histogram\n          .init(num_bins);\n        scalar\n          .init(0)\n          .flags(condition ? 1 : 0);\n    }\nMoving to the new style\nThose are concrete examples of converting stats to the new style:\nhere\n,\nhere\nand\nhere\n.\nMoving stats to the new style involves:\nCreating a struct\nStats::Group\n, and moving all stats variables there. This struct\u2019s scope should be\nprotected\n. The declaration of stat variables is usually in the header files.\nGetting rid of\nregStats()\n, and moving the initialzation of stat variables to\nStats::Group\nconstructor as shown in\nExample 1\n.\nIn both header files and cpp files, all stats variables should be pre-appended by the newly created\nStats::Group\nname as the stats are now under the\nStats::Group\nstruct.\nUpdating the class constructors to initialize\nStats::Group\nvariable. Usually, it\u2019s adding\nstats(this)\nto the constructors assuming the name of the variable is\nstats\n.\nSome examples,\nAn example of\nStats::Group\ndeclaration is\nhere\n.\nNote that all variables of type starting with\nStats::\nhave been moved to the struct.\nAn example of a\nStats::Group\nconstructor that utilizes\nADD_STAT\nis\nhere\n.\nIn the case where a stat variable requiring additional initializations other than\nname\nand\ndescription\n, you can follow\nthis example\n.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/statistics/api",
            "page_title": "Statistics APIs",
            "parent_section": "Appendix. Migrating to the new style of tracking statistics",
            "section_heading": "Migrating to the new style of tracking statistics"
        }
    },
    {
        "text": "Class overview\nClasses involved in the power model are:\nPowerModel\n:\nRepresents a power model for a hardware component.\nPowerModelState\n: Represents a\npower model for a hardware component in a certain power state. It is an\nabstract class that defines an interface that must be implemented for each\nmodel.\nMathExprPowerModel\n: Simple\nimplementation of\nPowerModelState\nthat assumes\nthat power can be modeled using a simple power.\nClasses involved in the thermal model are:\nThermalModel\n:\nContains the system thermal model logic and state. It performs the power query\nand temperature update. It also enables gem5 to query for temperature (for OS\nreporting).\nThermalDomain\n:\nRepresents an entity that generates heat. It\u2019s essentially a group of\nSimObjects\ngrouped\nunder a SubSystem component that have its own thermal behaviour.\nThermalNode\n:\nRepresents a node in the thermal circuital equivalent. The node has a\ntemperature and interacts with other nodes through connections (thermal\nresistors and capacitors).\nThermalReference\n: Temperature\nreference for the thermal model (essentially a thermal node with a fixed\ntemperature), can be used to model air or any other constant temperature\ndomains.\nThermalEntity\n:\nA thermal component that connects two thermal nodes and models a thermal\nimpedance between them. This class is just an abstract interface.\nThermalResistor\n: Implements\nThermalEntity\nto\nmodel a thermal resistance between the two nodes it connects. Thermal\nresistances model the capacity of a material to transfer heat (units in K/W).\nThermalCapacitor\n: Implements\nThermalEntity\nto\nmodel a thermal capacitance. Thermal capacitors are used to model material\u2019s\nthermal capacitance, this is, the ability to change a certain material\ntemperature (units in J/K).",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/thermal_model",
            "page_title": "Power and Thermal Model",
            "parent_section": "Class overview",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Thermal model\nThe thermal model works by creating a circuital equivalent of the simulated\nplatform. Each node in the circuit has a temperature (as voltage equivalent)\nand power flows between nodes (as current in a circuit).\nTo build this equivalent temperature model the platform is required to group\nthe power actors (any component that has a power model) under SubSystems and\nattach ThermalDomains to those subsystems. Other components might also be\ncreated (like ThermalReferences) and connected all together by creating thermal\nentities (capacitors and resistors).\nLast step to conclude the thermal model is to create the\nThermalModel\ninstance itself and\nattach all the instances used to it, so it can properly update them at runtime.\nOnly one thermal model instance is supported right now and it will\nautomatically report temperature when appropriate (ie. platform sensor\ndevices).",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/thermal_model",
            "page_title": "Power and Thermal Model",
            "parent_section": "Thermal model",
            "section_heading": "Overview"
        }
    },
    {
        "text": "Power model\nEvery\nClockedObject\nhas a power model\nassociated. If this power model is non-null power will be calculated at every\nstats dump (although it might be possible to force power evaluation at any\nother point, if the power model uses the stats, it is a good idea to keep both\nevents in sync). The definition of a power model is quite vague in the sense\nthat it is as flexible as users want it to be. The only enforced contraints so\nfar is the fact that a power model has several power state models, one for each\npossible power state for that hardware block. When it comes to compute power\nconsumption the power is just the weighted average of each power model.\nA power state model is essentially an interface that allows us to define two\npower functions for dynamic and static. As an example implementation a class\ncalled\nMathExprPowerModel\nhas been\nprovided. This implementation allows the user to define a power model as an\nequation involving several statistics. There\u2019s also some automatic (or \u201cmagic\u201d)\nvariables such as \u201ctemp\u201d, which reports temperature.",
        "metadata": {
            "source_url": "https://www.gem5.org/documentation/general_docs/thermal_model",
            "page_title": "Power and Thermal Model",
            "parent_section": "Power model",
            "section_heading": "Overview"
        }
    }
]